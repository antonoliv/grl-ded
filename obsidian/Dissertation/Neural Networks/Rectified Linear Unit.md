The Rectified Linear Unit or *ReLU* is an widely used activation function for [[Artificial Neural Network|ANNs]] defined as:

$$\text{ReLU}(x) = max(0,x)$$

