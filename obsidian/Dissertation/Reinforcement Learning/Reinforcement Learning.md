**Reinforcement Learning** or RL consists in a class of machine learning algorithms that focus on learning how to take good sequences of actions in order to achieve a goal associated with a maximising received numerical reward signal. An RL agent's main objective is to maximize the received cumulative reward by trying between the available actions and discovering which ones yield the most reward. This sequential decision making process becomes more complex when delayed reward is considered, given that an action immediate reward may not always reflect the delayed consequences of taking that decision, it's the learner's job to also take this into consideration during the learning process. These two concepts of *delayed reward* and *trial-and-error search* make up for the most important characteristics of Reinforcement Learning. The classic formalisation of this problem is the [[Markov Decision Process]], which is explained in the next subsection. 

A major challenge that arises in this machine learning paradigm, is the trade-off between exploration of new unknown actions and exploitation of the already known "good" actions. To choose the sequence of actions that return the highest reward the agent must choose actions it found to be effective in similar past situations, or *exploit* what it learned from past experience. Furthermore, given that initially the agent may not know the action-reward mappings, it has to *explore* possible actions that were not selected previously or may initially seem to yield a low reward in order to compute accurate reward estimates. The main problem is that neither exploitation or exploration can be favoured exclusively without failing at the task. Additionally, an agent's environment is uncertain and changes in the environments dynamics may also involve a re-estimation of action rewards.

Regarding

In conclusion, RL techniques enable the implementation of sequential decision-making agents that seek to maximize a reward signal analogous to an explicit (complex) goal. The agents need to balance between actions that yield a reward on posterior time steps and actions that produce immediate rewards. In addition, these agents are also faced with the task of balancing the exploitation of information from past experiences and the exploration of new decision paths that could potentially return a higher reward down the road. 


Model-free vs model-based
On-policy vs off-policy