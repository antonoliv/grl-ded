[
  {"id":"almasanDeepReinforcementLearning2022","abstract":"Deep Reinforcement Learning (DRL) has shown a dramatic improvement in decision-making and automated control problems. Consequently, DRL represents a promising technique to efficiently solve many relevant optimization problems (e.g., routing) in self-driving networks. However, existing DRL-based solutions applied to networking fail to generalize, which means that they are not able to operate properly when applied to network topologies not observed during training. This lack of generalization capability significantly hinders the deployment of DRL technologies in production networks. This is because state-of-the-art DRL-based networking solutions use standard neural networks (e.g., fully connected, convolutional), which are not suited to learn from information structured as graphs. In this paper, we integrate Graph Neural Networks (GNN) into DRL agents and we design a problem specific action space to enable generalization. GNNs are Deep Learning models inherently designed to generalize over graphs of different sizes and structures. This allows the proposed GNN-based DRL agent to learn and generalize over arbitrary network topologies. We test our DRL+GNN agent in a routing optimization use case in optical networks and evaluate it on 180 and 232 unseen synthetic and real-world network topologies respectively. The results show that the DRL+GNN agent is able to outperform state-of-the-art solutions in topologies never seen during training.","accessed":{"date-parts":[["2023",11,27]]},"author":[{"family":"Almasan","given":"Paul"},{"family":"Suárez-Varela","given":"José"},{"family":"Rusek","given":"Krzysztof"},{"family":"Barlet-Ros","given":"Pere"},{"family":"Cabellos-Aparicio","given":"Albert"}],"citation-key":"almasanDeepReinforcementLearning2022","container-title":"Computer Communications","container-title-short":"Computer Communications","DOI":"10.1016/j.comcom.2022.09.029","ISSN":"0140-3664","issued":{"date-parts":[["2022",12,1]]},"page":"184-194","source":"ScienceDirect","title":"Deep reinforcement learning meets graph neural networks: Exploring a routing optimization use case","title-short":"Deep reinforcement learning meets graph neural networks","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0140366422003784","volume":"196"},
  {"id":"antarisDeepGraphReinforcement2021","archive":"Scopus","author":[{"family":"Antaris","given":"S."},{"family":"Rafailidis","given":"D."},{"family":"Gidzijauskas","given":"S."}],"citation-key":"antarisDeepGraphReinforcement2021","DOI":"10.1109/BigData52589.2021.9671949","event-title":"Proceedings - 2021 IEEE International Conference on Big Data, Big Data 2021","issued":{"date-parts":[["2021"]]},"page":"1787-1796","title":"A Deep Graph Reinforcement Learning Model for Improving User Experience in Live Video Streaming","type":"paper-conference","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125360036&doi=10.1109%2fBigData52589.2021.9671949&partnerID=40&md5=aad9a476373f121d3ca7a63cab9b4f33"},
  {"id":"bayindirSmartGridTechnologies2016","abstract":"Smart grid technologies can be defined as self-sufficient systems that can find solutions to problems quickly in an available system that reduces the workforce and targets sustainable, reliable, safe and quality electricity to all consumers. In this respect, different technological applications can be seen from the perspective of researchers and investors. Even though these technological application studies constitute an initial step for the structure of the smart grid, they have not been fully completed in many countries. Associations of initial studies for the next step in smart grid applications will provide an economic benefit for the authorities in the long term, and will help to establish standards to be compatible with every application so that all smart grid applications can be coordinated under the control of the same authorities. In this study, a review has been made of technological methods of data transmission and the energy efficiency in smart grids as well as smart grid applications. Therefore, this study is expected to be an important guiding source for researchers and engineers studying the smart grid. It also helps transmission and distribution system operators to follow the right path as they are transforming their classical grids to smart grids.","accessed":{"date-parts":[["2023",11,20]]},"author":[{"family":"Bayindir","given":"R."},{"family":"Colak","given":"I."},{"family":"Fulli","given":"G."},{"family":"Demirtas","given":"K."}],"citation-key":"bayindirSmartGridTechnologies2016","container-title":"Renewable and Sustainable Energy Reviews","container-title-short":"Renewable and Sustainable Energy Reviews","DOI":"10.1016/j.rser.2016.08.002","ISSN":"1364-0321","issued":{"date-parts":[["2016",12,1]]},"page":"499-516","source":"ScienceDirect","title":"Smart grid technologies and applications","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S1364032116304191","volume":"66"},
  {"id":"bihaniStriderNetGraphReinforcement2023","abstract":"Optimization of atomic structures presents a challenging problem, due to their highly rough and non-convex energy landscape, with wide applications in the fields of drug design, materials discovery, and mechanics. Here, we present a graph reinforcement learning approach, StriderNet, that learns a policy to displace the atoms towards low energy configurations. We evaluate the performance of StriderNet on three complex atomic systems, namely, binary Lennard-Jones particles, calcium silicate hydrates gel, and disordered silicon. We show that StriderNet outperforms all classical optimization algorithms and enables the discovery of a lower energy minimum. In addition, StriderNet exhibits a higher rate of reaching minima with energies, as confirmed by the average over multiple realizations. Finally, we show that StriderNet exhibits inductivity to unseen system sizes that are an order of magnitude different from the training system. All the codes and datasets are available at https://github.com/M3RG-IITD/StriderNET.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Bihani","given":"Vaibhav"},{"family":"Manchanda","given":"Sahil"},{"family":"Sastry","given":"Srikanth"},{"family":"Ranu","given":"Sayan"},{"family":"Krishnan","given":"N. M. Anoop"}],"citation-key":"bihaniStriderNetGraphReinforcement2023","container-title":"Proceedings of the 40th International Conference on Machine Learning","event-title":"International Conference on Machine Learning","ISSN":"2640-3498","issued":{"date-parts":[["2023",7,3]]},"language":"en","page":"2431-2451","publisher":"PMLR","source":"proceedings.mlr.press","title":"StriderNet: A Graph Reinforcement Learning Approach to Optimize Atomic Structures on Rough Energy Landscapes","title-short":"StriderNet","type":"paper-conference","URL":"https://proceedings.mlr.press/v202/bihani23a.html"},
  {"id":"bihaniSTRIDERNETGraphReinforcement2023","archive":"Scopus","author":[{"family":"Bihani","given":"V."},{"family":"Manchanda","given":"S."},{"family":"Sastry","given":"S."},{"family":"Ranu","given":"S."},{"family":"Anoop Krishnan","given":"N.M."}],"citation-key":"bihaniSTRIDERNETGraphReinforcement2023","event-title":"Proceedings of Machine Learning Research","issued":{"date-parts":[["2023"]]},"page":"2431-2451","title":"STRIDERNET: A Graph Reinforcement Learning Approach to Optimize Atomic Structures on Rough Energy Landscapes","type":"paper-conference","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174408196&partnerID=40&md5=50bf988abbb8c7b96ff476fb63924e55","volume":"202"},
  {"id":"bozkusEnsembleGraphQLearning2023","abstract":"The optimization of large-scale networks such as finding the optimal control strategies through cost minimization is challenged by large state spaces. For networks that can be modeled via Markov Decision Processes (MDP), a previously proposed graph reduction strategy is used in conjunction with a novel ensemble learning method based on Q-learning algorithm for policy optimization in unknown environments. By exploiting the structural properties of the network, several structurally related Markov chains are created and these multiple chains are sampled to learn multiple policies which are fused. The convergence of the learning approach is analyzed and the ensemble learning strategy is shown to inherit the properties of classical Q-learning. Numerical results show that the proposed algorithm achieves a reduction of 60% with respect to the policy error and 80% for the runtime versus other state-of-the-art Q-learning algorithms.","accessed":{"date-parts":[["2023",11,13]]},"author":[{"family":"Bozkus","given":"Talha"},{"family":"Mitra","given":"Urbashi"}],"citation-key":"bozkusEnsembleGraphQLearning2023","container-title":"ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","DOI":"10.1109/ICASSP49357.2023.10094828","event-title":"ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","ISSN":"2379-190X","issued":{"date-parts":[["2023",6]]},"page":"1-5","source":"IEEE Xplore","title":"Ensemble Graph Q-Learning for Large Scale Networks","type":"paper-conference","URL":"https://ieeexplore.ieee.org/abstract/document/10094828"},
  {"id":"brunskillCS234ReinforcementLearning","accessed":{"date-parts":[["2024",1,5]]},"author":[{"family":"Brunskill","given":"Emma"}],"citation-key":"brunskillCS234ReinforcementLearning","genre":"Lecture Slides","title":"CS234: Reinforcement Learning Winter 2023","type":"manuscript","URL":"https://web.stanford.edu/class/cs234/"},
  {"id":"bukhariFractionalOrderLorenz2022","archive":"Scopus","author":[{"family":"Bukhari","given":"A.H."},{"family":"Raja","given":"M.A.Z."},{"family":"Shoaib","given":"M."},{"family":"Kiani","given":"A.K."}],"citation-key":"bukhariFractionalOrderLorenz2022","container-title":"Chaos, Solitons and Fractals","DOI":"10.1016/j.chaos.2022.112375","issued":{"date-parts":[["2022"]]},"title":"Fractional order Lorenz based physics informed SARFIMA-NARX model to monitor and mitigate megacities air pollution","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133946073&doi=10.1016%2fj.chaos.2022.112375&partnerID=40&md5=888b920572ade9c9f9583fd0fe10f07b","volume":"161"},
  {"id":"caiForecastingHourlyPM22023","archive":"Scopus","author":[{"family":"Cai","given":"P."},{"family":"Zhang","given":"C."},{"family":"Chai","given":"J."}],"citation-key":"caiForecastingHourlyPM22023","container-title":"Data Science and Management","DOI":"10.1016/j.dsm.2023.02.002","issue":"1","issued":{"date-parts":[["2023"]]},"page":"46-54","title":"Forecasting hourly PM2.5 concentrations based on decomposition-ensemble-reconstruction framework incorporating deep learning algorithms","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150366269&doi=10.1016%2fj.dsm.2023.02.002&partnerID=40&md5=68bc49e5b5816fde7d8be9dedee42a2f","volume":"6"},
  {"id":"caoDataDrivenMultiAgentDeep2021","abstract":"This paper proposes a novel model-free/data-driven centralized training and decentralized execution multi-agent deep reinforcement learning (MADRL) framework for distribution system voltage control with high penetration of PVs. The proposed MADRL can coordinate both the real and reactive power control of PVs with existing static var compensators and battery storage systems. Unlike the existing DRL-based voltage control methods, our proposed method does not rely on a system model during both the training and execution stages. This is achieved by developing a new interaction scheme between the surrogate modeling of the original system and the multi-agent soft actor critic (MASAC) MADRL algorithm. In particular, the sparse pseudo-Gaussian process with a few-shots of measurements is utilized to construct the surrogate model of the original environment, i.e., power flow model. This is a data-driven process and no model parameters are needed. Furthermore, the MASAC enabled MADRL allows to achieve better scalability by dividing the original system into different voltage control regions with the aid of real and reactive power sensitivities to voltage, where each region is treated as an agent. This also serves as the foundation for the centralized training and decentralized execution, thus significantly reducing the communication requirements as only local measurements are required for control. Comparative results with other alternatives on the IEEE 123-nodes and 342-nodes systems demonstrate the superiority of the proposed method.","accessed":{"date-parts":[["2023",11,17]]},"author":[{"family":"Cao","given":"Di"},{"family":"Zhao","given":"Junbo"},{"family":"Hu","given":"Weihao"},{"family":"Ding","given":"Fei"},{"family":"Huang","given":"Qi"},{"family":"Chen","given":"Zhe"},{"family":"Blaabjerg","given":"Frede"}],"citation-key":"caoDataDrivenMultiAgentDeep2021","container-title":"IEEE Transactions on Smart Grid","DOI":"10.1109/TSG.2021.3072251","ISSN":"1949-3061","issue":"5","issued":{"date-parts":[["2021",9]]},"page":"4137-4150","source":"IEEE Xplore","title":"Data-Driven Multi-Agent Deep Reinforcement Learning for Distribution System Decentralized Voltage Control With High Penetration of PVs","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9399637","volume":"12"},
  {"id":"charniakIntroductionDeepLearning2018","author":[{"family":"Charniak","given":"Eugene"}],"citation-key":"charniakIntroductionDeepLearning2018","event-place":"Cambridge, Massachusetts London, England","ISBN":"978-0-262-03951-2","issued":{"date-parts":[["2018"]]},"language":"en","number-of-pages":"174","publisher":"The MIT Press","publisher-place":"Cambridge, Massachusetts London, England","source":"K10plus ISBN","title":"Introduction to deep learning","type":"book"},
  {"id":"chenAutonomousExplorationUncertainty2020","abstract":"We consider an autonomous exploration problem in which a range-sensing mobile robot is tasked with accurately mapping the landmarks in an a priori unknown environment efficiently in real-time; it must choose sensing actions that both curb localization uncertainty and achieve information gain. For this problem, belief space planning methods that forward- simulate robot sensing and estimation may often fail in real-time implementation, scaling poorly with increasing size of the state, belief and action spaces. We propose a novel approach that uses graph neural networks (GNNs) in conjunction with deep reinforcement learning (DRL), enabling decision-making over graphs containing exploration information to predict a robot's optimal sensing action in belief space. The policy, which is trained in different random environments without human intervention, offers a real-time, scalable decision-making process whose high-performance exploratory sensing actions yield accurate maps and high rates of information gain.","accessed":{"date-parts":[["2023",12,18]]},"author":[{"family":"Chen","given":"Fanfei"},{"family":"Martin","given":"John D."},{"family":"Huang","given":"Yewei"},{"family":"Wang","given":"Jinkun"},{"family":"Englot","given":"Brendan"}],"citation-key":"chenAutonomousExplorationUncertainty2020","container-title":"2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","DOI":"10.1109/IROS45743.2020.9341657","event-title":"2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","ISSN":"2153-0866","issued":{"date-parts":[["2020",10]]},"page":"6140-6147","source":"IEEE Xplore","title":"Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning on Graphs","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9341657"},
  {"id":"chenAutonomousExplorationUncertainty2020a","archive":"Scopus","author":[{"family":"Chen","given":"F."},{"family":"Martin","given":"J.D."},{"family":"Huang","given":"Y."},{"family":"Wang","given":"J."},{"family":"Englot","given":"B."}],"citation-key":"chenAutonomousExplorationUncertainty2020a","DOI":"10.1109/IROS45743.2020.9341657","event-title":"IEEE International Conference on Intelligent Robots and Systems","issued":{"date-parts":[["2020"]]},"page":"6140-6147","title":"Autonomous exploration under uncertainty via deep reinforcement learning on graphs","type":"paper-conference","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102404320&doi=10.1109%2fIROS45743.2020.9341657&partnerID=40&md5=40edf243ea0f66a0adea7b1ab084e95b"},
  {"id":"chenGraphRepresentationLearningbased2023","abstract":"It is important to achieve an efficient home energy management system (HEMS) because of its role in promoting energy saving and emission reduction for end-users. Two critical issues in an efficient HEMS are identification of user behavior and energy management strategy. However, current HEMS methods usually assume perfect knowledge of user behavior or ignore the strong correlations of usage habits with different applications. This can lead to an insufficient description of behavior and suboptimal management strategy. To address these gaps, this paper proposes non-intrusive load monitoring (NILM) assisted graph reinforcement learning (GRL) for intelligent HEMS decision making. First, a behavior correlation graph incorporating NILM is introduced to represent the energy consumption behavior of users and a multi-label classification model is used to monitor the loads. Thus, efficient identification of user behavior and description of state transition can be achieved. Second, based on the online updating of the behavior correlation graph, a GRL model is proposed to extract information contained in the graph. Thus, reliable strategy under uncertainty of environment and behavior is available. Finally, the experimental results on several datasets verify the effectiveness of the proposed model.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Chen","given":"Xinpei"},{"family":"Yu","given":"Tao"},{"family":"Pan","given":"Zhenning"},{"family":"Wang","given":"Zihao"},{"family":"Yang","given":"Shengchun"}],"citation-key":"chenGraphRepresentationLearningbased2023","container-title":"Protection and Control of Modern Power Systems","container-title-short":"Prot Control Mod Power Syst","DOI":"10.1186/s41601-023-00305-x","ISSN":"2367-0983","issue":"1","issued":{"date-parts":[["2023",12]]},"language":"en","license":"2023 The Author(s)","number":"1","page":"1-13","publisher":"SpringerOpen","source":"pcmp.springeropen.com","title":"Graph representation learning-based residential electricity behavior identification and energy management","type":"article-journal","URL":"https://pcmp.springeropen.com/articles/10.1186/s41601-023-00305-x","volume":"8"},
  {"id":"chenPhysicalassistedMultiagentGraph2023","abstract":"Active distribution network is encountering serious voltage violations associated with the proliferation of distributed photovoltaic. Cutting-edge research has confirmed that voltage regulation techniques based on deep reinforcement learning manifest superior performance in addressing this issue. However, such techniques are typically applied to the specifically fixed network topologies and have insufficient learning efficiency. To address these challenges, a novel edge intelligence, featured by a multi-agent deep reinforcement learning algorithm with graph attention network and physical-assisted mechanism, is proposed. This novel method is unique in that it includes the graph attention network into reinforcement learning to capture spatial correlations and topological linkages among nodes, allowing agents to be “aware” of topology variations caused by reconfiguration real time. Furthermore, employing a relatively exact physical model to generate reference experiences and storing them in a replay buffer enables agents to identify effective actions faster during training and thus, greatly enhances the efficiency of learning voltage regulation laws. All agents are trained centralized to learn a coordinated voltage regulation strategy, which is then executed decentralized based solely on local observation for fast response. The proposed methodology is evaluated on the IEEE 33-node and 136-node systems, and it outperforms the previously implemented approaches in convergence and control performance.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Chen","given":"Yongdong"},{"family":"Liu","given":"Youbo"},{"family":"Zhao","given":"Junbo"},{"family":"Qiu","given":"Gao"},{"family":"Yin","given":"Hang"},{"family":"Li","given":"Zhengbo"}],"citation-key":"chenPhysicalassistedMultiagentGraph2023","container-title":"Applied Energy","container-title-short":"Applied Energy","DOI":"10.1016/j.apenergy.2023.121743","ISSN":"0306-2619","issued":{"date-parts":[["2023",12,1]]},"page":"121743","source":"ScienceDirect","title":"Physical-assisted multi-agent graph reinforcement learning enabled fast voltage regulation for PV-rich active distribution network","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0306261923011078","volume":"351"},
  {"id":"chenScalableGraphReinforcement2023","abstract":"Due to the increasing penetration of renewable energy source, power system is facing significant uncertainties. How to fully consider those uncertainties in dynamic economic dispatch (DED) has become a crucial problem to the safe and economic operation of power system. Reinforcement learning (RL) based approaches can provide the dispatch policy in response to uncertainty. However, current RL uses traditional Euclidean data representation, which greatly reduces the scalability and computational efficiency of economic dispatch algorithms. To address such obstacle, this paper develops a novel graph reinforcement learning (GRL) method for DED of power system. Firstly, DED is formulated as a Markov decision process and formulated as a dynamic sequential decision problem. Secondly, a novel graph-based representation of system state is proposed. Using graph data to represent dispatch operation data with non-Euclidean characteristics can effectively capture the implicit correlation of the uncertainty corresponding to the system topology. Thirdly, a GRL algorithm is proposed to learn the optimal policy which maps graph represented system state to DED decision. Compared with the traditional deep reinforcement learning (DRL), the GRL proposed in this paper has certain generalization ability and scalability, and can achieve higher quality solutions in online operation. Case studies illustrate that the optimality of the proposed method is 98.04%, which is 15.13% higher than that of the existing learning methods. The algorithm is scalable and improves the sample efficiency.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Chen","given":"Junbin"},{"family":"Yu","given":"Tao"},{"family":"Pan","given":"Zhenning"},{"family":"Zhang","given":"Mengyue"},{"family":"Deng","given":"Bairong"}],"citation-key":"chenScalableGraphReinforcement2023","container-title":"International Journal of Electrical Power & Energy Systems","container-title-short":"International Journal of Electrical Power & Energy Systems","DOI":"10.1016/j.ijepes.2023.109212","ISSN":"0142-0615","issued":{"date-parts":[["2023",10,1]]},"page":"109212","source":"ScienceDirect","title":"A scalable graph reinforcement learning algorithm based stochastic dynamic dispatch of power system under high penetration of renewable energy","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0142061523002697","volume":"152"},
  {"id":"chenZeroShotReinforcementLearning2021","archive":"Scopus","author":[{"family":"Chen","given":"F."},{"family":"Szenher","given":"P."},{"family":"Huang","given":"Y."},{"family":"Wang","given":"J."},{"family":"Shan","given":"T."},{"family":"Bai","given":"S."},{"family":"Englot","given":"B."}],"citation-key":"chenZeroShotReinforcementLearning2021","DOI":"10.1109/ICRA48506.2021.9561917","event-title":"Proceedings - IEEE International Conference on Robotics and Automation","issued":{"date-parts":[["2021"]]},"page":"5193-5199","title":"Zero-Shot Reinforcement Learning on Graphs for Autonomous Exploration under Uncertainty","type":"paper-conference","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124792138&doi=10.1109%2fICRA48506.2021.9561917&partnerID=40&md5=2e4db558eb237896d17419a75055d65c","volume":"2021-May"},
  {"id":"choLearningPhraseRepresentations2014","abstract":"In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases. © 2014 Association for Computational Linguistics.","archive":"Scopus","author":[{"family":"Cho","given":"K."},{"family":"Van Merriënboer","given":"B."},{"family":"Gulcehre","given":"C."},{"family":"Bahdanau","given":"D."},{"family":"Bougares","given":"F."},{"family":"Schwenk","given":"H."},{"family":"Bengio","given":"Y."}],"citation-key":"choLearningPhraseRepresentations2014","DOI":"10.3115/v1/d14-1179","event-title":"EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference","ISBN":"978-1-937284-96-1","issued":{"date-parts":[["2014"]]},"language":"English","page":"1724-1734","source":"Scopus","title":"Learning phrase representations using RNN encoder-decoder for statistical machine translation","type":"paper-conference"},
  {"id":"dangNovelMultivariateGrey2023","archive":"Scopus","author":[{"family":"Dang","given":"Y."},{"family":"Zhang","given":"Y."},{"family":"Wang","given":"J."}],"citation-key":"dangNovelMultivariateGrey2023","container-title":"Expert Systems with Applications","DOI":"10.1016/j.eswa.2022.118556","issued":{"date-parts":[["2023"]]},"title":"A novel multivariate grey model for forecasting periodic oscillation time series","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137165174&doi=10.1016%2fj.eswa.2022.118556&partnerID=40&md5=22b31187e19dd1182f8d07be79795f87","volume":"211"},
  {"id":"devaillyIGRLInductiveGraph2022","abstract":"Scaling adaptive traffic signal control involves dealing with combinatorial state and action spaces. Multi-agent reinforcement learning attempts to address this challenge by distributing control to specialized agents. However, specialization hinders generalization and transferability, and the computational graphs underlying neural-network architectures - dominating in the multi-agent setting - do not offer the flexibility to handle an arbitrary number of entities which changes both between road networks, and over time as vehicles traverse the network. We introduce Inductive Graph Reinforcement Learning (IG-RL) based on graph-convolutional networks which adapts to the structure of any road network, to learn detailed representations of traffic signal controllers and their surroundings. Our decentralized approach enables learning of a transferable-adaptive-traffic-signal-control policy. After being trained on an arbitrary set of road networks, our model can generalize to new road networks and traffic distributions, with no additional training and a constant number of parameters, enabling greater scalability compared to prior methods. Furthermore, our approach can exploit the granularity of available data by capturing the (dynamic) demand at both the lane level and the vehicle level. The proposed method is tested on both road networks and traffic settings never experienced during training. We compare IG-RL to multi-agent reinforcement learning and domain-specific baselines. In both synthetic road networks and in a larger experiment involving the control of the 3,971 traffic signals of Manhattan, we show that different instantiations of IG-RL outperform baselines. © 2000-2011 IEEE.","archive":"Scopus","author":[{"family":"Devailly","given":"F.-X."},{"family":"Larocque","given":"D."},{"family":"Charlin","given":"L."}],"citation-key":"devaillyIGRLInductiveGraph2022","container-title":"IEEE Transactions on Intelligent Transportation Systems","DOI":"10.1109/TITS.2021.3070835","ISSN":"1524-9050","issue":"7","issued":{"date-parts":[["2022"]]},"language":"English","page":"7496-7507","source":"Scopus","title":"IG-RL: Inductive Graph Reinforcement Learning for Massive-Scale Traffic Signal Control","title-short":"IG-RL","type":"article-journal","volume":"23"},
  {"id":"djakerScalableCostEfficient2020","archive":"Scopus","author":[{"family":"Djaker","given":"A.-B."},{"family":"Kechar","given":"B."},{"family":"Ibn-Khedher","given":"H."},{"family":"Moungla","given":"H."},{"family":"Afifi","given":"H."}],"citation-key":"djakerScalableCostEfficient2020","DOI":"10.1109/IWCMC48107.2020.9148257","event-title":"2020 International Wireless Communications and Mobile Computing, IWCMC 2020","issued":{"date-parts":[["2020"]]},"page":"539-544","title":"Scalable and Cost Efficient Maximum Concurrent Flow over IoT using Reinforcement Learning","type":"paper-conference","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089659832&doi=10.1109%2fIWCMC48107.2020.9148257&partnerID=40&md5=e9584fba2d9caf89da9b4e24d7424209"},
  {"id":"dolatabadiEnhancedIEEE332021","abstract":"The transformation of passive distribution systems to more active ones thanks to the increased penetration of distributed energy resources, such as dispersed generators, flexible demand, distributed storage, and electric vehicles, creates the necessity of an enhanced test system for distribution systems planning and operation studies. The value of the proposed test system, is that it provides an appropriate and comprehensive benchmark for future researches concerning distribution systems. The proposed test system is developed by modifying and updating the well-known 33 bus distribution system from Baran & Wu. It comprises both forms of balanced and unbalanced three-phase power systems, including new details on the integration of distributed and renewable generation units, reactive power compensation assets, reconfiguration infrastructures and appropriate datasets of load and renewable generation profiles for different case studies.","accessed":{"date-parts":[["2023",11,20]]},"author":[{"family":"Dolatabadi","given":"Sarineh Hacopian"},{"family":"Ghorbanian","given":"Maedeh"},{"family":"Siano","given":"Pierluigi"},{"family":"Hatziargyriou","given":"Nikos D."}],"citation-key":"dolatabadiEnhancedIEEE332021","container-title":"IEEE Transactions on Power Systems","DOI":"10.1109/TPWRS.2020.3038030","ISSN":"1558-0679","issue":"3","issued":{"date-parts":[["2021",5]]},"page":"2565-2572","source":"IEEE Xplore","title":"An Enhanced IEEE 33 Bus Benchmark Test System for Distribution System Studies","type":"article-journal","URL":"https://ieeexplore.ieee.org/abstract/document/9258930/references#references","volume":"36"},
  {"id":"dorronsoroOptimizationLearning6th2023","accessed":{"date-parts":[["2023",11,17]]},"citation-key":"dorronsoroOptimizationLearning6th2023","collection-title":"Communications in Computer and Information Science","DOI":"10.1007/978-3-031-34020-8","editor":[{"family":"Dorronsoro","given":"Bernabé"},{"family":"Chicano","given":"Francisco"},{"family":"Danoy","given":"Gregoire"},{"family":"Talbi","given":"El-Ghazali"}],"event-place":"Cham","ISBN":"978-3-031-34019-2 978-3-031-34020-8","issued":{"date-parts":[["2023"]]},"language":"en","publisher":"Springer Nature Switzerland","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"Optimization and Learning: 6th International Conference, OLA 2023, Malaga, Spain, May 3–5, 2023, Proceedings","title-short":"Optimization and Learning","type":"book","URL":"https://link.springer.com/10.1007/978-3-031-34020-8","volume":"1824"},
  {"id":"fanAttentionBasedMultiAgentGraph2023","abstract":"With the ongoing integration of distributed energy resources, modern distribution systems are getting sufficient generation capacity to perform active restoration after outages without transmission system support. The model-based approaches are widely used in resolving service restoration problems, relying on accurate system models. Deep reinforcement learning is believed as an alternative solution for problem solving, although it has not been sufficiently explored. In this paper, the service restoration process is described as a partially observable Markov decision process, and a multi-agent graph reinforcement learning approach based on attention is proposed to train multiple agents to co-achieve the restoration goal to reinforce the system resilience in coping with extreme events. To consider the connections and correlations between nodes during the service restoration, the state of the active distribution network is defined by graph data that contains features of both topology and nodes. The perceived ability of the agents is empowered by graph convolutional networks during the feature extraction, supplying agents with more comprehensive data to learn more reasonable restoration strategies. In addition, the centralized training with attention is developed for multi-agent systems, focusing on the relations between the agents to strengthen teamwork capability. The performance of the proposed method is verified by a set of comparative studies on the IEEE-118 system with dispatchable generators, rooftop photovoltaics, and energy storage systems simultaneously.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Fan","given":"Bangji"},{"family":"Liu","given":"Xinghua"},{"family":"Xiao","given":"Gaoxi"},{"family":"Kang","given":"Yu"},{"family":"Wang","given":"Dianhui"},{"family":"Wang","given":"Peng"}],"citation-key":"fanAttentionBasedMultiAgentGraph2023","container-title":"IEEE Transactions on Artificial Intelligence","DOI":"10.1109/TAI.2023.3314395","ISSN":"2691-4581","issued":{"date-parts":[["2023"]]},"page":"1-15","source":"IEEE Xplore","title":"Attention-Based Multi-Agent Graph Reinforcement Learning for Service Restoration","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/10247632"},
  {"id":"farhangiPathSmartGrid2010","abstract":"Exciting yet challenging times lie ahead. The electrical power industry is undergoing rapid change. The rising cost of energy, the mass electrification of everyday life, and climate change are the major drivers that will determine the speed at which such transformations will occur. Regardless of how quickly various utilities embrace smart grid concepts, technologies, and systems, they all agree onthe inevitability of this massive transformation. It is a move that will not only affect their business processes but also their organization and technologies.","accessed":{"date-parts":[["2023",11,20]]},"author":[{"family":"Farhangi","given":"Hassan"}],"citation-key":"farhangiPathSmartGrid2010","container-title":"IEEE Power and Energy Magazine","DOI":"10.1109/MPE.2009.934876","ISSN":"1558-4216","issue":"1","issued":{"date-parts":[["2010",1]]},"page":"18-28","source":"IEEE Xplore","title":"The path of the smart grid","type":"article-journal","URL":"https://ieeexplore.ieee.org/abstract/document/5357331","volume":"8"},
  {"id":"FeedforwardNeuralNetwork","accessed":{"date-parts":[["2023",12,13]]},"citation-key":"FeedforwardNeuralNetwork","title":"Feedforward Neural Network","type":"webpage","URL":"https://orgs.mines.edu/daa/wp-content/uploads/sites/38/2019/08/1_Gh5PS4R_A5drl5ebd_gNrg@2x.jpg"},
  {"id":"gadelhoApplicationGraphNeural2023","accessed":{"date-parts":[["2023",12,28]]},"author":[{"family":"Gadelho","given":"Ana Clara Moreira"}],"citation-key":"gadelhoApplicationGraphNeural2023","issued":{"date-parts":[["2023",7,21]]},"language":"eng","license":"openAccess","note":"Accepted: 2023-12-18T03:30:17Z","publisher":"FEUP","source":"repositorio-aberto.up.pt","title":"Application of Graph Neural Networks in Road Traffic Forecasting for Intelligent Transportation Systems","type":"thesis","URL":"https://repositorio-aberto.up.pt/handle/10216/151990"},
  {"id":"gammelliGraphReinforcementLearning2023","archive":"Scopus","author":[{"family":"Gammelli","given":"D."},{"family":"Harrison","given":"J."},{"family":"Yang","given":"K."},{"family":"Pavone","given":"M."},{"family":"Rodrigues","given":"F."},{"family":"Pereira","given":"F.C."}],"citation-key":"gammelliGraphReinforcementLearning2023","event-title":"Proceedings of Machine Learning Research","issued":{"date-parts":[["2023"]]},"page":"10587-10610","title":"Graph Reinforcement Learning for Network Control via Bi-Level Optimization","type":"paper-conference","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174416634&partnerID=40&md5=18be994d98cef07d942247070b8f09e4","volume":"202"},
  {"id":"gaoFastAdaptiveTask2023","abstract":"In multi-access edge computing, when many mobile devices (MDs) offload their tasks to an edge server (ES), its resources might become constrained. These tasks may take a long time to complete or even be thrown away. Since the unknown information of both the ESs and other MDs, it is difficult for each MD to determine its offloading policy independently. Furthermore, most offloading methods have poor generalization to new environment since they focus on model architecture with a fixed quantity of MDs and ESs, preventing trained models from transferring to other environments. In the paper, we provide a full decentralized offloading scheme based on the Curriculum Attention-weighted Graph Recurrent Network-based Multi-Agent Actor-Critic (CAGR-MAAC). First, we build MEC as a shared MD agents-ESs graph and an AGR-based message network is designed to enable each MD aggregate the information of ESs and other MDs and solve the partial observability of MD agents for MEC system. Second, a learnable differentiable encoder network is introduced to construct MD agent’s local information encoding. Subsequently, the MD agent converts overall the information regarding the MEC system into a fixed-size embedding via an AGR Network to handle different quantity of MDs and ESs. Finally, we introduce curriculum learning to address the huge complexity of the MEC system and the training difficulties induced by the large amounts of MDs and ESs. Experiments demonstrate that compared with existing algorithms, CAGR-MAAC boosts task completion rates and decreases system costs by 13.01% 15.03% and 16.45% 18.56%, and can quickly adapt to the new environment.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Gao","given":"Zhen"},{"family":"Yang","given":"Lei"},{"family":"Dai","given":"Yu"}],"citation-key":"gaoFastAdaptiveTask2023","container-title":"IEEE Internet of Things Journal","DOI":"10.1109/JIOT.2023.3285950","ISSN":"2327-4662","issued":{"date-parts":[["2023"]]},"page":"1-1","source":"IEEE Xplore","title":"Fast Adaptive Task Offloading and Resource Allocation in Large-Scale MEC Systems via Multi-Agent Graph Reinforcement Learning","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/10153416"},
  {"id":"gaoGraphUNets2019","abstract":"We consider the problem of representation learning for graph data. Convolutional neural networks can naturally operate on images, but have significant challenges in dealing with graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully applied on many image pixel-wise prediction tasks, similar methods are lacking for graph data. This is due to the fact that pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling (gPool) and unpooling (gUnpool) operations in this work. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values on a trainable projection vector. We further propose the gUnpool layer as the inverse operation of the gPool layer. The gUnpool layer restores the graph into its original structure using the position information of nodes selected in the corresponding gPool layer. Based on our proposed gPool and gUnpool layers, we develop an encoder-decoder model on graph, known as the graph U-Nets. Our experimental results on node classification and graph classification tasks demonstrate that our methods achieve consistently better performance than previous models.","accessed":{"date-parts":[["2024",1,5]]},"author":[{"family":"Gao","given":"Hongyang"},{"family":"Ji","given":"Shuiwang"}],"citation-key":"gaoGraphUNets2019","container-title":"Proceedings of the 36th International Conference on Machine Learning","event-title":"International Conference on Machine Learning","ISSN":"2640-3498","issued":{"date-parts":[["2019",5,24]]},"language":"en","page":"2083-2092","publisher":"PMLR","source":"proceedings.mlr.press","title":"Graph U-Nets","type":"paper-conference","URL":"https://proceedings.mlr.press/v97/gao19a.html"},
  {"id":"gaoMultiVehiclesDecisionMakingInteractive2022","abstract":"In the research of driverless decision-making, most of the current research is aimed at following, changing lanes, overtaking, and other scenarios. In this paper, algorithms and reward functions are designed to solve decision-making problems in interactive environments. An interaction and stochastic high-speed exit scenario of human-driven vehicles and autonomous vehicles (AVs) is designed. The features of the graph are directly extracted from the surrounding environment information of AVs by using the graph convolutional neural network (GCN). The steering angle and longitudinal acceleration are output through the neural network module. In addition, an exponentially increasing reward function based on driving purpose, traffic efficiency, driving comfort, and safety is designed. Moreover, this paper uses two algorithms, Graph Convolutional Deep Q-learning Network (GCN-DQN) and Graph Convolutional Double Deep Q-learning Network (GCN-DDQN), to train the driverless decision model and compare them with each other. According to simulation results, by adjusting the weight value in the reward function, the system can realize different control targets. Meanwhile, the decision-making model trained by GCN-DDQN has better performance under strong interactive random scenes than the GCN-DQN method.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Gao","given":"Xin"},{"family":"Luan","given":"Tian"},{"family":"Li","given":"Xueyuan"},{"family":"Liu","given":"Qi"},{"family":"Li","given":"Zirui"},{"family":"Yang","given":"Fan"}],"citation-key":"gaoMultiVehiclesDecisionMakingInteractive2022","container-title":"2022 IEEE 17th Conference on Industrial Electronics and Applications (ICIEA)","DOI":"10.1109/ICIEA54703.2022.10005925","event-title":"2022 IEEE 17th Conference on Industrial Electronics and Applications (ICIEA)","ISSN":"2158-2297","issued":{"date-parts":[["2022",12]]},"page":"534-539","source":"IEEE Xplore","title":"Multi-Vehicles Decision-Making in Interactive Highway Exit: A Graph Reinforcement Learning Approach","title-short":"Multi-Vehicles Decision-Making in Interactive Highway Exit","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/10005925"},
  {"id":"goodfellowDeepLearning2016","author":[{"family":"Goodfellow","given":"Ian"},{"family":"Bengio","given":"Yoshua"},{"family":"Courville","given":"Aaron"}],"call-number":"Q325.5 .G66 2016","citation-key":"goodfellowDeepLearning2016","collection-title":"Adaptive computation and machine learning","event-place":"Cambridge, Massachusetts","ISBN":"978-0-262-03561-3","issued":{"date-parts":[["2016"]]},"number-of-pages":"775","publisher":"The MIT Press","publisher-place":"Cambridge, Massachusetts","source":"Library of Congress ISBN","title":"Deep learning","type":"book"},
  {"id":"goreModifiedBureauPublic2023","abstract":"The Bureau of Public Roads (BPR) function is a widely used link cost function in transportation planning because of its simple mathematical form, easily measurable field variables, and consistent performance. However, the BPR function is deterministic and does not capture the stochastic relation between travel time and traffic flow. The present study develops a modified BPR (MBPR) function by incorporating travel time uncertainty (TTU) in the deterministic BPR function. In the MBPR function, the effect of TTU is incorporated using two parameters, \nγ\n and \nδ\n. A nonlinear optimization problem is formulated, and the generalized reduced gradient method is used to calibrate the BPR and the MBPR function. The applicability of the proposed MBPR function is demonstrated using empirical data collected for an urban arterial in India and simulated data developed for a real-world urban road network. The proposed MBPR function captures the heterogeneity in travel time for different traffic flow values. The function (a) captures the variability in travel time under oversaturated conditions and (b) captures the time-dependent relation between traffic volume and delay. The physical meaning of \nγ\n and \nδ\n in the context of inter-day heterogeneity, infrastructure potential, and traffic flow heterogeneity are discussed. The practical application of MBPR as an analytical tool for system-wide performance evaluation is demonstrated by investigating the impact of traffic signal control on travel times using a before–after perspective. Compared with the before case, a 13% reduction in travel time is observed for the after case. Therefore, the installation of traffic signal control has reduced congestion.","accessed":{"date-parts":[["2024",1,1]]},"author":[{"family":"Gore","given":"Ninad"},{"family":"Arkatkar","given":"Shriniwas"},{"family":"Joshi","given":"Gaurang"},{"family":"Antoniou","given":"Constantinos"}],"citation-key":"goreModifiedBureauPublic2023","container-title":"Transportation Research Record","DOI":"10.1177/03611981221138511","ISSN":"0361-1981","issue":"5","issued":{"date-parts":[["2023",5,1]]},"language":"en","page":"966-990","publisher":"SAGE Publications Inc","source":"SAGE Journals","title":"Modified Bureau of Public Roads Link Function","type":"article-journal","URL":"https://doi.org/10.1177/03611981221138511","volume":"2677"},
  {"id":"gunarathnaSolvingDynamicGraph2022","abstract":"Graph problems such as traveling salesman problem, or finding minimal Steiner trees are widely studied and used in data engineering and computer science. Typically, in real-world applications, the features of the graph tend to change over time, thus, finding a solution to the problem becomes challenging. The dynamic version of many graph problems are the key for a plethora of real-world problems in transportation, telecommunication, and social networks. In recent years, using deep learning techniques to find heuristic solutions for NP-hard graph combinatorial problems has gained much interest as these learned heuristics can find near-optimal solutions efficiently. However, most of the existing methods for learning heuristics focus on static graph problems. The dynamic nature makes NP-hard graph problems much more challenging to learn, and the existing methods fail to find reasonable solutions. In this paper, we propose a novel architecture named Graph Temporal Attention with Reinforcement Learning (GTA-RL) to learn heuristic solutions for graph-based dynamic combinatorial optimization problems. The GTA-RL architecture consists of an encoder capable of embedding temporal features of a combinatorial problem instance and a decoder capable of dynamically focusing on the embedded features to find a solution to a given combinatorial problem instance. We then extend our architecture to learn heuristics for the real-time version of combinatorial optimization problems where all input features of a problem are not known a prior, but rather learned in real-time. Our experimental results against several state-of-the-art learning-based algorithms and optimal solvers demonstrate that our approach outperforms the state-of-the-art learning-based approaches in terms of effectiveness and optimal solvers in terms of efficiency on dynamic and real-time graph combinatorial optimization.","accessed":{"date-parts":[["2023",12,18]]},"author":[{"family":"Gunarathna","given":"Udesh"},{"family":"Borovica-Gajic","given":"Renata"},{"family":"Karunasekara","given":"Shanika"},{"family":"Tanin","given":"Egemen"}],"citation-key":"gunarathnaSolvingDynamicGraph2022","DOI":"10.48550/arXiv.2201.04895","issued":{"date-parts":[["2022",1,13]]},"number":"arXiv:2201.04895","publisher":"arXiv","source":"arXiv.org","title":"Solving Dynamic Graph Problems with Multi-Attention Deep Reinforcement Learning","type":"article","URL":"http://arxiv.org/abs/2201.04895"},
  {"id":"guPowerSystemTransient2022","abstract":"The power transient stability analysis is one of the basis for determining the control strategy of power system security and stability. Considering the influence of power grid topology on the transient stability of power system, the transient stability evaluation model is constructed based on the graph attention neural network. The electrical components and their transient operation data are mapped to the graph data with the spatial topology characteristics of power system for model training, so as to improve the topological generalization performance of the model. The marginal contribution of input characteristics to the output of transient power angle stability evaluation model is quantitatively calculated based on Shapley additive explanation (SHAP), so as to improve the interpretability of data-driven method for transient power angle stability evaluation. The effectiveness of the proposed method is verified by the IEEE 39-bus system.","accessed":{"date-parts":[["2023",11,7]]},"author":[{"family":"Gu","given":"Sili"},{"family":"Qiao","given":"Ji"},{"family":"Zhao","given":"Zixuan"},{"family":"Zhu","given":"Qiongfeng"},{"family":"Han","given":"Fujia"}],"citation-key":"guPowerSystemTransient2022","container-title":"2022 4TH INTERNATIONAL CONFERENCE ON SMART POWER & INTERNET ENERGY SYSTEMS, SPIES","DOI":"10.1109/SPIES55999.2022.10082542","event-place":"New York","event-title":"4th International Conference on Smart Power and Internet Energy Systems (SPIES)","ISBN":"978-1-66548-957-7","issued":{"date-parts":[["2022"]]},"language":"English","note":"Web of Science ID: WOS:000994420700246","number-of-pages":"6","page":"1374-1379","publisher":"IEEE","publisher-place":"New York","source":"Clarivate Analytics Web of Science","title":"Power System Transient Stability Assessment Based on Graph Neural Network with Interpretable Attribution Analysis","type":"paper-conference","URL":"https://www.webofscience.com/api/gateway?GWVersion=2&SrcAuth=DynamicDOIConfProc&SrcApp=WOS&KeyAID=10.1109%2FSPIES55999.2022.10082542&DestApp=DOI&SrcAppSID=EUW1ED0DB18rZVcI0F2svR30WDcJg&SrcJTitle=2022+4TH+INTERNATIONAL+CONFERENCE+ON+SMART+POWER+%26+INTERNET+ENERGY+SYSTEMS%2C+SPIES&DestDOIRegistrantName=Institute+of+Electrical+and+Electronics+Engineers"},
  {"id":"gurEnvironmentGenerationZeroShot2021","archive":"Scopus","author":[{"family":"Gur","given":"I."},{"family":"Jaques","given":"N."},{"family":"Miao","given":"Y."},{"family":"Choi","given":"J."},{"family":"Tiwari","given":"M."},{"family":"Lee","given":"H."},{"family":"Faust","given":"A."}],"citation-key":"gurEnvironmentGenerationZeroShot2021","event-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[["2021"]]},"page":"4157-4169","title":"Environment Generation for Zero-Shot Compositional Reinforcement Learning","type":"paper-conference","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131729418&partnerID=40&md5=4125e7407e20d458d3aedf9ef2db4a3d","volume":"6"},
  {"id":"haarnojaSoftActorCriticOffPolicy2018","abstract":"Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.","accessed":{"date-parts":[["2023",12,16]]},"author":[{"family":"Haarnoja","given":"Tuomas"},{"family":"Zhou","given":"Aurick"},{"family":"Abbeel","given":"Pieter"},{"family":"Levine","given":"Sergey"}],"citation-key":"haarnojaSoftActorCriticOffPolicy2018","container-title":"Proceedings of the 35th International Conference on Machine Learning","event-title":"International Conference on Machine Learning","ISSN":"2640-3498","issued":{"date-parts":[["2018",7,3]]},"language":"en","page":"1861-1870","publisher":"PMLR","source":"proceedings.mlr.press","title":"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor","title-short":"Soft Actor-Critic","type":"paper-conference","URL":"https://proceedings.mlr.press/v80/haarnoja18b.html"},
  {"id":"hamiltonGraphRepresentationLearning","abstract":"Graph-structured data is ubiquitous throughout the natural and social sciences, from telecommunication networks to quantum chemistry. Building relational inductive biases into deep learning architectures is crucial if we want systems that can learn, reason, and generalize from this kind of data. Recent years have seen a surge in research on graph representation learning, including techniques for deep graph embeddings, generalizations of convolutional neural networks to graph-structured data, and neural message-passing approaches inspired by belief propagation. These advances in graph representation learning have led to new state-of-the-art results in numerous domains, including chemical synthesis, 3D-vision, recommender systems, question answering, and social network analysis.","author":[{"family":"Hamilton","given":"William L"}],"citation-key":"hamiltonGraphRepresentationLearning","language":"en","source":"Zotero","title":"Graph Representation Learning","type":"article-journal"},
  {"id":"hazraDynamicServiceDeployment2022","abstract":"Edge computing has lately appeared as a practical entrance to intensify the capabilities of the industrial networks by transferring the computation data to the edge of the networks. However, the challenge here is to locate processing devices and provide services through the shortest path by making a proper trade-off between latency and energy of the processing. These demands can be solved using complex optimization problems within a confined deadline, which is barely feasible with traditional statistical techniques. Thus, we propose a heuristic-based task controlling mechanism that makes service deployment decisions by eliminating the need to solve combinatorial optimization problems. Moreover, to achieve the least possible delay and find the best possible path towards the goal, we adopt Graph Reinforcement Learning (GRL) technique. Extensive numerical results illustrate that our proposed strategy generates near-optimal solutions within the given time-bound.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Hazra","given":"Abhishek"},{"family":"Adhikari","given":"Mainak"},{"family":"Amgoth","given":"Tarachand"}],"citation-key":"hazraDynamicServiceDeployment2022","container-title":"2022 International Conference on Computing, Communication, Security and Intelligent Systems (IC3SIS)","DOI":"10.1109/IC3SIS54991.2022.9885498","event-title":"2022 International Conference on Computing, Communication, Security and Intelligent Systems (IC3SIS)","issued":{"date-parts":[["2022",6]]},"page":"1-6","source":"IEEE Xplore","title":"Dynamic Service Deployment Strategy using Reinforcement Learning in Edge Networks","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9885498"},
  {"id":"hazraDynamicServiceDeployment2022a","archive":"Scopus","author":[{"family":"Hazra","given":"A."},{"family":"Adhikari","given":"M."},{"family":"Amgoth","given":"T."}],"citation-key":"hazraDynamicServiceDeployment2022a","DOI":"10.1109/IC3SIS54991.2022.9885498","event-title":"Proceedings of International Conference on Computing, Communication, Security and Intelligent Systems, IC3SIS 2022","issued":{"date-parts":[["2022"]]},"title":"Dynamic Service Deployment Strategy using Reinforcement Learning in Edge Networks","type":"paper-conference","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139161549&doi=10.1109%2fIC3SIS54991.2022.9885498&partnerID=40&md5=5b76356b2b733fe784b05e6832756171"},
  {"id":"hesselRainbowCombiningImprovements2018","abstract":"The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.","accessed":{"date-parts":[["2023",10,31]]},"author":[{"family":"Hessel","given":"Matteo"},{"family":"Modayil","given":"Joseph"},{"family":"Hasselt","given":"Hado","dropping-particle":"van"},{"family":"Schaul","given":"Tom"},{"family":"Ostrovski","given":"Georg"},{"family":"Dabney","given":"Will"},{"family":"Horgan","given":"Dan"},{"family":"Piot","given":"Bilal"},{"family":"Azar","given":"Mohammad"},{"family":"Silver","given":"David"}],"citation-key":"hesselRainbowCombiningImprovements2018","container-title":"Proceedings of the AAAI Conference on Artificial Intelligence","DOI":"10.1609/aaai.v32i1.11796","ISSN":"2374-3468","issue":"1","issued":{"date-parts":[["2018",4,29]]},"language":"en","license":"Copyright (c)","number":"1","source":"ojs.aaai.org","title":"Rainbow: Combining Improvements in Deep Reinforcement Learning","title-short":"Rainbow","type":"article-journal","URL":"https://ojs.aaai.org/index.php/AAAI/article/view/11796","volume":"32"},
  {"id":"huActualUrbanTraffic2014","abstract":"Urban traffic congestion is a pandemic illness affecting many cities around the world. We have developed and tested an actual urban traffic simulative model (AUTM) for predicting and avoiding traffic congestion. The model includes three key components, the map and transfer (MT) conversion, optimized spatial evolution rules, and a congestion-avoidance routing algorithm. (1) The MT conversion method is proposed to get actual urban cellular spaces, which apply optimized spatial evolution rules to simulate vehicular dynamics better. (2)AUTM is proposed for simulating traffic congestion and predicting the effect of adding overpasses and roadblocks. (3)The congestion-avoidance routing algorithm is proposed for vehicles to dynamically update their routes toward their destinations, which can achieve traffic optimization in urban simulations. Extensive experimental simulations in various actual cities are carried out. Our results in this extreme case are encouraging: the traffic congestion forecasting accuracy is more than 89%, and the variance of road density forecast is less than 0.2.","accessed":{"date-parts":[["2024",1,1]]},"author":[{"family":"Hu","given":"Wenbin"},{"family":"Wang","given":"Huan"},{"family":"Yan","given":"Liping"}],"citation-key":"huActualUrbanTraffic2014","container-title":"17th International IEEE Conference on Intelligent Transportation Systems (ITSC)","DOI":"10.1109/ITSC.2014.6958119","event-title":"17th International IEEE Conference on Intelligent Transportation Systems (ITSC)","ISSN":"2153-0017","issued":{"date-parts":[["2014",10]]},"page":"2681-2686","source":"IEEE Xplore","title":"An actual urban traffic simulation model for predicting and avoiding traffic congestion","type":"paper-conference","URL":"https://ieeexplore.ieee.org/abstract/document/6958119?casa_token=UBL5vHkXZykAAAAA:4mJjOblDrkReX5wKX8OSIUF7VaCjgbHTSNFQYBMD2ZUbZYRFVdwdNNaUNIP1K8XUuCzartmu3So"},
  {"id":"huActualUrbanTraffic2014a","abstract":"Urban traffic congestion is a pandemic illness affecting many cities around the world. We have developed and tested an actual urban traffic simulative model (AUTM) for predicting and avoiding traffic congestion. The model includes three key components, the map and transfer (MT) conversion, optimized spatial evolution rules, and a congestion-avoidance routing algorithm. (1) The MT conversion method is proposed to get actual urban cellular spaces, which apply optimized spatial evolution rules to simulate vehicular dynamics better. (2)AUTM is proposed for simulating traffic congestion and predicting the effect of adding overpasses and roadblocks. (3)The congestion-avoidance routing algorithm is proposed for vehicles to dynamically update their routes toward their destinations, which can achieve traffic optimization in urban simulations. Extensive experimental simulations in various actual cities are carried out. Our results in this extreme case are encouraging: the traffic congestion forecasting accuracy is more than 89%, and the variance of road density forecast is less than 0.2.","accessed":{"date-parts":[["2024",1,1]]},"author":[{"family":"Hu","given":"Wenbin"},{"family":"Wang","given":"Huan"},{"family":"Yan","given":"Liping"}],"citation-key":"huActualUrbanTraffic2014a","container-title":"17th International IEEE Conference on Intelligent Transportation Systems (ITSC)","DOI":"10.1109/ITSC.2014.6958119","event-title":"17th International IEEE Conference on Intelligent Transportation Systems (ITSC)","ISSN":"2153-0017","issued":{"date-parts":[["2014",10]]},"page":"2681-2686","source":"IEEE Xplore","title":"An actual urban traffic simulation model for predicting and avoiding traffic congestion","type":"paper-conference","URL":"https://ieeexplore.ieee.org/abstract/document/6958119?casa_token=UBL5vHkXZykAAAAA:4mJjOblDrkReX5wKX8OSIUF7VaCjgbHTSNFQYBMD2ZUbZYRFVdwdNNaUNIP1K8XUuCzartmu3So"},
  {"id":"huActualUrbanTraffic2014b","abstract":"Urban traffic congestion is a pandemic illness affecting many cities around the world. We have developed and tested an actual urban traffic simulative model (AUTM) for predicting and avoiding traffic congestion. The model includes three key components, the map and transfer (MT) conversion, optimized spatial evolution rules, and a congestion-avoidance routing algorithm. (1) The MT conversion method is proposed to get actual urban cellular spaces, which apply optimized spatial evolution rules to simulate vehicular dynamics better. (2)AUTM is proposed for simulating traffic congestion and predicting the effect of adding overpasses and roadblocks. (3)The congestion-avoidance routing algorithm is proposed for vehicles to dynamically update their routes toward their destinations, which can achieve traffic optimization in urban simulations. Extensive experimental simulations in various actual cities are carried out. Our results in this extreme case are encouraging: the traffic congestion forecasting accuracy is more than 89%, and the variance of road density forecast is less than 0.2.","accessed":{"date-parts":[["2024",1,1]]},"author":[{"family":"Hu","given":"Wenbin"},{"family":"Wang","given":"Huan"},{"family":"Yan","given":"Liping"}],"citation-key":"huActualUrbanTraffic2014b","container-title":"17th International IEEE Conference on Intelligent Transportation Systems (ITSC)","DOI":"10.1109/ITSC.2014.6958119","event-title":"17th International IEEE Conference on Intelligent Transportation Systems (ITSC)","ISSN":"2153-0017","issued":{"date-parts":[["2014",10]]},"page":"2681-2686","source":"IEEE Xplore","title":"An actual urban traffic simulation model for predicting and avoiding traffic congestion","type":"paper-conference","URL":"https://ieeexplore.ieee.org/abstract/document/6958119?casa_token=UBL5vHkXZykAAAAA:4mJjOblDrkReX5wKX8OSIUF7VaCjgbHTSNFQYBMD2ZUbZYRFVdwdNNaUNIP1K8XUuCzartmu3So"},
  {"id":"huActualUrbanTraffic2014c","abstract":"Urban traffic congestion is a pandemic illness affecting many cities around the world. We have developed and tested an actual urban traffic simulative model (AUTM) for predicting and avoiding traffic congestion. The model includes three key components, the map and transfer (MT) conversion, optimized spatial evolution rules, and a congestion-avoidance routing algorithm. (1) The MT conversion method is proposed to get actual urban cellular spaces, which apply optimized spatial evolution rules to simulate vehicular dynamics better. (2)AUTM is proposed for simulating traffic congestion and predicting the effect of adding overpasses and roadblocks. (3)The congestion-avoidance routing algorithm is proposed for vehicles to dynamically update their routes toward their destinations, which can achieve traffic optimization in urban simulations. Extensive experimental simulations in various actual cities are carried out. Our results in this extreme case are encouraging: the traffic congestion forecasting accuracy is more than 89%, and the variance of road density forecast is less than 0.2.","accessed":{"date-parts":[["2024",1,1]]},"author":[{"family":"Hu","given":"Wenbin"},{"family":"Wang","given":"Huan"},{"family":"Yan","given":"Liping"}],"citation-key":"huActualUrbanTraffic2014c","container-title":"17th International IEEE Conference on Intelligent Transportation Systems (ITSC)","DOI":"10.1109/ITSC.2014.6958119","event-title":"17th International IEEE Conference on Intelligent Transportation Systems (ITSC)","ISSN":"2153-0017","issued":{"date-parts":[["2014",10]]},"page":"2681-2686","source":"IEEE Xplore","title":"An actual urban traffic simulation model for predicting and avoiding traffic congestion","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/6958119"},
  {"id":"huangDeepGraphReinforcement2022","archive":"Scopus","author":[{"family":"Huang","given":"R."},{"family":"Guan","given":"W."},{"family":"Zhai","given":"G."},{"family":"He","given":"J."},{"family":"Chu","given":"X."}],"citation-key":"huangDeepGraphReinforcement2022","container-title":"Applied Sciences (Switzerland)","DOI":"10.3390/app12041951","issue":"4","issued":{"date-parts":[["2022"]]},"title":"Deep Graph Reinforcement Learning Based Intelligent Traffic Routing Control for Software-Defined Wireless Sensor Networks","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124770576&doi=10.3390%2fapp12041951&partnerID=40&md5=0d52107ae11f86bd7011ce5d1d7b6dde","volume":"12"},
  {"id":"huangDeepGraphReinforcement2022a","archive":"Scopus","author":[{"family":"Huang","given":"R."},{"family":"Guan","given":"W."},{"family":"Zhai","given":"G."},{"family":"He","given":"J."},{"family":"Chu","given":"X."}],"citation-key":"huangDeepGraphReinforcement2022a","container-title":"Applied Sciences (Switzerland)","DOI":"10.3390/app12041951","issue":"4","issued":{"date-parts":[["2022"]]},"title":"Deep Graph Reinforcement Learning Based Intelligent Traffic Routing Control for Software-Defined Wireless Sensor Networks","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124770576&doi=10.3390%2fapp12041951&partnerID=40&md5=0d52107ae11f86bd7011ce5d1d7b6dde","volume":"12"},
  {"id":"huLivingObjectGrasping2021","archive":"Scopus","author":[{"family":"Hu","given":"Z."},{"family":"Zheng","given":"Y."},{"family":"Pan","given":"J."}],"citation-key":"huLivingObjectGrasping2021","container-title":"IEEE Robotics and Automation Letters","DOI":"10.1109/LRA.2021.3060636","issue":"2","issued":{"date-parts":[["2021"]]},"page":"1950-1957","title":"Living Object Grasping Using Two-Stage Graph Reinforcement Learning","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101736179&doi=10.1109%2fLRA.2021.3060636&partnerID=40&md5=b7fccf2b48f5df780131e8b1789777e7","volume":"6"},
  {"id":"huMultiAgentDeepReinforcement2022","abstract":"The increasing penetration of distributed renewable energy resources causes voltage fluctuations in distribution networks. The controllable active and reactive power resources such as energy storage (ES) systems and electric vehicles (EVs) in active distribution networks play an important role in mitigating the voltage excursions. This paper proposes a two-timescale hybrid voltage control strategy based on a mixed-integer optimization method and multi-agent reinforcement learning (MARL) to reduce power loss and mitigate voltage violations. In the slow-timescale, the active and reactive power optimization problem involving capacitor banks (CBs), on-load tap changers (OLTC), and ES systems is formulated as a mixed-integer second-order cone programming problem. In the fast-timescale, the reactive power of smart inverters connected to solar photovoltaic systems and active power of EVs are adjusted to mitigate short-term voltage fluctuations with a MARL algorithm. Specifically, we propose an experience augmented multi-agent actor-critic (EA-MAAC) algorithm with an attention mechanism to learn high-quality control policies. The control policies are executed online in a decentralized manner. The proposed hybrid voltage control strategy is validated on an IEEE testing distribution feeder. The numerical results show that our proposed control strategy is not only sample-efficient and robust but also effective in mitigating voltage fluctuations.","accessed":{"date-parts":[["2023",11,17]]},"author":[{"family":"Hu","given":"Daner"},{"family":"Ye","given":"Zhenhui"},{"family":"Gao","given":"Yuanqi"},{"family":"Ye","given":"Zuzhao"},{"family":"Peng","given":"Yonggang"},{"family":"Yu","given":"Nanpeng"}],"citation-key":"huMultiAgentDeepReinforcement2022","container-title":"IEEE Transactions on Smart Grid","DOI":"10.1109/TSG.2022.3185975","ISSN":"1949-3061","issue":"6","issued":{"date-parts":[["2022",11]]},"page":"4873-4886","source":"IEEE Xplore","title":"Multi-Agent Deep Reinforcement Learning for Voltage Control With Coordinated Active and Reactive Power Optimization","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9805763","volume":"13"},
  {"id":"huMultiagentGraphReinforcement2024","abstract":"Volt/Var control (VVC) is a crucial function in power distribution systems to minimize power loss and maintain voltages within allowable limits. However, incomplete and inaccurate information about the distribution network makes model-based VVC methods difficult to implement in practice. In this paper, we propose a novel multi-agent graph-based deep reinforcement learning (DRL) algorithm named MASAC-HGRN to address the VVC problem under partial observation constraints. Our proposed algorithm divides the power distribution system into several regions, each region treated as an agent. Unlike traditional model-based or global-observation-based DRL methods, our proposed method leverages a practical decentralized training and decentralized execution (DTDE) paradigm to address the partial observation constraints. The well-trained agents gather information only from their interconnected neighbors and realize decentralized local control. Numerical studies with IEEE 33-bus and 123-bus distribution test feeders demonstrate that our proposed MASAC-HGRN algorithm outperforms the state-of-art RL algorithms and traditional model-based approaches in terms of VVC performance. Moreover, the DTDE framework exhibits flexibility and robustness in extensive robustness experiments.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Hu","given":"Daner"},{"family":"Li","given":"Zichen"},{"family":"Ye","given":"Zhenhui"},{"family":"Peng","given":"Yonggang"},{"family":"Xi","given":"Wei"},{"family":"Cai","given":"Tiantian"}],"citation-key":"huMultiagentGraphReinforcement2024","container-title":"International Journal of Electrical Power & Energy Systems","container-title-short":"International Journal of Electrical Power & Energy Systems","DOI":"10.1016/j.ijepes.2023.109531","ISSN":"0142-0615","issued":{"date-parts":[["2024",1,1]]},"page":"109531","source":"ScienceDirect","title":"Multi-agent graph reinforcement learning for decentralized Volt-VAR control in power distribution systems","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0142061523005884","volume":"155"},
  {"id":"jiangCooperativePlanningMultiUAV2023","abstract":"In this paper, we design a deep reinforcement learning algorithm based on graph neural network to solve the problem of cooperation control of multiple UAVs. Our algorithm can control multiple UAV swarms to complete package delivery tasks in an unexplored area under partial observation. Since each UAV has only a limited observation space and a small range of communication, we propose a reinforcement learning algorithm based on graph neural network, which can process multiple graphs simultaneously time and aggregate the feature vectors of the neighbor agents to address the cooperative issue of heterogeneous multi-agent coordination. We conduct a couple of ablation experiments to prove the effectiveness and performance characteristics of our algorithm.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Jiang","given":"Zhiling"},{"family":"Chen","given":"Yining"},{"family":"Song","given":"Guanghua"},{"family":"Yang","given":"Bowei"},{"family":"Jiang","given":"Xiaohong"}],"citation-key":"jiangCooperativePlanningMultiUAV2023","container-title":"International Conference on Computer Application and Information Security (ICCAIS 2022)","DOI":"10.1117/12.2671868","event-title":"International Conference on Computer Application and Information Security (ICCAIS 2022)","issued":{"date-parts":[["2023",3,21]]},"page":"129-137","publisher":"SPIE","source":"www.spiedigitallibrary.org","title":"Cooperative planning of multi-UAV logistics delivery by multi-graph reinforcement learning","type":"paper-conference","URL":"https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12609/126090I/Cooperative-planning-of-multi-UAV-logistics-delivery-by-multi-graph/10.1117/12.2671868.full","volume":"12609"},
  {"id":"jiangGraphConvolutionalReinforcement2020","abstract":"Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.","accessed":{"date-parts":[["2023",12,18]]},"author":[{"family":"Jiang","given":"Jiechuan"},{"family":"Dun","given":"Chen"},{"family":"Huang","given":"Tiejun"},{"family":"Lu","given":"Zongqing"}],"citation-key":"jiangGraphConvolutionalReinforcement2020","DOI":"10.48550/arXiv.1810.09202","issued":{"date-parts":[["2020",2,11]]},"number":"arXiv:1810.09202","publisher":"arXiv","source":"arXiv.org","title":"Graph Convolutional Reinforcement Learning","type":"article","URL":"http://arxiv.org/abs/1810.09202"},
  {"id":"jiangPathSpuriousnessawareReinforcement2023","abstract":"Multi-hop reasoning, a prevalent approach for query answering, aims at inferring new facts along reasonable paths over a knowledge graph. Reinforcement learning (RL) methods can be adopted by formulating the problem into a Markov decision process. However, common suffering within RL-based reasoning models is that the agent can be biased to spurious paths which coincidentally lead to the correct answer with poor explanation. In this work, we take a deep dive into this phenomenon and deﬁne a metric named Path Spuriousness (PS), to quantitatively estimate to what extent a path is spurious. Guided by the deﬁnition of PS, we design a model with a new reward that considers both answer accuracy and path reasonableness. We test our method on ﬁve datasets and experiments reveal that our method considerably enhances the agent’s capacity to prevent spurious paths while keeping comparable to state-of-the-art performance.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Jiang","given":"Chunyang"},{"family":"Zhu","given":"Tianchen"},{"family":"Zhou","given":"Haoyi"},{"family":"Liu","given":"Chang"},{"family":"Deng","given":"Ting"},{"family":"Hu","given":"Chunming"},{"family":"Li","given":"Jianxin"}],"citation-key":"jiangPathSpuriousnessawareReinforcement2023","container-title":"Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics","DOI":"10.18653/v1/2023.eacl-main.232","event-place":"Dubrovnik, Croatia","event-title":"Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics","issued":{"date-parts":[["2023"]]},"language":"en","page":"3181-3192","publisher":"Association for Computational Linguistics","publisher-place":"Dubrovnik, Croatia","source":"DOI.org (Crossref)","title":"Path Spuriousness-aware Reinforcement Learning for Multi-Hop Knowledge Graph Reasoning","type":"paper-conference","URL":"https://aclanthology.org/2023.eacl-main.232"},
  {"id":"johnnGRAPHReinforcementLearning2023","abstract":"ALNS is a popular metaheuristic with renowned efficiency in solving combinatorial optimisation problems. However, despite 16 years of intensive research into ALNS, whether the embedded adaptive layer can efficiently select operators to improve the incumbent remains an open question. In this work, we formulate the choice of operators as a Markov Decision Process, and propose a practical approach based on Deep Reinforcement Learning and Graph Neural Networks. The results show that our proposed method achieves better performance than the classic ALNS adaptive layer due to the choice of operator being conditioned on the current solution. We also discuss important considerations such as the size of the operator portfolio and the impact of the choice of operator scales. Notably, our approach can also save significant time and labour costs for handcrafting problem-specific operator portfolios.","author":[{"family":"Johnn","given":"Syu-Ning"},{"family":"Darvariu","given":"Victor-Alexandru"},{"family":"Handl","given":"Julia"},{"family":"Kalcsics","given":"Joerg"}],"citation-key":"johnnGRAPHReinforcementLearning2023","collection-title":"Communications in Computer and Information Science","container-title":"Optimization and Learning","DOI":"10.1007/978-3-031-34020-8_15","editor":[{"family":"Dorronsoro","given":"Bernabé"},{"family":"Chicano","given":"Francisco"},{"family":"Danoy","given":"Gregoire"},{"family":"Talbi","given":"El-Ghazali"}],"event-place":"Cham","ISBN":"978-3-031-34020-8","issued":{"date-parts":[["2023"]]},"language":"en","page":"200-212","publisher":"Springer Nature Switzerland","publisher-place":"Cham","source":"Springer Link","title":"GRAPH Reinforcement Learning for Operator Selection in the ALNS Metaheuristic","type":"paper-conference"},
  {"id":"johnnGRAPHReinforcementLearning2023a","archive":"Scopus","author":[{"family":"Johnn","given":"S.-N."},{"family":"Darvariu","given":"V.-A."},{"family":"Handl","given":"J."},{"family":"Kalcsics","given":"J."}],"citation-key":"johnnGRAPHReinforcementLearning2023a","DOI":"10.1007/978-3-031-34020-8_15","event-title":"Communications in Computer and Information Science","issued":{"date-parts":[["2023"]]},"page":"200-212","title":"GRAPH Reinforcement Learning for Operator Selection in the ALNS Metaheuristic","type":"paper-conference","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163380834&doi=10.1007%2f978-3-031-34020-8_15&partnerID=40&md5=b793441ae3ffeba0f768f40c592e7d6b","volume":"1824 CCIS"},
  {"id":"kaelblingReinforcementLearningSurvey1996","abstract":"This paper surveys the field of reinforcement learning from    a computer-science perspective. It is written to be accessible to    researchers familiar with machine learning.  Both the historical basis    of the field and a broad selection of current work are summarized.    Reinforcement learning is the problem faced by an agent that learns    behavior through trial-and-error interactions with a dynamic    environment.  The work described here has a resemblance to work in    psychology, but differs considerably in the details and in the use of    the word ``reinforcement.''  The paper discusses central issues of    reinforcement learning, including trading off exploration and    exploitation, establishing the foundations of the field via Markov    decision theory, learning from delayed reinforcement, constructing    empirical models to accelerate learning, making use of generalization    and hierarchy, and coping with hidden state.  It concludes with a    survey of some implemented systems and an assessment of the practical    utility of current methods for reinforcement learning.","accessed":{"date-parts":[["2023",9,21]]},"author":[{"family":"Kaelbling","given":"L. P."},{"family":"Littman","given":"M. L."},{"family":"Moore","given":"A. W."}],"citation-key":"kaelblingReinforcementLearningSurvey1996","container-title":"Journal of Artificial Intelligence Research","DOI":"10.1613/jair.301","ISSN":"1076-9757","issued":{"date-parts":[["1996",5,1]]},"language":"en","license":"Copyright (c)","page":"237-285","source":"www.jair.org","title":"Reinforcement Learning: A Survey","title-short":"Reinforcement Learning","type":"article-journal","URL":"https://www.jair.org/index.php/jair/article/view/10166","volume":"4"},
  {"id":"khadkaOPTIMIZINGMEMORYPLACEMENT2021","archive":"Scopus","author":[{"family":"Khadka","given":"S."},{"family":"Aflalo","given":"E."},{"family":"Marder","given":"M."},{"family":"Ben-David","given":"A."},{"family":"Miret","given":"S."},{"family":"Mannor","given":"S."},{"family":"Hazan","given":"T."},{"family":"Tang","given":"H."},{"family":"Majumdar","given":"S."}],"citation-key":"khadkaOPTIMIZINGMEMORYPLACEMENT2021","event-title":"ICLR 2021 - 9th International Conference on Learning Representations","issued":{"date-parts":[["2021"]]},"title":"OPTIMIZING MEMORY PLACEMENT USING EVOLUTIONARY GRAPH REINFORCEMENT LEARNING","type":"paper-conference","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127890255&partnerID=40&md5=f72adf0441b1a557498e8c127fb4d814"},
  {"id":"khoslaDeepReinforcementLearning2020","abstract":"Deep RL approaches build much of their success on the ability of the deep neural network to generate useful internal representations. Nevertheless, they suffer from a high sample-complexity and starting with a good input representation can have a","accessed":{"date-parts":[["2023",11,20]]},"author":[{"family":"Khosla","given":"Megha"}],"citation-key":"khoslaDeepReinforcementLearning2020","issued":{"date-parts":[["2020",4,29]]},"source":"www.academia.edu","title":"Deep Reinforcement Learning with Graph-based State Representations","type":"article-journal","URL":"https://www.academia.edu/75453903/Deep_Reinforcement_Learning_with_Graph_based_State_Representations"},
  {"id":"kipfSemiSupervisedClassificationGraph2017","abstract":"We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.","accessed":{"date-parts":[["2023",11,6]]},"author":[{"family":"Kipf","given":"Thomas N."},{"family":"Welling","given":"Max"}],"citation-key":"kipfSemiSupervisedClassificationGraph2017","DOI":"10.48550/arXiv.1609.02907","issued":{"date-parts":[["2017",2,22]]},"number":"arXiv:1609.02907","publisher":"arXiv","source":"arXiv.org","title":"Semi-Supervised Classification with Graph Convolutional Networks","type":"article","URL":"http://arxiv.org/abs/1609.02907"},
  {"id":"koolAttentionLearnSolve2018","abstract":"The recently presented idea to learn heuristics for combinatorial optimization problems is promising as it can save costly development. However, to push this idea towards practical implementation, we need better models and better ways of training. We contribute in both directions: we propose a model based on attention layers with benefits over the Pointer Network and we show how to train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which we find is more efficient than using a value function. We significantly improve over recent learned heuristics for the Travelling Salesman Problem (TSP), getting close to optimal results for problems up to 100 nodes. With the same hyperparameters, we learn strong heuristics for two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP), outperforming a wide range of baselines and getting results close to highly optimized and specialized algorithms.","accessed":{"date-parts":[["2024",1,2]]},"author":[{"family":"Kool","given":"Wouter"},{"family":"Hoof","given":"Herke","non-dropping-particle":"van"},{"family":"Welling","given":"Max"}],"citation-key":"koolAttentionLearnSolve2018","container-title":"arXiv.org","issued":{"date-parts":[["2018",3,22]]},"language":"en","title":"Attention, Learn to Solve Routing Problems!","type":"webpage","URL":"https://arxiv.org/abs/1803.08475v3"},
  {"id":"krieselBriefIntroductionNeural2017","abstract":"A Brief Introduction to Neural Networks  Manuscript Download - Zeta2 Version  Filenames are subject to change. Thus, if you place links, please do so with this subpage as target.      Original version    eBookReader optimized   English   [PDF], 6.2MB, 244 pages","accessed":{"date-parts":[["2023",11,6]]},"author":[{"family":"Kriesel","given":"David"}],"citation-key":"krieselBriefIntroductionNeural2017","issued":{"date-parts":[["2017",9,5]]},"language":"en","section":"2014-12-18T12:57:47+01:00","title":"A Brief Introduction to Neural Networks","type":"book","URL":"https://www.dkriesel.com/en/science/neural_networks"},
  {"id":"kumarConceptsTechniquesGraph2023","author":[{"family":"Kumar","given":"Vinod"},{"family":"Rajput","given":"Dharmendra S."}],"citation-key":"kumarConceptsTechniquesGraph2023","issued":{"date-parts":[["2023"]]},"language":"en","source":"Zotero","title":"Concepts and Techniques of Graph Neural Networks","type":"book"},
  {"id":"kumarFundamentalsGraphGraph2023","abstract":"The vertices, which are also known as nodes or points, and the edges, which are responsible for connecting the vertices to one another, are the two primary components that make up a graph. Graph theory is the mathematical study of graphs, which are structures that are used to depict relations betwee...","accessed":{"date-parts":[["2023",12,19]]},"author":[{"family":"Kumar","given":"Vinod"},{"family":"Prajapati","given":"Himanshu"},{"family":"Ponnusamy","given":"Sasikala"}],"citation-key":"kumarFundamentalsGraphGraph2023","container-title":"Concepts and Techniques of Graph Neural Networks","DOI":"10.4018/978-1-6684-6903-3.ch001","ISBN":"978-1-66846-903-3","issued":{"date-parts":[["2023"]]},"language":"en","license":"Access limited to members","page":"1-18","publisher":"IGI Global","source":"www.igi-global.com","title":"Fundamentals of Graph for Graph Neural Network","type":"chapter","URL":"https://www.igi-global.com/chapter/fundamentals-of-graph-for-graph-neural-network/www.igi-global.com/chapter/fundamentals-of-graph-for-graph-neural-network/323818"},
  {"id":"leSelfAttentiveAssociativeMemory2020","archive":"Scopus","author":[{"family":"Le","given":"H."},{"family":"Tran","given":"T."},{"family":"Venkatesh","given":"S."}],"citation-key":"leSelfAttentiveAssociativeMemory2020","event-title":"37th International Conference on Machine Learning, ICML 2020","issued":{"date-parts":[["2020"]]},"page":"5638-5647","title":"Self-Attentive associative memory","type":"paper-conference","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105553834&partnerID=40&md5=c57d04bb4dcafb70b1dd8ed01c808722","volume":"PartF168147-8"},
  {"id":"leSelfAttentiveAssociativeMemory2020a","archive":"Scopus","author":[{"family":"Le","given":"H."},{"family":"Tran","given":"T."},{"family":"Venkatesh","given":"S."}],"citation-key":"leSelfAttentiveAssociativeMemory2020a","event-title":"37th International Conference on Machine Learning, ICML 2020","issued":{"date-parts":[["2020"]]},"page":"5638-5647","title":"Self-Attentive associative memory","type":"paper-conference","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105553834&partnerID=40&md5=c57d04bb4dcafb70b1dd8ed01c808722","volume":"PartF168147-8"},
  {"id":"liangAcceleratedPrimalDualPolicy2018","abstract":"Constrained Markov Decision Process (CMDP) is a natural framework for reinforcement learning tasks with safety constraints, where agents learn a policy that maximizes the long-term reward while satisfying the constraints on the long-term cost. A canonical approach for solving CMDPs is the primal-dual method which updates parameters in primal and dual spaces in turn. Existing methods for CMDPs only use on-policy data for dual updates, which results in sample inefficiency and slow convergence. In this paper, we propose a policy search method for CMDPs called Accelerated Primal-Dual Optimization (APDO), which incorporates an off-policy trained dual variable in the dual update procedure while updating the policy in primal space with on-policy likelihood ratio gradient. Experimental results on a simulated robot locomotion task show that APDO achieves better sample efficiency and faster convergence than state-of-the-art approaches for CMDPs.","accessed":{"date-parts":[["2023",11,16]]},"author":[{"family":"Liang","given":"Qingkai"},{"family":"Que","given":"Fanyu"},{"family":"Modiano","given":"Eytan"}],"citation-key":"liangAcceleratedPrimalDualPolicy2018","DOI":"10.48550/arXiv.1802.06480","issued":{"date-parts":[["2018",2,18]]},"number":"arXiv:1802.06480","publisher":"arXiv","source":"arXiv.org","title":"Accelerated Primal-Dual Policy Optimization for Safe Reinforcement Learning","type":"article","URL":"http://arxiv.org/abs/1802.06480"},
  {"id":"liGatedGraphSequence2016","abstract":"Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be described as abstract data structures. © ICLR 2016: San Juan, Puerto Rico. All Rights Reserved.","archive":"Scopus","author":[{"family":"Li","given":"Y."},{"family":"Zemel","given":"R."},{"family":"Brockschmidt","given":"M."},{"family":"Tarlow","given":"D."}],"citation-key":"liGatedGraphSequence2016","event-title":"4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings","issued":{"date-parts":[["2016"]]},"language":"English","source":"Scopus","title":"Gated graph sequence neural networks","type":"paper-conference"},
  {"id":"liGraphReinforcementLearningbased2022","abstract":"This paper studies the computational offloading of CNN inference in dynamic multi-access edge computing (MEC) networks. To address the uncertainties in communication time and Edge servers' available capacity, we use early-exit mechanism to terminate the computation earlier to meet the deadline of inference tasks. We design a reward function to trade off the communication, computation and inference accuracy, and formu-late the offloading problem of CNN inference as a maximization problem with the goal of maximizing the average inference accuracy and throughput in long term. To solve the maxi-mization problem, we propose a graph reinforcement learning-based early-exit mechanism (GRLE), which outperforms the state-of-the-art work, deep reinforcement learning-based online offloading (DROO) and its enhanced method, DROO with early-exit mechanism (DROOE), under different dynamic scenarios. The experimental results show that G RLE achieves the average accuracy up to 3.41 x over graph reinforcement learning (GRL) and 1.45x over DROOE, which shows the advantages of GRLE for offloading decision-making in dynamic MEC.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Li","given":"Nan"},{"family":"Iosifidis","given":"Alexandros"},{"family":"Zhang","given":"Qi"}],"citation-key":"liGraphReinforcementLearningbased2022","container-title":"GLOBECOM 2022 - 2022 IEEE Global Communications Conference","DOI":"10.1109/GLOBECOM48099.2022.10001067","event-title":"GLOBECOM 2022 - 2022 IEEE Global Communications Conference","issued":{"date-parts":[["2022",12]]},"page":"982-987","source":"IEEE Xplore","title":"Graph Reinforcement Learning-based CNN Inference Offloading in Dynamic Edge Computing","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/10001067"},
  {"id":"lillicrapContinuousControlDeep2019","abstract":"We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.","accessed":{"date-parts":[["2023",12,16]]},"author":[{"family":"Lillicrap","given":"Timothy P."},{"family":"Hunt","given":"Jonathan J."},{"family":"Pritzel","given":"Alexander"},{"family":"Heess","given":"Nicolas"},{"family":"Erez","given":"Tom"},{"family":"Tassa","given":"Yuval"},{"family":"Silver","given":"David"},{"family":"Wierstra","given":"Daan"}],"citation-key":"lillicrapContinuousControlDeep2019","DOI":"10.48550/arXiv.1509.02971","issued":{"date-parts":[["2019",7,5]]},"number":"arXiv:1509.02971","publisher":"arXiv","source":"arXiv.org","title":"Continuous control with deep reinforcement learning","type":"article","URL":"http://arxiv.org/abs/1509.02971"},
  {"id":"linEmergencyControlPower2022","abstract":"The current power grid topology changes frequently, and power angle instability and voltage instability often occur at the same time. Emergency control methods based on physical characteristics are usually difficult to model in complex power systems, and have poor adaptability to power system structural changes. The traditional analysis methods based on the physical characteristics of the power grid can no longer meet the requirements of power grid control, and new methods need to be found to solve this problem. In this paper, an emergency control method based on graph reinforcement learning (GRL) is proposed for the emergency control problem of AC/DC hybrid grid. The graph neural network is used to extract the environmental features, and the state space, action space and reward function of reinforcement learning are designed for the emergency control task. Combined with the transient stability analysis, the emergency control strategy framework is constructed, and the effective emergency control decision after failure is realized. The effectiveness of the method in the topology change scenario is verified.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Lin","given":"Hanxing"},{"family":"Chen","given":"Jinyu"},{"family":"Chen","given":"Wenxin"},{"family":"Chen","given":"Zihan"}],"citation-key":"linEmergencyControlPower2022","container-title":"2022 12th International Conference on Power and Energy Systems (ICPES)","DOI":"10.1109/ICPES56491.2022.10073160","event-title":"2022 12th International Conference on Power and Energy Systems (ICPES)","ISSN":"2767-732X","issued":{"date-parts":[["2022",12]]},"page":"498-503","source":"IEEE Xplore","title":"Emergency Control of Power Grid under Topology Changes Based on Graph Reinforcement Learning","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/10073160"},
  {"id":"linEmergencyControlPower2022a","archive":"Scopus","author":[{"family":"Lin","given":"H."},{"family":"Chen","given":"J."},{"family":"Chen","given":"W."},{"family":"Chen","given":"Z."}],"citation-key":"linEmergencyControlPower2022a","DOI":"10.1109/ICPES56491.2022.10073160","event-title":"2022 12th International Conference on Power and Energy Systems, ICPES 2022","issued":{"date-parts":[["2022"]]},"page":"498-503","title":"Emergency Control of Power Grid under Topology Changes Based on Graph Reinforcement Learning","type":"paper-conference","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152194683&doi=10.1109%2fICPES56491.2022.10073160&partnerID=40&md5=f53af9284bef6bed262e2e27bf2584fa"},
  {"id":"liNovelGraphReinforcement2022","abstract":"Due to the fast development of new-type power systems, the power grid is facing increasing uncertainties brought by high penetration of distributed generations. How to improve the decision quality of economic dispatch under such conditions becomes a very crucial task. Therefore, a novel graph reinforcement learning (GRL) approach for dynamic economic dispatch under high penetration of renewable energy is proposed. Compared with other reinforcement learning method, a novel graph-based representation of system state is adopted. Thus, the implicit correlations of uncertainties considering system topology can be more effectively captured. As a fully model-free method, the proposed methodology does not rely on the explicit models of physical system and uncertainty distributions. The asymptotic optimal policy can be obtained by continuous interaction with the environment. Case simulations illustrate that the proposed method achieves a better performance in terms of optimality and efficiency compared with existing learning methods.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Li","given":"Peng"},{"family":"Huang","given":"Wenqi"},{"family":"Dai","given":"Zhen"},{"family":"Hou","given":"Jiaxuan"},{"family":"Cao","given":"Shang"},{"family":"Zhang","given":"Jiayu"},{"family":"Chen","given":"Junbin"}],"citation-key":"liNovelGraphReinforcement2022","container-title":"2022 4th Asia Energy and Electrical Engineering Symposium (AEEES)","DOI":"10.1109/AEEES54426.2022.9759565","event-title":"2022 4th Asia Energy and Electrical Engineering Symposium (AEEES)","issued":{"date-parts":[["2022",3]]},"page":"498-503","source":"IEEE Xplore","title":"A Novel Graph Reinforcement Learning Approach for Stochastic Dynamic Economic Dispatch under High Penetration of Renewable Energy","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9759565"},
  {"id":"liuDecomposingSharedNetworks2023","abstract":"Sharing network parameters between agents is an essential and typical operation to improve the scalability of multi-agent reinforcement learning algorithms. However, agents with different tasks sharing the same network parameters are not conducive to distinguishing the agents' skills. In addition, the importance of communication between agents undertaking the same task is much higher than that with external agents. Therefore, we propose Dual Cooperation Networks (DCN). In order to distinguish whether agents undertake the same task, all agents are grouped according to their status through the graph neural network instead of the traditional proximity. The agent communicates within the group to achieve strong cooperation. After that, the global value function is decomposed by groups to facilitate cooperation between groups. Finally, we have verified it in simulation and physical hardware, and the algorithm has achieved excellent performance.","accessed":{"date-parts":[["2023",11,7]]},"author":[{"family":"Liu","given":"Weiwei"},{"family":"Peng","given":"Linpeng"},{"family":"Wen","given":"Licheng"},{"family":"Yang","given":"Jian"},{"family":"Liu","given":"Yong"}],"citation-key":"liuDecomposingSharedNetworks2023","container-title":"Information Sciences","container-title-short":"Information Sciences","DOI":"10.1016/j.ins.2023.119085","ISSN":"0020-0255","issued":{"date-parts":[["2023",9,1]]},"page":"119085","source":"ScienceDirect","title":"Decomposing shared networks for separate cooperation with multi-agent reinforcement learning","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0020025523006709","volume":"641"},
  {"id":"liuGraphConvolutionBasedDeep2022","abstract":"A reliable multi-agent decision-making system is highly demanded for safe and efficient operations of connected and autonomous vehicles (CAVs). In order to represent the mutual effects between vehicles and model the dynamic traffic environments, this research proposes an integrated and open-source framework to realize different Graph Reinforcement Learning (GRL) methods for better decision-making in interactive driving scenarios. Firstly, an interactive driving scenario on the highway with two ramps is constructed. The vehicles in this scenario are modeled by graph representation, and features are extracted via Graph Neural Network (GNN). Secondly, several GRL approaches are implemented and compared in detail. Finally, The simulation in the SUMO platform is carried out to evaluate the performance of different G RL approaches. Results are analyzed from multiple perspectives to compare the performance of different G RL methods in intelligent transportation scenarios. Experiments show that the implementation of GNN can well model the interactions between vehicles, and the proposed framework can improve the overall performance of multi-agent decision-making. The source code of our work can be found at https://github.com/Jacklinkk/TorchGRL.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Liu","given":"Qi"},{"family":"Li","given":"Zirui"},{"family":"Li","given":"Xueyuan"},{"family":"Wu","given":"Jingda"},{"family":"Yuan","given":"Shihua"}],"citation-key":"liuGraphConvolutionBasedDeep2022","container-title":"2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)","DOI":"10.1109/ITSC55140.2022.9922001","event-title":"2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)","issued":{"date-parts":[["2022",10]]},"page":"4074-4081","source":"IEEE Xplore","title":"Graph Convolution-Based Deep Reinforcement Learning for Multi-Agent Decision-Making in Interactive Traffic Scenarios","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9922001"},
  {"id":"liuGraphReinforcementLearning2022","abstract":"Proper functioning of connected and automated vehicles (CAVs) is crucial for the safety and efficiency of future intelligent transport systems. Meanwhile, transitioning to fully autonomous driving requires a long period of mixed autonomy traffic, including both CAVs and human-driven vehicles. Thus, collaboration decision-making for CAVs is essential to generate appropriate driving behaviors to enhance the safety and efficiency of mixed autonomy traffic. In recent years, deep reinforcement learning (DRL) has been widely used in solving decision-making problems. However, the existing DRL-based methods have been mainly focused on solving the decision-making of a single CAV. Using the existing DRL-based methods in mixed autonomy traffic cannot accurately represent the mutual effects of vehicles and model dynamic traffic environments. To address these shortcomings, this article proposes a graph reinforcement learning (GRL) approach for multi-agent decision-making of CAVs in mixed autonomy traffic. First, a generic and modular GRL framework is designed. Then, a systematic review of DRL and GRL methods is presented, focusing on the problems addressed in recent research. Moreover, a comparative study on different GRL methods is further proposed based on the designed framework to verify the effectiveness of GRL methods. Results show that the GRL methods can well optimize the performance of multi-agent decision-making for CAVs in mixed autonomy traffic compared to the DRL methods. Finally, challenges and future research directions are summarized. This study can provide a valuable research reference for solving the multi-agent decision-making problems of CAVs in mixed autonomy traffic and can promote the implementation of GRL-based methods into intelligent transportation systems. The source code of our work can be found at https://github.com/Jacklinkk/Graph_CAVs.","accessed":{"date-parts":[["2023",11,20]]},"author":[{"family":"Liu","given":"Qi"},{"family":"Li","given":"Xueyuan"},{"family":"Li","given":"Zirui"},{"family":"Wu","given":"Jingda"},{"family":"Du","given":"Guodong"},{"family":"Gao","given":"Xin"},{"family":"Yang","given":"Fan"},{"family":"Yuan","given":"Shihua"}],"citation-key":"liuGraphReinforcementLearning2022","DOI":"10.48550/arXiv.2211.03005","issued":{"date-parts":[["2022",11,5]]},"number":"arXiv:2211.03005","publisher":"arXiv","source":"arXiv.org","title":"Graph Reinforcement Learning Application to Co-operative Decision-Making in Mixed Autonomy Traffic: Framework, Survey, and Challenges","title-short":"Graph Reinforcement Learning Application to Co-operative Decision-Making in Mixed Autonomy Traffic","type":"article","URL":"http://arxiv.org/abs/2211.03005"},
  {"id":"liuGraphReinforcementLearningBased2023","abstract":"The proper functioning of connected and autonomous vehicles (CAVs) is crucial for the safety and efficiency of future intelligent transport systems. Meanwhile, transitioning to fully autonomous driving requires a long period of mixed autonomy traffic, including both CAVs and human-driven vehicles. Thus, collaborative decision-making technology for CAVs is essential to generate appropriate driving behaviors to enhance the safety and efficiency of mixed autonomy traffic. In recent years, deep reinforcement learning (DRL) methods have become an efficient way in solving decision-making problems. However, with the development of computing technology, graph reinforcement learning (GRL) methods have gradually demonstrated the large potential to further improve the decision-making performance of CAVs, especially in the area of accurately representing the mutual effects of vehicles and modeling dynamic traffic environments. To facilitate the development of GRL-based methods for autonomous driving, this paper proposes a review of GRL-based methods for the decision-making technologies of CAVs. Firstly, a generic GRL framework is proposed in the beginning to gain an overall understanding of the decision-making technology. Then, the GRL-based decision-making technologies are reviewed from the perspective of the construction methods of mixed autonomy traffic, methods for graph representation of the driving environment, and related works about graph neural networks (GNN) and DRL in the field of decision-making for autonomous driving. Moreover, validation methods are summarized to provide an efficient way to verify the performance of decision-making methods. Finally, challenges and future research directions of GRL-based decision-making methods are summarized.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Liu","given":"Qi"},{"family":"Li","given":"Xueyuan"},{"family":"Tang","given":"Yujie"},{"family":"Gao","given":"Xin"},{"family":"Yang","given":"Fan"},{"family":"Li","given":"Zirui"}],"citation-key":"liuGraphReinforcementLearningBased2023","container-title":"Sensors","DOI":"10.3390/s23198229","ISSN":"1424-8220","issue":"19","issued":{"date-parts":[["2023",1]]},"language":"en","license":"http://creativecommons.org/licenses/by/3.0/","number":"19","page":"8229","publisher":"Multidisciplinary Digital Publishing Institute","source":"www.mdpi.com","title":"Graph Reinforcement Learning-Based Decision-Making Technology for Connected and Autonomous Vehicles: Framework, Review, and Future Trends","title-short":"Graph Reinforcement Learning-Based Decision-Making Technology for Connected and Autonomous Vehicles","type":"article-journal","URL":"https://www.mdpi.com/1424-8220/23/19/8229","volume":"23"},
  {"id":"liuGraphReinforcementLearningBased2023a","archive":"Scopus","author":[{"family":"Liu","given":"Q."},{"family":"Li","given":"X."},{"family":"Tang","given":"Y."},{"family":"Gao","given":"X."},{"family":"Yang","given":"F."},{"family":"Li","given":"Z."}],"citation-key":"liuGraphReinforcementLearningBased2023a","container-title":"Sensors","DOI":"10.3390/s23198229","issue":"19","issued":{"date-parts":[["2023"]]},"title":"Graph Reinforcement Learning-Based Decision-Making Technology for Connected and Autonomous Vehicles: Framework, Review, and Future Trends","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174067583&doi=10.3390%2fs23198229&partnerID=40&md5=4afb9350c4e86fb123e60f9b2a05934b","volume":"23"},
  {"id":"liuIntroductionGraphNeural2020","accessed":{"date-parts":[["2023",12,10]]},"author":[{"family":"Liu","given":"Zhiyuan"},{"family":"Zhou","given":"Jie"}],"citation-key":"liuIntroductionGraphNeural2020","collection-title":"Synthesis Lectures on Artificial Intelligence and Machine Learning","DOI":"10.1007/978-3-031-01587-8","event-place":"Cham","ISBN":"978-3-031-00459-9 978-3-031-01587-8","issued":{"date-parts":[["2020"]]},"language":"en","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"Introduction to Graph Neural Networks","type":"book","URL":"https://link.springer.com/10.1007/978-3-031-01587-8"},
  {"id":"liuMultiAgentsInteractionApproach2022","abstract":"Due to the development of artificial intelligence technology, the research of unmanned equipment has gained great progress. However, how to make multi-agents in unmanned clusters to be reasonable for interaction is the key problem to be solved at present. And the characteristics of unmanned clusters can be derived from a variety of work modes. At the same time these mode are inseparable from the interaction and collaboration mechanism. Graph reinforcement learning is a new algorithm that combines graph neural network and reinforcement learning. This paper is based on graph reinforcement learning to solve the problem of interaction between various nodes in unmanned clusters. And the approach provides the technical guidance for the future development of unmanned cluster technology.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Liu","given":"Lian"},{"family":"Wang","given":"Zimeng"}],"citation-key":"liuMultiAgentsInteractionApproach2022","container-title":"2022 9th International Conference on Dependable Systems and Their Applications (DSA)","DOI":"10.1109/DSA56465.2022.00101","event-title":"2022 9th International Conference on Dependable Systems and Their Applications (DSA)","ISSN":"2767-6684","issued":{"date-parts":[["2022",8]]},"page":"710-715","source":"IEEE Xplore","title":"Multi-Agents Interaction Approach based on Graph Network and Reinforcement Learning","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9914496"},
  {"id":"liuNewMultidatadrivenSpatiotemporal2021","abstract":"Spatiotemporal PM2.5 forecasting technology plays an important role in urban traffic environment management and planning. In order to establish a satisfactory high-precision PM2.5 prediction model, a new multidata-driven spatiotemporal PM2.5 forecasting model is proposed in this paper. The overall modelling framework consists of three main parts. In part I, the graph convolutional network uses an adjacency matrix to effectively aggregate spatiotemporal pollutant data from different nodes and extract the most valuable feature information for target point modeling from the original data. In part II, the extracted feature information is used as the input of the gated recursive unit and the long short-term memory network to construct the prediction model. In part III, the Q-learning algorithm builds the best ensemble PM2.5 forecasting model by analyzing the processing ability and analysis ability of different predictors. Based on the analysis of multiple cases, the following conclusions can be drawn: (1) Graphic convolutional networks can effectively analyze the spatiotemporal correlation of PM2.5 data and achieve better performance than traditional convolutional neural networks. (2) Q-learning can adaptively optimize the ensemble weight coefficient and achieve better results than the traditional optimization algorithm. (3) The proposed GCN-LSTM-GRU-Q model can achieve better results than the 24 benchmark models.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Liu","given":"Xinwei"},{"family":"Qin","given":"Muchuan"},{"family":"He","given":"Yue"},{"family":"Mi","given":"Xiwei"},{"family":"Yu","given":"Chengqing"}],"citation-key":"liuNewMultidatadrivenSpatiotemporal2021","container-title":"Atmospheric Pollution Research","container-title-short":"Atmospheric Pollution Research","DOI":"10.1016/j.apr.2021.101197","ISSN":"1309-1042","issue":"10","issued":{"date-parts":[["2021",10,1]]},"page":"101197","source":"ScienceDirect","title":"A new multi-data-driven spatiotemporal PM2.5 forecasting model based on an ensemble graph reinforcement learning convolutional network","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S1309104221002622","volume":"12"},
  {"id":"liuOnlineMultiAgentReinforcement2021","abstract":"The distributed Volt/Var control (VVC) methods have been widely studied for active distribution networks(ADNs), which is based on perfect model and real-time P2P communication. However, the model is always incomplete with significant parameter errors and such P2P communication system is hard to maintain. In this article, we propose an online multi-agent reinforcement learning and decentralized control framework (OLDC) for VVC. In this framework, the VVC problem is formulated as a constrained Markov game and we propose a novel multi-agent constrained soft actor-critic (MACSAC) reinforcement learning algorithm. MACSAC is used to train the control agents online, so the accurate ADN model is no longer needed. Then, the trained agents can realize decentralized optimization using local measurements without real-time P2P communication. The OLDC with MACSAC has shown extraordinary flexibility, efficiency and robustness to various computing and communication conditions. Numerical simulations on IEEE test cases not only demonstrate that the proposed MACSAC outperforms the state-of-art learning algorithms, but also support the superiority of our OLDC framework in the online application.","accessed":{"date-parts":[["2023",11,17]]},"author":[{"family":"Liu","given":"Haotian"},{"family":"Wu","given":"Wenchuan"}],"citation-key":"liuOnlineMultiAgentReinforcement2021","container-title":"IEEE Transactions on Smart Grid","DOI":"10.1109/TSG.2021.3060027","ISSN":"1949-3061","issue":"4","issued":{"date-parts":[["2021",7]]},"page":"2980-2990","source":"IEEE Xplore","title":"Online Multi-Agent Reinforcement Learning for Decentralized Inverter-Based Volt-VAR Control","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9356806","volume":"12"},
  {"id":"luoMultiAgentCollaborativeExploration2019","abstract":"Autonomous exploration by a single or multiple agents in an unknown environment leads to various applications in automation, such as cleaning, search and rescue, etc. Traditional methods normally take frontier locations and segmented regions of the environment into account to efficiently allocate target locations to different agents to visit. They may employ ad hoc solutions to allocate the task to the agents, but the allocation may not be efficient. In the literature, few studies focused on enhancing the traditional methods by applying machine learning models for agent performance improvement. In this paper, we propose a graph-based deep reinforcement learning approach to effectively perform multi-agent exploration. Specifically, we first design a hierarchical map segmentation method to transform the environment exploration problem to the graph domain, wherein each node of the graph corresponds to a segmented region in the environment and each edge indicates the distance between two nodes. Subsequently, based on the graph structure, we apply a Graph Convolutional Network (GCN) to allocate the exploration target to each agent. Our experiments show that our proposed model significantly improves the efficiency of map explorations across varying sizes of collaborative agents over the traditional methods.","accessed":{"date-parts":[["2023",11,13]]},"author":[{"family":"Luo","given":"Tianze"},{"family":"Subagdja","given":"Budhitama"},{"family":"Wang","given":"Di"},{"family":"Tan","given":"Ah-Hwee"}],"citation-key":"luoMultiAgentCollaborativeExploration2019","container-title":"2019 IEEE International Conference on Agents (ICA)","DOI":"10.1109/AGENTS.2019.8929168","event-title":"2019 IEEE International Conference on Agents (ICA)","issued":{"date-parts":[["2019",10]]},"page":"2-7","source":"IEEE Xplore","title":"Multi-Agent Collaborative Exploration through Graph-based Deep Reinforcement Learning","type":"paper-conference","URL":"https://ieeexplore.ieee.org/abstract/document/8929168"},
  {"id":"luzMultiAgentDeepReinforcement2022","accessed":{"date-parts":[["2023",12,28]]},"author":[{"family":"Luz","given":"João Henrique Afonso Marques Reguengo","dropping-particle":"da"}],"citation-key":"luzMultiAgentDeepReinforcement2022","issued":{"date-parts":[["2022",7,15]]},"language":"eng","license":"openAccess","note":"Accepted: 2023-01-23T03:35:39Z","publisher":"FEUP","source":"repositorio-aberto.up.pt","title":"Multi-Agent Deep Reinforcement Learning for Resource Management in Earth Observation Satellites Constellations","type":"thesis","URL":"https://repositorio-aberto.up.pt/handle/10216/142722"},
  {"id":"lvGraphReinforcementLearningbased2023","archive":"Scopus","author":[{"family":"Lv","given":"W."},{"family":"Yang","given":"P."},{"family":"Zheng","given":"T."},{"family":"Lin","given":"C."},{"family":"Wang","given":"Z."},{"family":"Deng","given":"M."},{"family":"Wang","given":"Q."}],"citation-key":"lvGraphReinforcementLearningbased2023","container-title":"IEEE Internet of Things Journal","DOI":"10.1109/JIOT.2023.3289228","issued":{"date-parts":[["2023"]]},"page":"1-1","title":"Graph Reinforcement Learning-based Dependency-Aware Microservice Deployment in Edge Computing","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163549728&doi=10.1109%2fJIOT.2023.3289228&partnerID=40&md5=dad9a99cbe07c817adbb4059b5d22ec3"},
  {"id":"lvReinforcementLearningList2023","abstract":"Existing list recommendation methods present a list consisting of multiple items for feedback recommendation to user requests, which has the advantages of high flexibility and direct user feedback. However, the structured representation of state data limits the embedding of users and items, making them isolated from each other, missing some useful infomation for recommendation. In addition, the traditional non-end-to-end learning series takes a long time and accumulates errors. During the model training process, the results of each task can easily affect the next calculation, thus affecting the entire training effect. Aiming at the above problems, this paper proposes a Reinforcement Learning List Recommendation Model Fused with a Graph Neural Network, GNLR. The goal of this model is to maximize the recommendation effect while ensuring that the list recommendation system accurately analyzes user preferences to improve user experience. To this end, firstly, we use an user–item bipartite graph and Graph Neural Network to aggregate neighborhood information for users and items to generate graph structured representation; secondly, we adopt an attention mechanism to assign corresponding weights to neighborhood information to reduce the influence of noise nodes in heterogeneous information networks; finally, we alleviate the problems of traditional non-end-to-end methods through end-to-end training methods. The experimental results show that the method proposed in this paper can alleviate the above problems, and the recommendation hit rate and accuracy rate increase by about 10%.","accessed":{"date-parts":[["2023",11,14]]},"author":[{"family":"Lv","given":"Zhongming"},{"family":"Tong","given":"Xiangrong"}],"citation-key":"lvReinforcementLearningList2023","container-title":"Electronics","DOI":"10.3390/electronics12183748","ISSN":"2079-9292","issue":"18","issued":{"date-parts":[["2023",1]]},"language":"en","license":"http://creativecommons.org/licenses/by/3.0/","number":"18","page":"3748","publisher":"Multidisciplinary Digital Publishing Institute","source":"www.mdpi.com","title":"A Reinforcement Learning List Recommendation Model Fused with Graph Neural Networks","type":"article-journal","URL":"https://www.mdpi.com/2079-9292/12/18/3748","volume":"12"},
  {"id":"martinsLastMileDeliveryProblem2023","accessed":{"date-parts":[["2023",12,28]]},"author":[{"family":"Martins","given":"Clara Alves"}],"citation-key":"martinsLastMileDeliveryProblem2023","issued":{"date-parts":[["2023",7,24]]},"language":"eng","license":"openAccess","note":"Accepted: 2023-12-15T00:09:42Z","publisher":"FEUP","source":"repositorio-aberto.up.pt","title":"Last-Mile Delivery Problem Representation in Reinforcement Learning","type":"thesis","URL":"https://repositorio-aberto.up.pt/handle/10216/151971"},
  {"id":"mnihAsynchronousMethodsDeep2016","abstract":"Wc propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input. © 2016 by the author(s).","archive":"Scopus","author":[{"family":"Mnih","given":"V."},{"family":"Badia","given":"A.P."},{"family":"Mirza","given":"L."},{"family":"Graves","given":"A."},{"family":"Harley","given":"T."},{"family":"Lillicrap","given":"T.P."},{"family":"Silver","given":"D."},{"family":"Kavukcuoglu","given":"K."}],"citation-key":"mnihAsynchronousMethodsDeep2016","event-title":"33rd International Conference on Machine Learning, ICML 2016","ISBN":"978-1-5108-2900-8","issued":{"date-parts":[["2016"]]},"language":"English","page":"2850-2869","source":"Scopus","title":"Asynchronous methods for deep reinforcement learning","type":"paper-conference","volume":"4"},
  {"id":"mnihHumanlevelControlDeep2015","abstract":"An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.","accessed":{"date-parts":[["2024",1,5]]},"author":[{"family":"Mnih","given":"Volodymyr"},{"family":"Kavukcuoglu","given":"Koray"},{"family":"Silver","given":"David"},{"family":"Rusu","given":"Andrei A."},{"family":"Veness","given":"Joel"},{"family":"Bellemare","given":"Marc G."},{"family":"Graves","given":"Alex"},{"family":"Riedmiller","given":"Martin"},{"family":"Fidjeland","given":"Andreas K."},{"family":"Ostrovski","given":"Georg"},{"family":"Petersen","given":"Stig"},{"family":"Beattie","given":"Charles"},{"family":"Sadik","given":"Amir"},{"family":"Antonoglou","given":"Ioannis"},{"family":"King","given":"Helen"},{"family":"Kumaran","given":"Dharshan"},{"family":"Wierstra","given":"Daan"},{"family":"Legg","given":"Shane"},{"family":"Hassabis","given":"Demis"}],"citation-key":"mnihHumanlevelControlDeep2015","container-title":"Nature","DOI":"10.1038/nature14236","ISSN":"1476-4687","issue":"7540","issued":{"date-parts":[["2015",2]]},"language":"en","license":"2015 Springer Nature Limited","number":"7540","page":"529-533","publisher":"Nature Publishing Group","source":"www.nature.com","title":"Human-level control through deep reinforcement learning","type":"article-journal","URL":"https://www.nature.com/articles/nature14236","volume":"518"},
  {"id":"moralesGrokkingDeepReinforcement2020","abstract":"Grokking Deep Reinforcement Learning uses engaging exercises to teach you how to build deep learning systems. This book combines annotated Python code with intuitive explanations to explore DRL techniques. You'll see how algorithms function and learn to develop your own DRL agents using evaluative feedback","author":[{"family":"Morales","given":"Miguel"}],"citation-key":"moralesGrokkingDeepReinforcement2020","event-place":"Shelter Island [New York]","ISBN":"978-1-61729-545-4","issued":{"date-parts":[["2020"]]},"language":"eng","note":"OCLC: 1228577398","publisher":"Manning","publisher-place":"Shelter Island [New York]","source":"Open WorldCat","title":"Grokking Deep Reinforcement Learning","type":"book"},
  {"id":"morenoMultiAgentDeepReinforcement2020","accessed":{"date-parts":[["2023",11,20]]},"author":[{"family":"Moreno","given":"Gonçalo Vasconcelos Cunha Miranda"}],"citation-key":"morenoMultiAgentDeepReinforcement2020","issued":{"date-parts":[["2020",10,14]]},"language":"eng","license":"openAccess","note":"Accepted: 2022-10-14T23:08:05Z","source":"repositorio-aberto.up.pt","title":"Multi-Agent Deep Reinforcement Learning for autonomous driving","type":"article-journal","URL":"https://repositorio-aberto.up.pt/handle/10216/132006"},
  {"id":"moxleyVideoAnnotationSearch2010","archive":"Scopus","author":[{"family":"Moxley","given":"E."},{"family":"Mei","given":"T."},{"family":"Manjunath","given":"B.S."}],"citation-key":"moxleyVideoAnnotationSearch2010","container-title":"IEEE Transactions on Multimedia","DOI":"10.1109/TMM.2010.2041101","issue":"3","issued":{"date-parts":[["2010"]]},"page":"184-193","title":"Video annotation through search and graph reinforcement mining","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949676020&doi=10.1109%2fTMM.2010.2041101&partnerID=40&md5=40dc9c7359b3a167e775851aea9ef93b","volume":"12"},
  {"id":"nicaBriefIntroductionSpectral2018","accessed":{"date-parts":[["2023",12,24]]},"author":[{"family":"Nica","given":"Bogdan"}],"citation-key":"nicaBriefIntroductionSpectral2018","DOI":"10.4171/188","edition":"1","ISBN":"978-3-03719-188-0 978-3-03719-688-5","issued":{"date-parts":[["2018",5,30]]},"language":"en","publisher":"EMS Press","source":"DOI.org (Crossref)","title":"A Brief Introduction to Spectral Graph Theory","type":"book","URL":"https://ems.press/doi/10.4171/188"},
  {"id":"nieReinforcementLearningGraphs2023","abstract":"Graph mining tasks arise from many different application domains, including social networks, biological networks, transportation, and E-commerce, which have been receiving great attention from the theoretical and algorithmic design communities in recent years, and there has been some pioneering work employing the research-rich Reinforcement Learning (RL) techniques to address graph mining tasks. However, these fusion works are dispersed in different research domains, which makes them difficult to compare. In this survey, we provide a comprehensive overview of these fusion works and generalize these works to Graph Reinforcement Learning (GRL) as a unified formulation. We further discuss the applications of GRL methods across various domains, and simultaneously propose the key challenges and advantages of integrating graph mining and RL methods. Furthermore, we propose important directions and challenges to be solved in the future. To our knowledge, this is the latest work on a comprehensive survey of GRL, this work provides a global view and a learning resource for scholars. Based on our review, we create a collection of papers for both interested scholars who want to enter this rapidly developing domain and experts who would like to compare GRL methods.","accessed":{"date-parts":[["2023",11,27]]},"author":[{"family":"Nie","given":"Mingshuo"},{"family":"Chen","given":"Dongming"},{"family":"Wang","given":"Dongqi"}],"citation-key":"nieReinforcementLearningGraphs2023","container-title":"IEEE Transactions on Emerging Topics in Computational Intelligence","DOI":"10.1109/TETCI.2022.3222545","ISSN":"2471-285X","issue":"4","issued":{"date-parts":[["2023",8]]},"page":"1065-1082","source":"IEEE Xplore","title":"Reinforcement Learning on Graphs: A Survey","title-short":"Reinforcement Learning on Graphs","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/10121200","volume":"7"},
  {"id":"openaiSpinningDocumentation","accessed":{"date-parts":[["2023",12,26]]},"author":[{"family":"OpenAI","given":""}],"citation-key":"openaiSpinningDocumentation","title":"Spinning Up Documentation","type":"webpage","URL":"https://spinningup.openai.com/en/latest/index.html"},
  {"id":"panCalibrationDynamicVolumeDelay2022","abstract":"Volume-delay functions (VDF) are the critical building block in static traffic assignment and general demand-supply analysis. This paper aims to provide a rolling horizon-based modeling framework to establish and further calibrate dynamic VDF (DVDF) for a corridor. The development and application of classic VDF in recent traffic planning studies are first reviewed. Analytical formulas based on a rolling horizon framework are then developed to redefine critical elements in the Bureau of Public Roads (BPR) function to capture the time-dependent volume–delay relationship. Time-dependent average demand and discharge rate are used in a real-world bottleneck in oversaturated conditions. By constructing an estimate or approximate for the dynamic degree of saturation, the proposed method could (i) better interpret the underlying mechanism of the time-dependent demand–delay function; (ii) provide a valuable tool to estimate the speed for a time rolling horizon with given real-time data in practice, and (iii) analyze the correlation between a bottleneck and upstream or downstream on a road network to acquire accurate discharge rates for different situations. Experiments using corridors in Beijing and Los Angeles demonstrate that the proposed dynamic analytical methods can outperform the traditional BPR function in dynamic congestion cases. The results improve the DVDF goodness-of-fit from \nR2\n of 44% to 87% under different conditions, which sheds more light on future online traffic simulation applications.","accessed":{"date-parts":[["2024",1,1]]},"author":[{"family":"Pan","given":"Yuyan"},{"family":"Guo","given":"Jifu"},{"family":"Chen","given":"Yanyan"}],"citation-key":"panCalibrationDynamicVolumeDelay2022","container-title":"Transportation Research Record","DOI":"10.1177/03611981211044727","ISSN":"0361-1981","issue":"2","issued":{"date-parts":[["2022",2,1]]},"language":"en","page":"606-620","publisher":"SAGE Publications Inc","source":"SAGE Journals","title":"Calibration of Dynamic Volume-Delay Functions: A Rolling Horizon-Based Parsimonious Modeling Perspective","title-short":"Calibration of Dynamic Volume-Delay Functions","type":"article-journal","URL":"https://doi.org/10.1177/03611981211044727","volume":"2676"},
  {"id":"paulEfficientPlanningMultiRobot2023","abstract":"Efficient multi-robot task allocation (MRTA) is fundamental to various time-sensitive applications such as disaster response, warehouse operations, and construction. This paper tackles a particular class of these problems that we call MRTA-collective transport or MRTA-CT - here tasks present varying workloads and deadlines, and robots are subject to flight range, communication range, and payload constraints. For large instances of these problems involving 100s-1000's of tasks and 10s-100s of robots, traditional non-learning solvers are often time-inefficient, and emerging learning-based policies do not scale well to larger-sized problems without costly retraining. To address this gap, we use a recently proposed encoder-decoder graph neural network involving Capsule networks and multi-head attention mechanism, and innovatively add topological descriptors (TD) as new features to improve transferability to unseen problems of similar and larger size. Persistent homology is used to derive the TD, and proximal policy optimization is used to train our TD-augmented graph neural network. The resulting policy model compares favorably to state-of-the-art non-learning baselines while being much faster. The benefit of using TD is readily evident when scaling to test problems of size larger than those used in training.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Paul","given":"Steve"},{"family":"Li","given":"Wenyuan"},{"family":"Smyth","given":"Brian"},{"family":"Chen","given":"Yuzhou"},{"family":"Gel","given":"Yulia"},{"family":"Chowdhury","given":"Souma"}],"citation-key":"paulEfficientPlanningMultiRobot2023","container-title":"2023 IEEE International Conference on Robotics and Automation (ICRA)","DOI":"10.1109/ICRA48891.2023.10161517","event-title":"2023 IEEE International Conference on Robotics and Automation (ICRA)","issued":{"date-parts":[["2023",5]]},"page":"5779-5785","source":"IEEE Xplore","title":"Efficient Planning of Multi-Robot Collective Transport using Graph Reinforcement Learning with Higher Order Topological Abstraction","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/10161517"},
  {"id":"peiEmergencyControlStrategy2023","abstract":"Undervoltage load shedding (UVLS) is the last line of defense to ensure the safe and stable operation of the power system. The existing UVLS technique has difﬁculty adapting and generalizing to new topology variation scenarios of the power network, which greatly affects the reliability of the control strategy. This paper proposes a UVLS emergency control scheme based on a graph deep reinforcement learning method named GraphSAGE-D3QN (graph sample and aggregate-double dueling deep q network). During ofﬂine training, a GraphSAGE-based feature extraction mechanism of the power grid with topology variation is designed that can better capture the changes in system state characteristics. A novel reinforcement learning framework based on D3QN is developed for UVLS modeling, which reduces overestimations of control action and leads to a better control effect. Then, online emergency decision-making is achieved. The simulation results on the modiﬁed IEEE 39-bus system and IEEE 300-bus power system show that the proposed UVLS scheme can always provide more economical and reliable control strategies for power networks with topology variations and achieves better beneﬁts in both adaptability and generalization performances for previously unseen topology variation scenarios.","accessed":{"date-parts":[["2023",11,13]]},"author":[{"family":"Pei","given":"Yangzhou"},{"family":"Yang","given":"Jun"},{"family":"Wang","given":"Jundong"},{"family":"Xu","given":"Peidong"},{"family":"Zhou","given":"Ting"},{"family":"Wu","given":"Fuzhang"}],"citation-key":"peiEmergencyControlStrategy2023","container-title":"IET Generation, Transmission & Distribution","container-title-short":"IET Generation Trans &amp; Dist","DOI":"10.1049/gtd2.12795","ISSN":"1751-8687, 1751-8695","issue":"9","issued":{"date-parts":[["2023",5]]},"language":"en","page":"2130-2141","source":"DOI.org (Crossref)","title":"An emergency control strategy for undervoltage load shedding of power system: A graph deep reinforcement learning method","title-short":"An emergency control strategy for undervoltage load shedding of power system","type":"article-journal","URL":"https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/gtd2.12795","volume":"17"},
  {"id":"peiEmergencyControlStrategy2023a","archive":"Scopus","author":[{"family":"Pei","given":"Y."},{"family":"Yang","given":"J."},{"family":"Wang","given":"J."},{"family":"Xu","given":"P."},{"family":"Zhou","given":"T."},{"family":"Wu","given":"F."}],"citation-key":"peiEmergencyControlStrategy2023a","container-title":"IET Generation, Transmission and Distribution","DOI":"10.1049/gtd2.12795","issue":"9","issued":{"date-parts":[["2023"]]},"page":"2130-2141","title":"An emergency control strategy for undervoltage load shedding of power system: A graph deep reinforcement learning method","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148514246&doi=10.1049%2fgtd2.12795&partnerID=40&md5=4a07220d0fb11754feef1c88a5108615","volume":"17"},
  {"id":"qianDeepReinforcementLearning2020","abstract":"A coordinated operation of smart grid (SG) and intelligent transportation system (ITS) provides electric vehicle (EV) owners with a myriad of power and transportation network data for EV charging navigation. However, the optimal charging navigation would be a challenging task owing to the randomness of traffic conditions, charging prices and waiting time at EV charging station (EVCS). In this paper, we propose a deep reinforcement learning (DRL)-based EV charging navigation, aiming at minimizing the total travel time and the charging cost at EVCS. First, we utilize the deterministic shortest charging route model (DSCRM) to extract feature states out of collected stochastic data and then formulate EV charging navigation as a Markov Decision Process (MDP) with an unknown transition probability. The proposed DRL-based approach will approximate the solution, which can adaptively learn the optimal strategy without any prior knowledge of uncertainties. Case studies are carried out within a practical zone in Xi'an city, China. Numerous experimental results verity the effectiveness of the proposed approach and illustrate its adaptation to EV driver preferences. The coordination effect of SG and ITS on reducing the waiting time and the charging cost in EV charging navigations is also analyzed.","accessed":{"date-parts":[["2023",12,18]]},"author":[{"family":"Qian","given":"Tao"},{"family":"Shao","given":"Chengcheng"},{"family":"Wang","given":"Xiuli"},{"family":"Shahidehpour","given":"Mohammad"}],"citation-key":"qianDeepReinforcementLearning2020","container-title":"IEEE Transactions on Smart Grid","DOI":"10.1109/TSG.2019.2942593","ISSN":"1949-3061","issue":"2","issued":{"date-parts":[["2020",3]]},"page":"1714-1723","source":"IEEE Xplore","title":"Deep Reinforcement Learning for EV Charging Navigation by Coordinating Smart Grid and Intelligent Transportation System","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/8845652","volume":"11"},
  {"id":"qiSpatialtemporalEvolutionMechanism2023","archive":"Scopus","author":[{"family":"Qi","given":"G."},{"family":"Wei","given":"W."},{"family":"Wang","given":"Z."},{"family":"Wang","given":"Z."},{"family":"Wei","given":"L."}],"citation-key":"qiSpatialtemporalEvolutionMechanism2023","container-title":"Journal of Environmental Management","DOI":"10.1016/j.jenvman.2022.116671","issued":{"date-parts":[["2023"]]},"title":"The spatial-temporal evolution mechanism of PM2.5 concentration based on China's climate zoning","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141265767&doi=10.1016%2fj.jenvman.2022.116671&partnerID=40&md5=d4dec691c54e6e44ea756e130db8bcb7","volume":"325"},
  {"id":"qiuRecalibrationBPRFunction2022","abstract":"This paper assesses the adequacy of the BPR volume delay function for the strategic modelling of Connected and Autonomous Vehicles (CAVs). Three testbed environments are simulated at 10% increments of CAV penetration rates (CPR) to observe network performance in mixed fleet environments. The microsimulation dataset is compared with the BPR travel time predictions to evaluate the need for recalibration. Where appropriate, the BPR modelling parameters are redefined as a function of the CPR. The predictive quality of the recalibrated model is then validated by comparing it against the BPR function on synthetic data. The numerical results indicate an overall improvement in travel time prediction using the recalibrated model, with a significant reduction in root mean square error from 15.16 to 8.86. The recalibrated model also outperformed the traditional BPR model in 67% of the 4620 cases used for validation, and better-predicted travel time by 5.43 times.","accessed":{"date-parts":[["2024",1,1]]},"author":[{"family":"Qiu","given":"Esta"},{"family":"Virdi","given":"Navreet"},{"family":"Grzybowska","given":"Hanna"},{"family":"Waller","given":"Travis"}],"citation-key":"qiuRecalibrationBPRFunction2022","container-title":"Transportmetrica B: Transport Dynamics","DOI":"10.1080/21680566.2022.2040063","ISSN":"2168-0566","issue":"1","issued":{"date-parts":[["2022",12,31]]},"page":"779-800","publisher":"Taylor & Francis","source":"Taylor and Francis+NEJM","title":"Recalibration of the BPR function for the strategic modelling of connected and autonomous vehicles","type":"article-journal","URL":"https://doi.org/10.1080/21680566.2022.2040063","volume":"10"},
  {"id":"RenewablesSupplied882023","abstract":"Renewable utilities supplied 88% of Portugal's electricity consumption in January, as heavy rains coupled with good wind and solar conditions allowed to sharply reduce the use of gas-fired power plants, grid operator REN said on Wednesday.","accessed":{"date-parts":[["2023",11,25]]},"citation-key":"RenewablesSupplied882023","container-title":"Reuters","issued":{"date-parts":[["2023",2,1]]},"language":"en","section":"Americas","source":"www.reuters.com","title":"Renewables supplied 88% of Portugal's electricity consumption in January","type":"article-newspaper","URL":"https://www.reuters.com/world/americas/renewables-supplied-88-portugals-electricity-consumption-january-2023-02-01/"},
  {"id":"renSchedulingUAVSwarm2023","archive":"Scopus","author":[{"family":"Ren","given":"J."},{"family":"Xu","given":"Y."},{"family":"Li","given":"Z."},{"family":"Hong","given":"C."},{"family":"Zhang","given":"X.-P."},{"family":"Chen","given":"X."}],"citation-key":"renSchedulingUAVSwarm2023","DOI":"10.1145/3594739.3612905","event-title":"UbiComp/ISWC 2023 Adjunct - Adjunct Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2023 ACM International Symposium on Wearable Computing","issued":{"date-parts":[["2023"]]},"page":"670-675","title":"Scheduling UAV Swarm with Attention-based Graph Reinforcement Learning for Ground-to-air Heterogeneous Data Communication","type":"paper-conference","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175484837&doi=10.1145%2f3594739.3612905&partnerID=40&md5=3c6759444a0553f95537bc25fa5d3b70"},
  {"id":"ruckinAdaptiveInformativePath2022","archive":"Scopus","author":[{"family":"Ruckin","given":"J."},{"family":"Jin","given":"L."},{"family":"Popovic","given":"M."}],"citation-key":"ruckinAdaptiveInformativePath2022","DOI":"10.1109/ICRA46639.2022.9812025","event-title":"Proceedings - IEEE International Conference on Robotics and Automation","issued":{"date-parts":[["2022"]]},"page":"4473-4479","title":"Adaptive Informative Path Planning Using Deep Reinforcement Learning for UAV-based Active Sensing","type":"paper-conference","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136329963&doi=10.1109%2fICRA46639.2022.9812025&partnerID=40&md5=ccbd3620a257a1436c148932586a0d19"},
  {"id":"scarselliGraphNeuralNetwork2009","abstract":"Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.","accessed":{"date-parts":[["2023",10,31]]},"author":[{"family":"Scarselli","given":"Franco"},{"family":"Gori","given":"Marco"},{"family":"Tsoi","given":"Ah Chung"},{"family":"Hagenbuchner","given":"Markus"},{"family":"Monfardini","given":"Gabriele"}],"citation-key":"scarselliGraphNeuralNetwork2009","container-title":"IEEE Transactions on Neural Networks","DOI":"10.1109/TNN.2008.2005605","ISSN":"1941-0093","issue":"1","issued":{"date-parts":[["2009",1]]},"page":"61-80","source":"IEEE Xplore","title":"The Graph Neural Network Model","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/4700287","volume":"20"},
  {"id":"schulmanProximalPolicyOptimization2017","abstract":"We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.","accessed":{"date-parts":[["2023",12,16]]},"author":[{"family":"Schulman","given":"John"},{"family":"Wolski","given":"Filip"},{"family":"Dhariwal","given":"Prafulla"},{"family":"Radford","given":"Alec"},{"family":"Klimov","given":"Oleg"}],"citation-key":"schulmanProximalPolicyOptimization2017","DOI":"10.48550/arXiv.1707.06347","issued":{"date-parts":[["2017",8,28]]},"number":"arXiv:1707.06347","publisher":"arXiv","source":"arXiv.org","title":"Proximal Policy Optimization Algorithms","type":"article","URL":"http://arxiv.org/abs/1707.06347"},
  {"id":"shakyaReinforcementLearningAlgorithms2023","abstract":"Reinforcement Learning (RL) is a machine learning (ML) technique to learn sequential decision-making in complex problems. RL is inspired by trial-and-error based human/animal learning. It can learn an optimal policy autonomously with knowledge obtained by continuous interaction with a stochastic dynamical environment. Problems considered virtually impossible to solve, such as learning to play video games just from pixel information, are now successfully solved using deep reinforcement learning. Without human intervention, RL agents can surpass human performance in challenging tasks. This review gives a broad overview of RL, covering its fundamental principles, essential methods, and illustrative applications. The authors aim to develop an initial reference point for researchers commencing their research work in RL. In this review, the authors cover some fundamental model-free RL algorithms and pathbreaking function approximation-based deep RL (DRL) algorithms for complex uncertain tasks with continuous action and state spaces, making RL useful in various interdisciplinary fields. This article also provides a brief review of model-based and multi-agent RL approaches. Finally, some promising research directions for RL are briefly presented.","accessed":{"date-parts":[["2023",11,14]]},"author":[{"family":"Shakya","given":"Ashish Kumar"},{"family":"Pillai","given":"Gopinatha"},{"family":"Chakrabarty","given":"Sohom"}],"citation-key":"shakyaReinforcementLearningAlgorithms2023","container-title":"Expert Systems with Applications","container-title-short":"Expert Systems with Applications","DOI":"10.1016/j.eswa.2023.120495","ISSN":"0957-4174","issued":{"date-parts":[["2023",11,30]]},"page":"120495","source":"ScienceDirect","title":"Reinforcement learning algorithms: A brief survey","title-short":"Reinforcement learning algorithms","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0957417423009971","volume":"231"},
  {"id":"shangNewEnsembleDeep2022","abstract":"Spatio-temporal traffic volume forecasting technologies can effectively improve freeway traffic efficiency and the travel comfort of humans. To construct a high-precision traffic volume forecasting model, this study proposed a new ensemble deep graph reinforcement learning network. The modeling process of the spatio-temporal prediction model mainly included three steps. In step I, raw spatiotemporal traffic network datasets (traffic volumes, traffic speeds, weather, and holidays) were preprocessed and the adjacency matrix was constructed. In step II, a graph attention network (GAT) and graph convolution network (GCN) were used as the main predictors to build the spatio-temporal traffic volume forecasting model and obtain the forecasting results, respectively. In step III, deep reinforcement learning was used to effectively analyze the correlations between the forecasting results from these two neural networks and the final results, so as to optimize the weight coefficient. The final result of the proposed model was obtained by combining the forecasting results from the GAT and GCN with the weight coefficient. Based on summarizing and analyzing the experimental results, it can be concluded that: (1) deep reinforcement learning can effectively integrate the two different graph neural networks and achieve better results than traditional ensemble methods; and (2) the presented ensemble model performs better than twenty-one models proposed by other researchers for all studied cases.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Shang","given":"Pan"},{"family":"Liu","given":"Xinwei"},{"family":"Yu","given":"Chengqing"},{"family":"Yan","given":"Guangxi"},{"family":"Xiang","given":"Qingqing"},{"family":"Mi","given":"Xiwei"}],"citation-key":"shangNewEnsembleDeep2022","container-title":"Digital Signal Processing","container-title-short":"Digital Signal Processing","DOI":"10.1016/j.dsp.2022.103419","ISSN":"1051-2004","issued":{"date-parts":[["2022",4,30]]},"page":"103419","source":"ScienceDirect","title":"A new ensemble deep graph reinforcement learning network for spatio-temporal traffic volume forecasting in a freeway network","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S1051200422000367","volume":"123"},
  {"id":"shangNewEnsembleDeep2022a","archive":"Scopus","author":[{"family":"Shang","given":"P."},{"family":"Liu","given":"X."},{"family":"Yu","given":"C."},{"family":"Yan","given":"G."},{"family":"Xiang","given":"Q."},{"family":"Mi","given":"X."}],"citation-key":"shangNewEnsembleDeep2022a","container-title":"Digital Signal Processing: A Review Journal","DOI":"10.1016/j.dsp.2022.103419","issued":{"date-parts":[["2022"]]},"title":"A new ensemble deep graph reinforcement learning network for spatio-temporal traffic volume forecasting in a freeway network","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124316298&doi=10.1016%2fj.dsp.2022.103419&partnerID=40&md5=eca0123f18faba677bf8421240de8d0b","volume":"123"},
  {"id":"silverMasteringChessShogi2017","abstract":"The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.","accessed":{"date-parts":[["2023",12,30]]},"author":[{"family":"Silver","given":"David"},{"family":"Hubert","given":"Thomas"},{"family":"Schrittwieser","given":"Julian"},{"family":"Antonoglou","given":"Ioannis"},{"family":"Lai","given":"Matthew"},{"family":"Guez","given":"Arthur"},{"family":"Lanctot","given":"Marc"},{"family":"Sifre","given":"Laurent"},{"family":"Kumaran","given":"Dharshan"},{"family":"Graepel","given":"Thore"},{"family":"Lillicrap","given":"Timothy"},{"family":"Simonyan","given":"Karen"},{"family":"Hassabis","given":"Demis"}],"citation-key":"silverMasteringChessShogi2017","DOI":"10.48550/arXiv.1712.01815","issued":{"date-parts":[["2017",12,5]]},"number":"arXiv:1712.01815","publisher":"arXiv","source":"arXiv.org","title":"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm","type":"article","URL":"http://arxiv.org/abs/1712.01815"},
  {"id":"suEvolutionStrategiesbasedOptimized2023","abstract":"The job shop scheduling problem (JSSP) with dynamic events and uncertainty is a strongly NP-hard combinatorial optimization problem (COP) with extensive applications in the manufacturing system. Recently, growing interest has been aroused in utilizing machine learning techniques to solve the JSSP. However, most prior arts cannot handle dynamic events and barely consider uncertainties. To close this gap, this paper proposes a framework to solve a dynamic JSSP (DJSP) with machine breakdown and stochastic processing time based on Graph Neural Network (GNN) and deep reinforcement learning (DRL). To this end, we first formulate the DJSP as a Markov Decision Process (MDP), where disjunctive graph represent the states. Secondly, we propose a GNN-based model to effectively extract the embeddings of the state by considering the features of the dynamic events and the stochasticity of the problem, e.g., the machine breakdown and stochastic processing time. Then, the model constructs solutions by dispatching optimal operations to machines based on the learned embeddings. Notably, we propose to use the evolution strategies (ES) to find optimal policies that are more stable and robust than conventional DRL algorithms. The extensive experiments show that our method substantially outperforms existing reinforcement learning-based and traditional methods on multiple classic benchmarks.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Su","given":"Chupeng"},{"family":"Zhang","given":"Cong"},{"family":"Xia","given":"Dan"},{"family":"Han","given":"Baoan"},{"family":"Wang","given":"Chuang"},{"family":"Chen","given":"Gang"},{"family":"Xie","given":"Longhan"}],"citation-key":"suEvolutionStrategiesbasedOptimized2023","container-title":"Applied Soft Computing","container-title-short":"Applied Soft Computing","DOI":"10.1016/j.asoc.2023.110596","ISSN":"1568-4946","issued":{"date-parts":[["2023",9,1]]},"page":"110596","source":"ScienceDirect","title":"Evolution strategies-based optimized graph reinforcement learning for solving dynamic job shop scheduling problem","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S1568494623006142","volume":"145"},
  {"id":"suEvolutionStrategiesbasedOptimized2023a","archive":"Scopus","author":[{"family":"Su","given":"C."},{"family":"Zhang","given":"C."},{"family":"Xia","given":"D."},{"family":"Han","given":"B."},{"family":"Wang","given":"C."},{"family":"Chen","given":"G."},{"family":"Xie","given":"L."}],"citation-key":"suEvolutionStrategiesbasedOptimized2023a","container-title":"Applied Soft Computing","DOI":"10.1016/j.asoc.2023.110596","issued":{"date-parts":[["2023"]]},"title":"Evolution strategies-based optimized graph reinforcement learning for solving dynamic job shop scheduling problem","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165540331&doi=10.1016%2fj.asoc.2023.110596&partnerID=40&md5=5d05b509305f35472706f68b37e5bb21","volume":"145"},
  {"id":"sukurAutomatedProgramImprovement2023","archive":"Scopus","author":[{"family":"Sukur","given":"N."},{"family":"Milošević","given":"N."},{"family":"Pracner","given":"D."},{"family":"Budimac","given":"Z."}],"citation-key":"sukurAutomatedProgramImprovement2023","container-title":"Soft Computing","DOI":"10.1007/s00500-023-08559-1","issued":{"date-parts":[["2023"]]},"title":"Automated program improvement with reinforcement learning and graph neural networks","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160861647&doi=10.1007%2fs00500-023-08559-1&partnerID=40&md5=8f31b665e0a0762d68a81c3d84e68699"},
  {"id":"sunGraphReinforcementLearningBasedTaskOffloading2023","abstract":"Network applications involve massive heterogeneous data fusion and analysis. Artificial intelligence can significantly improve the convenience and user experience, but it requires a lot of storage, bandwidth, and computing resources. Multiaccess edge computing (MEC) extends intelligence services to IoT devices through offloading approaches and joint processing, which solves the resource bottleneck. However, designing advanced collaboration technology to offload tasks to MEC servers is still challenging. Heuristic algorithms and deep reinforcement learning (DRL)-based approaches have been proposed to offload tasks and minimize application latency. However, heuristic algorithms heavily depend on accurate mathematical models for the MEC system, and DRL does not make fair use of the relationship between devices in the MEC graph. To solve this, we propose a task offloading mechanism based on graph neural network (GNN), which can directly learn on graph data with messages passing and aggregation. We propose a graph reinforcement learning-based offloading (GRLO) framework, which models MEC as an acyclic graph and the offloading policy by graph state migration. GRLO combines GNN with the actor-critic network and trains offloading decision makers without labels. To efficiently train the GRLO, we propose a method that quickly explores action space and approaches the optimal solution. The numerical results show that the GRLO has lower latency compared to baselines while having generalization ability to new environments and topologies. Moreover, we verified the effectiveness of GRLO on a prototype.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Sun","given":"Zhenchuan"},{"family":"Mo","given":"Yijun"},{"family":"Yu","given":"Chen"}],"citation-key":"sunGraphReinforcementLearningBasedTaskOffloading2023","container-title":"IEEE Internet of Things Journal","DOI":"10.1109/JIOT.2021.3123822","ISSN":"2327-4662","issue":"4","issued":{"date-parts":[["2023",2]]},"page":"3138-3150","source":"IEEE Xplore","title":"Graph-Reinforcement-Learning-Based Task Offloading for Multiaccess Edge Computing","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9592681","volume":"10"},
  {"id":"suttonMDPsSemiMDPsFramework1999","abstract":"Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options—closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options deﬁned over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macroutility problem. © 1999 Published by Elsevier Science B.V. All rights reserved.","accessed":{"date-parts":[["2023",9,23]]},"author":[{"family":"Sutton","given":"Richard S."},{"family":"Precup","given":"Doina"},{"family":"Singh","given":"Satinder"}],"citation-key":"suttonMDPsSemiMDPsFramework1999","container-title":"Artificial Intelligence","container-title-short":"Artificial Intelligence","DOI":"10.1016/S0004-3702(99)00052-1","ISSN":"00043702","issue":"1-2","issued":{"date-parts":[["1999",8]]},"language":"en","page":"181-211","source":"DOI.org (Crossref)","title":"Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning","title-short":"Between MDPs and semi-MDPs","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S0004370299000521","volume":"112"},
  {"id":"suttonReinforcementLearningIntroduction2014","author":[{"family":"Sutton","given":"Richard S."},{"family":"Barto","given":"Andrew"}],"citation-key":"suttonReinforcementLearningIntroduction2014","collection-title":"Adaptive computation and machine learning","edition":"Nachdruck","event-place":"Cambridge, Massachusetts","ISBN":"978-0-262-19398-6","issued":{"date-parts":[["2014"]]},"language":"en","number-of-pages":"322","publisher":"The MIT Press","publisher-place":"Cambridge, Massachusetts","source":"K10plus ISBN","title":"Reinforcement learning: an introduction","title-short":"Reinforcement learning","type":"book"},
  {"id":"traskGrokkingDeepLearning2019","abstract":"\"Grokking Deep Learning teaches you to build deep learning neural networks from scratch! In his engaging style, seasoned deep learning expert Andrew Trask shows you the science under the hood, so you grok for yourself every detail of training neural networks. Using only Python and its math-supporting library, NumPy, you'll train your own neural networks to see and understand images, translate text into different languages, and even write like Shakespeare!\"--","author":[{"family":"Trask","given":"Andrew W."}],"call-number":"QA76.87 .T73 2019","citation-key":"traskGrokkingDeepLearning2019","event-place":"Shelter Island","ISBN":"978-1-61729-370-2","issued":{"date-parts":[["2019"]]},"note":"OCLC: on1084981313","number-of-pages":"309","publisher":"Manning","publisher-place":"Shelter Island","source":"Library of Congress ISBN","title":"Grokking Deep Learning","type":"book"},
  {"id":"vaswaniAttentionAllYou2017","abstract":"The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.","accessed":{"date-parts":[["2024",1,2]]},"author":[{"family":"Vaswani","given":"Ashish"},{"family":"Shazeer","given":"Noam"},{"family":"Parmar","given":"Niki"},{"family":"Uszkoreit","given":"Jakob"},{"family":"Jones","given":"Llion"},{"family":"Gomez","given":"Aidan N"},{"family":"Kaiser","given":"Łukasz"},{"family":"Polosukhin","given":"Illia"}],"citation-key":"vaswaniAttentionAllYou2017","container-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[["2017"]]},"publisher":"Curran Associates, Inc.","source":"Neural Information Processing Systems","title":"Attention is All you Need","type":"paper-conference","URL":"https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html","volume":"30"},
  {"id":"velickovicGraphAttentionNetworks2018","abstract":"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of computationally intensive matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).","accessed":{"date-parts":[["2023",10,31]]},"author":[{"family":"Veličković","given":"Petar"},{"family":"Cucurull","given":"Guillem"},{"family":"Casanova","given":"Arantxa"},{"family":"Romero","given":"Adriana"},{"family":"Liò","given":"Pietro"},{"family":"Bengio","given":"Yoshua"}],"citation-key":"velickovicGraphAttentionNetworks2018","event-title":"International Conference on Learning Representations","issued":{"date-parts":[["2018",2,15]]},"language":"en","source":"openreview.net","title":"Graph Attention Networks","type":"paper-conference","URL":"https://openreview.net/forum?id=rJXMpikCZ"},
  {"id":"vijayapriyaSmartGridOverview2011","abstract":"This paper briefly discusses evolution of Smart Grid development. Smart Grid is important as it will take us towards energy independence and environmentally sustainable economic growth. Growth of Smart Power Grid in India will slowly but surely take us towards fulfilling the dreams of former President Dr. A.P.J. Abdul Kalam, “Energy for all and Energy forever”.","accessed":{"date-parts":[["2023",11,20]]},"author":[{"family":"Vijayapriya","given":"Tamilmaran"},{"family":"Kothari","given":"Dwarkadas Pralhadas"}],"citation-key":"vijayapriyaSmartGridOverview2011","container-title":"Smart Grid and Renewable Energy","container-title-short":"SGRE","DOI":"10.4236/sgre.2011.24035","ISSN":"2151-481X, 2151-4844","issue":"04","issued":{"date-parts":[["2011"]]},"language":"en","page":"305-311","source":"DOI.org (Crossref)","title":"Smart Grid: An Overview","title-short":"Smart Grid","type":"article-journal","URL":"http://www.scirp.org/journal/doi.aspx?DOI=10.4236/sgre.2011.24035","volume":"02"},
  {"id":"wangArtificialIntelligenceThings2023","archive":"Scopus","author":[{"family":"Wang","given":"Y."},{"family":"Zhang","given":"B."},{"family":"Ma","given":"J."},{"family":"Jin","given":"Q."}],"citation-key":"wangArtificialIntelligenceThings2023","container-title":"Concurrency and Computation: Practice and Experience","DOI":"10.1002/cpe.7827","issue":"23","issued":{"date-parts":[["2023"]]},"title":"Artificial intelligence of things (AIoT) data acquisition based on graph neural networks: A systematical review","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161507161&doi=10.1002%2fcpe.7827&partnerID=40&md5=db0b1006ddf322b9a3bcf486b3175205","volume":"35"},
  {"id":"wangCacheAidedMECIoT2023","abstract":"With the growing demand for latency-sensitive and compute-intensive services in the Internet of Things (IoT), multiaccess edge computing (MEC)-enabled IoT is envisioned as a promising technique that allows network nodes to have computing and caching capabilities. In this article, we propose a cache-aided MEC (CA-MEC) offloading framework for joint optimization of communication, computing, and caching (3C) resources in the MEC-enabled IoT. Our goal is to optimize the offloading decision and resource allocation strategy to minimize the system latency subject to dynamic cache capacities and computing resource constraints. We first formulate this optimization problem as a multiagent decision problem, a partially observable Markov decision process (POMDP). Then, the deep graph convolution reinforcement learning (DGRL) method is applied to motivate the agents to learn optimal strategies cooperatively in a highly dynamic environment. Simulations show that our method is highly effective for computation offloading and resource allocation and performs superior results in a large-scale network.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Wang","given":"Dan"},{"family":"Bai","given":"Yalu"},{"family":"Huang","given":"Gang"},{"family":"Song","given":"Bin"},{"family":"Yu","given":"F. Richard"}],"citation-key":"wangCacheAidedMECIoT2023","container-title":"IEEE Internet of Things Journal","DOI":"10.1109/JIOT.2023.3244909","ISSN":"2327-4662","issue":"13","issued":{"date-parts":[["2023",7]]},"page":"11486-11496","source":"IEEE Xplore","title":"Cache-Aided MEC for IoT: Resource Allocation Using Deep Graph Reinforcement Learning","title-short":"Cache-Aided MEC for IoT","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/10044184","volume":"10"},
  {"id":"wangGCNRLCircuitDesigner2020","abstract":"Automatic transistor sizing is a challenging problem in circuit design due to the large design space, complex performance tradeoffs, and fast technology advancements. Although there have been plenty of work on transistor sizing targeting on one circuit, limited research has been done on transferring the knowledge from one circuit to another to reduce the re-design overhead. In this paper, we present GCN-RL Circuit Designer, leveraging reinforcement learning (RL) to transfer the knowledge between different technology nodes and topologies. Moreover, inspired by the simple fact that circuit is a graph, we learn on the circuit topology representation with graph convolutional neural networks (GCN). The GCN-RL agent extracts features of the topology graph whose vertices are transistors, edges are wires. Our learning-based optimization consistently achieves the highest Figures of Merit (FoM) on four different circuits compared with conventional black box optimization methods (Bayesian Optimization, Evolutionary Algorithms), random search and human expert designs. Experiments on transfer learning between five technology nodes and two circuit topologies demonstrate that RL with transfer learning can achieve much higher FoMs than methods without knowledge transfer. Our transferable optimization method makes transistor sizing and design porting more effective and efficient.","accessed":{"date-parts":[["2023",12,18]]},"author":[{"family":"Wang","given":"Hanrui"},{"family":"Wang","given":"Kuan"},{"family":"Yang","given":"Jiacheng"},{"family":"Shen","given":"Linxiao"},{"family":"Sun","given":"Nan"},{"family":"Lee","given":"Hae-Seung"},{"family":"Han","given":"Song"}],"citation-key":"wangGCNRLCircuitDesigner2020","container-title":"2020 57th ACM/IEEE Design Automation Conference (DAC)","DOI":"10.1109/DAC18072.2020.9218757","event-title":"2020 57th ACM/IEEE Design Automation Conference (DAC)","ISSN":"0738-100X","issued":{"date-parts":[["2020",7]]},"page":"1-6","source":"IEEE Xplore","title":"GCN-RL Circuit Designer: Transferable Transistor Sizing with Graph Neural Networks and Reinforcement Learning","title-short":"GCN-RL Circuit Designer","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9218757"},
  {"id":"waradpandeGraphbasedStateRepresentation2021","abstract":"Deep RL approaches build much of their success on the ability of the deep neural network to generate useful internal representations. Nevertheless, they suffer from a high sample-complexity and starting with a good input representation can have a significant impact on the performance. In this paper, we exploit the fact that the underlying Markov decision process (MDP) represents a graph, which enables us to incorporate the topological information for effective state representation learning. Motivated by the recent success of node representations for several graph analytical tasks we specifically investigate the capability of node representation learning methods to effectively encode the topology of the underlying MDP in Deep RL. To this end we perform a comparative analysis of several models chosen from 4 different classes of representation learning algorithms for policy learning in grid-world navigation tasks, which are representative of a large class of RL problems. We find that all embedding methods outperform the commonly used matrix representation of grid-world environments in all of the studied cases. Moreoever, graph convolution based methods are outperformed by simpler random walk based methods and graph linear autoencoders.","accessed":{"date-parts":[["2023",12,17]]},"author":[{"family":"Waradpande","given":"Vikram"},{"family":"Kudenko","given":"Daniel"},{"family":"Khosla","given":"Megha"}],"citation-key":"waradpandeGraphbasedStateRepresentation2021","DOI":"10.48550/arXiv.2004.13965","issued":{"date-parts":[["2021",2,16]]},"number":"arXiv:2004.13965","publisher":"arXiv","source":"arXiv.org","title":"Graph-based State Representation for Deep Reinforcement Learning","type":"article","URL":"http://arxiv.org/abs/2004.13965"},
  {"id":"weiGraphLearningIts2023","abstract":"Graph learning aims to learn complex relationships among nodes and the topological structure of graphs, such as social networks, academic networks and e-commerce networks, which are common in the real world. Those relationships make graphs special compared with traditional tabular data in which nodes are dependent on non-Euclidean space and contain rich information to explore. Graph learning developed from graph theory to graph data mining and now is empowered with representation learning, making it achieve great performances in various scenarios, even including text, image, chemistry, and biology. Due to the broad application prospects in the real world, graph learning has become a popular and promising area in machine learning. Thousands of works have been proposed to solve various kinds of problems in graph learning and is appealing more and more attention in academic community, which makes it pivotal to survey previous valuable works. Although some of the researchers have noticed this phenomenon and ﬁnished impressive surveys on graph learning. However, they failed to link related objectives, methods and applications in a more logical way and cover current ample scenarios as well as challenging problems due to the rapid expansion of the graph learning.","accessed":{"date-parts":[["2023",11,17]]},"author":[{"family":"Wei","given":"Shaopeng"},{"family":"Zhao","given":"Yu"},{"family":"Chen","given":"Xingyan"},{"family":"Li","given":"Qing"},{"family":"Zhuang","given":"Fuzhen"},{"family":"Liu","given":"Ji"},{"family":"Kou","given":"Gang"}],"citation-key":"weiGraphLearningIts2023","issued":{"date-parts":[["2023",6,3]]},"language":"en","number":"arXiv:2212.08966","publisher":"arXiv","source":"arXiv.org","title":"Graph Learning and Its Applications: A Holistic Survey","title-short":"Graph Learning and Its Applications","type":"article","URL":"http://arxiv.org/abs/2212.08966"},
  {"id":"wuComprehensiveSurveyGraph2021","abstract":"Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial-temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.","accessed":{"date-parts":[["2023",12,17]]},"author":[{"family":"Wu","given":"Zonghan"},{"family":"Pan","given":"Shirui"},{"family":"Chen","given":"Fengwen"},{"family":"Long","given":"Guodong"},{"family":"Zhang","given":"Chengqi"},{"family":"Yu","given":"Philip S."}],"citation-key":"wuComprehensiveSurveyGraph2021","container-title":"IEEE Transactions on Neural Networks and Learning Systems","DOI":"10.1109/TNNLS.2020.2978386","ISSN":"2162-2388","issue":"1","issued":{"date-parts":[["2021",1]]},"page":"4-24","source":"IEEE Xplore","title":"A Comprehensive Survey on Graph Neural Networks","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9046288","volume":"32"},
  {"id":"wuGraphNeuralNetworks2022","accessed":{"date-parts":[["2023",11,28]]},"citation-key":"wuGraphNeuralNetworks2022","DOI":"10.1007/978-981-16-6054-2","editor":[{"family":"Wu","given":"Lingfei"},{"family":"Cui","given":"Peng"},{"family":"Pei","given":"Jian"},{"family":"Zhao","given":"Liang"}],"event-place":"Singapore","ISBN":"9789811660535 9789811660542","issued":{"date-parts":[["2022"]]},"language":"en","publisher":"Springer Nature Singapore","publisher-place":"Singapore","source":"DOI.org (Crossref)","title":"Graph Neural Networks: Foundations, Frontiers, and Applications","title-short":"Graph Neural Networks","type":"book","URL":"https://link.springer.com/10.1007/978-981-16-6054-2"},
  {"id":"wuModelingSimulationTraffic2022","abstract":"Connected automated vehicles (CAVs) can significantly shorten the headway of car following, thereby effectively improving the traffic capacity and injecting new power to alleviate traffic congestion. To investigate the congestion characteristics of mixed traffic flow with CAVs and human-driven vehicles (HDVs), this paper proposes a cell transmission model to capture and simulate traffic congestion for mixed traffic flow. Firstly, the Newell, adaptive cruise control (ACC), and cooperative adaptive cruise control (CACC) models are adopted to capture the car-following behavior of different vehicles. Secondly, the fundamental diagram under different penetration rates of CAVs is derived based on car-following models. Then, the cell transmission model (CTM) of mixed traffic flow is developed based on the classical CTM and fundamental diagram of mixed traffic flow. Finally, two simulation methods, mixed traffic flow CTM and micro-simulation, are designed to verify the effectiveness of the proposed model. Moreover, taking the moving bottleneck on the expressway as an example, the congestion characteristics of mixed traffic flow are analyzed using multiple indexes, such as average travel speed, congestion delay, and congestion scale. The results show the following: (i) CAVs can significantly alleviate traffic congestion, (ii) the duration of the bottleneck is positively correlated with the degree of traffic congestion, and (iii) The traffic congestion assessment results under different model parameters slightly differ, but the impact is negligible.","accessed":{"date-parts":[["2024",1,1]]},"author":[{"family":"Wu","given":"Yunxia"},{"family":"Lin","given":"Yalan"},{"family":"Hu","given":"Rong"},{"family":"Wang","given":"Zilan"},{"family":"Zhao","given":"Bin"},{"family":"Yao","given":"Zhihong"}],"citation-key":"wuModelingSimulationTraffic2022","container-title":"Journal of Advanced Transportation","DOI":"10.1155/2022/8348726","ISSN":"0197-6729","issued":{"date-parts":[["2022",6,27]]},"language":"en","page":"e8348726","publisher":"Hindawi","source":"www.hindawi.com","title":"Modeling and Simulation of Traffic Congestion for Mixed Traffic Flow with Connected Automated Vehicles: A Cell Transmission Model Approach","title-short":"Modeling and Simulation of Traffic Congestion for Mixed Traffic Flow with Connected Automated Vehicles","type":"article-journal","URL":"https://www.hindawi.com/journals/jat/2022/8348726/","volume":"2022"},
  {"id":"xingBilevelGraphReinforcement2023","abstract":"This letter proposes a bilevel graph reinforcement learning method for electric vehicle (EV) fleet charging guidance, achieving collaborative optimization of the transportation-electrification coupled system. A dual-agent architecture is first constructed, where the upper-level is used for charging and the lower-level is used for routing. The EV traveling and charging behavior is characterized as a graph-structured interaction process. A graph attention network (GAT) is leveraged to extract the topology correlation and feature information. Then the extracted topology embedded with knowledge, as intermediate latent environment states, is fed into the underlying network of deep reinforcement learning (DRL). A DRL-based sequential scheduling pattern is developed to realize the guidance of multiple EVs. Extensive experimental results verify the superiority and adaptability of our proposed methodology.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Xing","given":"Qiang"},{"family":"Xu","given":"Yan"},{"family":"Chen","given":"Zhong"}],"citation-key":"xingBilevelGraphReinforcement2023","container-title":"IEEE Transactions on Smart Grid","DOI":"10.1109/TSG.2023.3240580","ISSN":"1949-3061","issue":"4","issued":{"date-parts":[["2023",7]]},"page":"3309-3312","source":"IEEE Xplore","title":"A Bilevel Graph Reinforcement Learning Method for Electric Vehicle Fleet Charging Guidance","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/10029881","volume":"14"},
  {"id":"xingBilevelGraphReinforcement2023a","archive":"Scopus","author":[{"family":"Xing","given":"Q."},{"family":"Xu","given":"Y."},{"family":"Chen","given":"Z."}],"citation-key":"xingBilevelGraphReinforcement2023a","container-title":"IEEE Transactions on Smart Grid","DOI":"10.1109/TSG.2023.3240580","issue":"4","issued":{"date-parts":[["2023"]]},"page":"3309-3312","title":"A Bilevel Graph Reinforcement Learning Method for Electric Vehicle Fleet Charging Guidance","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148470544&doi=10.1109%2fTSG.2023.3240580&partnerID=40&md5=abe594d7b1eaf72d9aa8d1e78cfcd858","volume":"14"},
  {"id":"xingGraphReinforcementLearningBased2023","abstract":"To provide efficient charging behavior decision-making for urban electric vehicles (EVs), this article proposes a new platform for real-time EV charging navigation (EVCN) based on graph reinforcement learning. Considering the interaction of EVs with charging stations (CSs) and traffic networks, the navigation goal of the “vehicle-station-network” coupled system is to minimize the charging cost and traveling time of EV owners. Specifically, to realize data acquisition and decision-making output, we first characterize the EV charging and traveling behavior as the dynamic interaction process of graph-structured networks. A graph convolutional network is used to extract the environment information required for EVCN, and the generated environment feature is fed into the underlying network of deep reinforcement learning (DRL), which can help the agent better understand massive graph-structured data. Then, the real-time navigation problem is duly formulated as a finite Markov decision process. A sequential scheduling pattern is built according to the sorting of EV charging urgency and solved by a Rainbow-based DRL algorithm. It achieves the sequential recommendation of CSs and planning of traveling routes for multiple EVs. Case studies are conducted within a practical zone in Nanjing, China. Simulation results verify the developed platform and the solving method.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Xing","given":"Qiang"},{"family":"Xu","given":"Yan"},{"family":"Chen","given":"Zhong"},{"family":"Zhang","given":"Ziqi"},{"family":"Shi","given":"Zhao"}],"citation-key":"xingGraphReinforcementLearningBased2023","container-title":"IEEE Transactions on Industrial Informatics","DOI":"10.1109/TII.2022.3210264","ISSN":"1941-0050","issue":"3","issued":{"date-parts":[["2023",3]]},"page":"3284-3295","source":"IEEE Xplore","title":"A Graph Reinforcement Learning-Based Decision-Making Platform for Real-Time Charging Navigation of Urban Electric Vehicles","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9906438?denied=","volume":"19"},
  {"id":"xingGraphReinforcementLearningBased2023a","archive":"Scopus","author":[{"family":"Xing","given":"Q."},{"family":"Xu","given":"Y."},{"family":"Chen","given":"Z."},{"family":"Zhang","given":"Z."},{"family":"Shi","given":"Z."}],"citation-key":"xingGraphReinforcementLearningBased2023a","container-title":"IEEE Transactions on Industrial Informatics","DOI":"10.1109/TII.2022.3210264","issue":"3","issued":{"date-parts":[["2023"]]},"page":"3284-3295","title":"A Graph Reinforcement Learning-Based Decision-Making Platform for Real-Time Charging Navigation of Urban Electric Vehicles","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139466803&doi=10.1109%2fTII.2022.3210264&partnerID=40&md5=e29e3f2990f2304e7ef1ee97457d25eb","volume":"19"},
  {"id":"xingModellingDrivingCharging2021","abstract":"With the popularization and promotion of electric vehicles (EVs), their interactions with power grids and traffic networks have increasingly deepened. Accurate modelling of EV behaviour can faithfully depict the characteristics of EV driving and charging. However, most existing modelling researches fail to adopt real-world travel data and consider realistic perceptual decision-making psychology of owners. Thus, this paper proposes a novel behavioural modelling for EVs based on a data-driven approach combined with behavioural economics theory. To characterize the driving behaviour of owners using actual data, a systematic data mining and modelling approach is firstly proposed based on the open-source ‘Didi’ traffic travel data set, which obtains the traffic operation rules and the regenerative behaviour characteristics data. According to the subjective perceptual characteristics of social economic man, a Cumulative Prospect Theory-based modelling framework is further developed to quantify the uncertain and stochastic charging decision-making behaviour of EV users. Moreover, the user's preferences and attitudes are evaluated by calculating their cumulative prospect value when choosing charging stations. Finally, the most suitable charging station is recommended for EVs with charging requirements. Case studies are conducted within a practical zone in Nanjing, China. The results demonstrate that the traffic travel rules of vehicle owners have typical date types and functional area distribution characteristics. And the travel time and space of private and commercial vehicles are relatively regular, whereas the travel rules of public vehicles are random. Besides, this proposed methodology can not only effectively capture the irrational decision-making characteristics of EV users' charging behaviour, but also achieve promising performance in terms of reducing the charging waiting cost. The user's decision-making regarding charging behaviour exhibits a higher risk-seeking preference than a risk-aversion preference.","accessed":{"date-parts":[["2023",12,18]]},"author":[{"family":"Xing","given":"Qiang"},{"family":"Chen","given":"Zhong"},{"family":"Zhang","given":"Ziqi"},{"family":"Wang","given":"Ruisheng"},{"family":"Zhang","given":"Tian"}],"citation-key":"xingModellingDrivingCharging2021","container-title":"Journal of Cleaner Production","container-title-short":"Journal of Cleaner Production","DOI":"10.1016/j.jclepro.2021.129243","ISSN":"0959-6526","issued":{"date-parts":[["2021",11,15]]},"page":"129243","source":"ScienceDirect","title":"Modelling driving and charging behaviours of electric vehicles using a data-driven approach combined with behavioural economics theory","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0959652621034296","volume":"324"},
  {"id":"xingRealtimeOptimalScheduling2023","abstract":"To improve the economy and safety of the active distribution network (ADN) operation, this paper proposes a real-time optimal scheduling strategy based on graph reinforcement learning (GRL), achieving online collaborative optimization of controllable equipment. For realizing the perception of inborn graph-structured features of the ADN and the formulation of the real-time scheduling scheme, the active-reactive power coordination process is firstly characterized as a dynamic graph-structured network interaction. A graph attention network (GAT) block is leveraged to extract and aggregate the topology branch correlation and node power information. Then the obtained topology information embedded with node features, as intermediate latent environment states, is fed into the underlying network architecture of deep reinforcement learning (DRL). Moreover, the power regulation problem of controllable equipment is duly formulated as a finite Markov decision process (FMDP). And the complex decision-making is solved by a deep deterministic policy gradient (DDPG) algorithm. Specially, under the electrical fault scenarios with variations in topology structures and feature information, GRL helps the agent efficiently learn and capture new graph-structured knowledge. Case studies are conducted within a modified IEEE 33-bus system. Extensive experimental results verify the superiority and adaptability of our proposed methodology.","accessed":{"date-parts":[["2023",11,20]]},"author":[{"family":"Xing","given":"Qiang"},{"family":"Chen","given":"Zhong"},{"family":"Zhang","given":"Tian"},{"family":"Li","given":"Xu"},{"family":"Sun","given":"KeHui"}],"citation-key":"xingRealtimeOptimalScheduling2023","container-title":"International Journal of Electrical Power & Energy Systems","container-title-short":"International Journal of Electrical Power & Energy Systems","DOI":"10.1016/j.ijepes.2022.108637","ISSN":"0142-0615","issued":{"date-parts":[["2023",2,1]]},"page":"108637","source":"ScienceDirect","title":"Real-time optimal scheduling for active distribution networks: A graph reinforcement learning method","title-short":"Real-time optimal scheduling for active distribution networks","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0142061522006330","volume":"145"},
  {"id":"xingRealtimeOptimalScheduling2023a","archive":"Scopus","author":[{"family":"Xing","given":"Q."},{"family":"Chen","given":"Z."},{"family":"Zhang","given":"T."},{"family":"Li","given":"X."},{"family":"Sun","given":"K."}],"citation-key":"xingRealtimeOptimalScheduling2023a","container-title":"International Journal of Electrical Power and Energy Systems","DOI":"10.1016/j.ijepes.2022.108637","issued":{"date-parts":[["2023"]]},"title":"Real-time optimal scheduling for active distribution networks: A graph reinforcement learning method","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138474191&doi=10.1016%2fj.ijepes.2022.108637&partnerID=40&md5=59257124b25f702ce0d0755f87044875","volume":"145"},
  {"id":"xuCombatDecisionSupport2022","archive":"Scopus","author":[{"family":"Xu","given":"B."},{"family":"Zeng","given":"W."}],"citation-key":"xuCombatDecisionSupport2022","DOI":"10.1109/CCDC55256.2022.10033986","event-title":"Proceedings of the 34th Chinese Control and Decision Conference, CCDC 2022","issued":{"date-parts":[["2022"]]},"page":"4872-4878","title":"A Combat Decision Support Method Based on OODA and Dynamic Graph Reinforcement Learning","type":"paper-conference","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149510611&doi=10.1109%2fCCDC55256.2022.10033986&partnerID=40&md5=e9cc7171010dfb4fdac9013330c122c3"},
  {"id":"xuGraphReinforcementLearning2024","archive":"Scopus","author":[{"family":"Xu","given":"J."},{"family":"Wang","given":"Y."},{"family":"Zhang","given":"B."},{"family":"Ma","given":"J."}],"citation-key":"xuGraphReinforcementLearning2024","container-title":"Future Generation Computer Systems","DOI":"10.1016/j.future.2023.09.017","issued":{"date-parts":[["2024"]]},"page":"412-423","title":"A Graph reinforcement learning based SDN routing path selection for optimizing long-term revenue","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172230739&doi=10.1016%2fj.future.2023.09.017&partnerID=40&md5=f8b7196c671bdce75435dd34c2324df4","volume":"150"},
  {"id":"xuHowPowerfulAre2019","abstract":"Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.","accessed":{"date-parts":[["2023",12,17]]},"author":[{"family":"Xu","given":"Keyulu"},{"family":"Hu","given":"Weihua"},{"family":"Leskovec","given":"Jure"},{"family":"Jegelka","given":"Stefanie"}],"citation-key":"xuHowPowerfulAre2019","DOI":"10.48550/arXiv.1810.00826","issued":{"date-parts":[["2019",2,22]]},"number":"arXiv:1810.00826","publisher":"arXiv","source":"arXiv.org","title":"How Powerful are Graph Neural Networks?","type":"article","URL":"http://arxiv.org/abs/1810.00826"},
  {"id":"xuRealtimeFastCharging2022","abstract":"With the increasing penetration rate of electric vehicles, the fast charging demands of electric vehicles will have a significant influence on the operation of coupled power-transportation networks. To promote the interests of the coupled system, fast charging stations, and electric vehicle users, in this paper, a multi-objective system-level fast charging station recommendation method is proposed to dynamically allocate electric vehicles to suitable stations. The recommendation problem is formulated as a sequential decision-making problem and a deep reinforcement learning method is adopted. To deal with the network-structure coupled system states, graph attention networks are introduced. Considering the heterogeneity between entities, we propose a physical connection-based graph formulation method with feature projection to integrate multi-dimensional information from charging stations, traffic nodes, and power grid buses into a graph. The graph convolution of coupled system states can then be realized to promote environment perception. Besides, to address the long time-delay action execution in recommendation problem, a double-prioritized DQN(λ) training mechanism is developed to update the guidance strategy, where an attention-prioritized cache construction method is proposed to improve the training efficiency cooperated with prioritized experience replay. The proposed graph reinforcement learning method is trained and evaluated in a joint power-transportation simulation platform. Simulation results show that the proposed strategy can promote the interest of multiple facets in coupled power-transportation networks by handling the requests in a real-time manner. Its feasibility and robustness in the urban transportation systems are also demonstrated.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Xu","given":"Peidong"},{"family":"Zhang","given":"Jun"},{"family":"Gao","given":"Tianlu"},{"family":"Chen","given":"Siyuan"},{"family":"Wang","given":"Xiaohui"},{"family":"Jiang","given":"Huaiguang"},{"family":"Gao","given":"Wenzhong"}],"citation-key":"xuRealtimeFastCharging2022","container-title":"International Journal of Electrical Power & Energy Systems","container-title-short":"International Journal of Electrical Power & Energy Systems","DOI":"10.1016/j.ijepes.2022.108030","ISSN":"0142-0615","issued":{"date-parts":[["2022",10,1]]},"page":"108030","source":"ScienceDirect","title":"Real-time fast charging station recommendation for electric vehicles in coupled power-transportation networks: A graph reinforcement learning method","title-short":"Real-time fast charging station recommendation for electric vehicles in coupled power-transportation networks","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0142061522000746","volume":"141"},
  {"id":"xuSimulationconstraintGraphReinforcement2020","archive":"Scopus","author":[{"family":"Xu","given":"P."},{"family":"Pei","given":"Y."},{"family":"Zheng","given":"X."},{"family":"Zhang","given":"J."}],"citation-key":"xuSimulationconstraintGraphReinforcement2020","DOI":"10.1109/EI250167.2020.9347305","event-title":"2020 IEEE 4th Conference on Energy Internet and Energy System Integration: Connecting the Grids Towards a Low-Carbon High-Efficiency Energy System, EI2 2020","issued":{"date-parts":[["2020"]]},"page":"319-324","title":"A simulation-constraint graph reinforcement learning method for line flow control","type":"paper-conference","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101608430&doi=10.1109%2fEI250167.2020.9347305&partnerID=40&md5=9ecf76ad468e09085ffb09c2e4854af0"},
  {"id":"xuSynthesisGadoliniumenhancedLiver2021","archive":"Scopus","author":[{"family":"Xu","given":"C."},{"family":"Zhang","given":"D."},{"family":"Chong","given":"J."},{"family":"Chen","given":"B."},{"family":"Li","given":"S."}],"citation-key":"xuSynthesisGadoliniumenhancedLiver2021","container-title":"Medical Image Analysis","DOI":"10.1016/j.media.2021.101976","issued":{"date-parts":[["2021"]]},"title":"Synthesis of gadolinium-enhanced liver tumors on nonenhanced liver MR images using pixel-level graph reinforcement learning","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100055324&doi=10.1016%2fj.media.2021.101976&partnerID=40&md5=e7e481fde0af089fb44b5c2b7d76728f","volume":"69"},
  {"id":"yanAutomaticVirtualNetwork2020","abstract":"Virtual network embedding arranges virtual network services onto substrate network components. The performance of embedding algorithms determines the effectiveness and efficiency of a virtualized network, making it a critical part of the network virtualization technology. To achieve better performance, the algorithm needs to automatically detect the network status which is complicated and changes in a time-varying manner, and to dynamically provide solutions that can best fit the current network status. However, most existing algorithms fail to provide automatic embedding solutions in an acceptable running time. In this paper, we combine deep reinforcement learning with a novel neural network structure based on graph convolutional networks, and propose a new and efficient algorithm for automatic virtual network embedding. In addition, a parallel reinforcement learning framework is used in training along with a newly-designed multi-objective reward function, which has proven beneficial to the proposed algorithm for automatic embedding of virtual networks. Extensive simulation results under different scenarios show that our algorithm achieves best performance on most metrics compared with the existing state-of-the-art solutions, with upto 39.6% and 70.6% improvement on acceptance ratio and average revenue, respectively. Moreover, the results also demonstrate that the proposed solution possesses good robustness.","accessed":{"date-parts":[["2024",1,2]]},"author":[{"family":"Yan","given":"Zhongxia"},{"family":"Ge","given":"Jingguo"},{"family":"Wu","given":"Yulei"},{"family":"Li","given":"Liangxiong"},{"family":"Li","given":"Tong"}],"citation-key":"yanAutomaticVirtualNetwork2020","container-title":"IEEE Journal on Selected Areas in Communications","DOI":"10.1109/JSAC.2020.2986662","ISSN":"1558-0008","issue":"6","issued":{"date-parts":[["2020",6]]},"page":"1040-1057","source":"IEEE Xplore","title":"Automatic Virtual Network Embedding: A Deep Reinforcement Learning Approach With Graph Convolutional Networks","title-short":"Automatic Virtual Network Embedding","type":"article-journal","URL":"https://ieeexplore.ieee.org/abstract/document/9060910","volume":"38"},
  {"id":"yangGAAPPONovelGraph2023","abstract":"The Graph Convolutional Network (GCN) has demonstrated impressive performance in processing graph structured data. However recent studies have revealed that GCN is vulnerable to adversarial attacks, where a small amount of data modification can significantly affect the performance of the GCN models. While most existing studies node injection attacks with graph reinforcement learning by considering gradient information, they still suffer from the problems that the step size of the policy gradient is difficult to determine, and the attack effect needs to be further improved. In light of the above issues, this paper proposes a Graph Adversarial Attack method by incorporating Proximal Policy Optimization named GAA-PPO, which fills subtasks of sequentially generating features and links for injected nodes without modifying existing nodes or edges. GAA-PPO comprises two main components: node injection attack network (actor network) and value prediction network (critic network). Specifically, the actor network leverages a node generator and an edge sampler to generate appropriate features and edges for the injected nodes. Notably, a novel edge sampler that incorporates Approximation Personalized Propagation of Neural Prediction (APPNP) is introduced to effectively propagate malicious features of the injected nodes. On the other hand, the critic network evaluates the performance of the perturbed graph at each stage. To enhance the stability of the algorithm, GAA-PPO employs the importance sampling technique of Proximal Policy Optimization (PPO) during the training process. Extensive experiments on three publicly benchmark datasets show that GAA-PPO yields significant performance advantages over the state-of-the-art method.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Yang","given":"Shuxin"},{"family":"Chang","given":"Xiaoyang"},{"family":"Zhu","given":"Guixiang"},{"family":"Cao","given":"Jie"},{"family":"Qin","given":"Weiping"},{"family":"Wang","given":"Youquan"},{"family":"Wang","given":"Zhendong"}],"citation-key":"yangGAAPPONovelGraph2023","container-title":"Neurocomputing","container-title-short":"Neurocomputing","DOI":"10.1016/j.neucom.2023.126707","ISSN":"0925-2312","issued":{"date-parts":[["2023",11,7]]},"page":"126707","source":"ScienceDirect","title":"GAA-PPO: A novel graph adversarial attack method by incorporating proximal policy optimization","title-short":"GAA-PPO","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0925231223008305","volume":"557"},
  {"id":"yangGAAPPONovelGraph2023a","archive":"Scopus","author":[{"family":"Yang","given":"S."},{"family":"Chang","given":"X."},{"family":"Zhu","given":"G."},{"family":"Cao","given":"J."},{"family":"Qin","given":"W."},{"family":"Wang","given":"Y."},{"family":"Wang","given":"Z."}],"citation-key":"yangGAAPPONovelGraph2023a","container-title":"Neurocomputing","DOI":"10.1016/j.neucom.2023.126707","issued":{"date-parts":[["2023"]]},"title":"GAA-PPO: A novel graph adversarial attack method by incorporating proximal policy optimization","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172415094&doi=10.1016%2fj.neucom.2023.126707&partnerID=40&md5=2bbb4e0204f9fbd2ba3b7204cd22adac","volume":"557"},
  {"id":"yangGeneralizedSingleVehicleBasedGraph2022","abstract":"In the autonomous driving process, the decision-making system is mainly used to provide macro-control instructions based on the information captured by the sensing system. Learning-based algorithms have apparent advantages in information processing and understanding for an increasingly complex driving environment. To incorporate the interactive information between agents in the environment into the decision-making process, this paper proposes a generalized single-vehicle-based graph neural network reinforcement learning algorithm (SGRL algorithm). The SGRL algorithm introduces graph convolution into the traditional deep neural network (DQN) algorithm, adopts the training method for a single agent, designs a more explicit incentive reward function, and significantly improves the dimension of the action space. The SGRL algorithm is compared with the traditional DQN algorithm (NGRL) and the multi-agent training algorithm (MGRL) in the highway ramp scenario. Results show that the SGRL algorithm has outstanding advantages in network convergence, decision-making effect, and training efficiency.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Yang","given":"Fan"},{"family":"Li","given":"Xueyuan"},{"family":"Liu","given":"Qi"},{"family":"Li","given":"Zirui"},{"family":"Gao","given":"Xin"}],"citation-key":"yangGeneralizedSingleVehicleBasedGraph2022","container-title":"Sensors","DOI":"10.3390/s22134935","ISSN":"1424-8220","issue":"13","issued":{"date-parts":[["2022",1]]},"language":"en","license":"http://creativecommons.org/licenses/by/3.0/","number":"13","page":"4935","publisher":"Multidisciplinary Digital Publishing Institute","source":"www.mdpi.com","title":"Generalized Single-Vehicle-Based Graph Reinforcement Learning for Decision-Making in Autonomous Driving","type":"article-journal","URL":"https://www.mdpi.com/1424-8220/22/13/4935","volume":"22"},
  {"id":"yangNewCombinationModel2022","archive":"Scopus","author":[{"family":"Yang","given":"H."},{"family":"Wang","given":"C."},{"family":"Li","given":"G."}],"citation-key":"yangNewCombinationModel2022","container-title":"Journal of Environmental Management","DOI":"10.1016/j.jenvman.2022.115498","issued":{"date-parts":[["2022"]]},"title":"A new combination model using decomposition ensemble framework and error correction technique for forecasting hourly PM2.5 concentration","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132804283&doi=10.1016%2fj.jenvman.2022.115498&partnerID=40&md5=d7256e779b88ec97259bf44b0e63053e","volume":"318"},
  {"id":"yanGraphCooperationDeep2023","abstract":"Cooperation between intersections in large-scale road networks is critical in traffic congestion. Currently, most traffic signals cooperate via pre-defined timing phases, which is extremely inefficient in real-time traffic scenarios. Most existing studies on multi-agent reinforcement learning (MARL) traffic signal control have focused on designing efficient communication methods, but have ignored the importance of how agents interact in cooperative communication. To achieve more efficient cooperation among traffic signals and alleviate urban traffic congestion, this study constructs a Graph Cooperation Q-learning Network Traffic Signal Control (GCQN-TSC) model, which is a graph cooperation network with an embedded self-attention mechanism that enables agents to adjust their attention in real time according to the dynamic traffic flow information, perceive the traffic environment quickly and effectively in a larger range, and help agents achieve more effective collaboration. Moreover, the Deep Graph Q-learning (DGQ) algorithm is proposed in this model to optimize the traffic signal control strategy according to the spatio-temporal characteristics of different traffic scenes and provide the optimal signal phase for each intersection. This study also integrates the ecological traffic concept into MARL traffic signal control, which aims to reduce traffic exhaust emissions. Finally, the proposed GCQN-TSC is experimentally validated both in a synthetic traffic grid and a real-world traffic network using the SUMO simulator. The experimental results show that GCQN-TSC outperforms other traffic signal control methods in almost all performance metrics, including average queue length and waiting time, as it can aggregate information acquired from collaborative agents and make network-level signal optimization decisions.","accessed":{"date-parts":[["2023",11,13]]},"author":[{"family":"Yan","given":"Liping"},{"family":"Zhu","given":"Lulong"},{"family":"Song","given":"Kai"},{"family":"Yuan","given":"Zhaohui"},{"family":"Yan","given":"Yunjuan"},{"family":"Tang","given":"Yue"},{"family":"Peng","given":"Chan"}],"citation-key":"yanGraphCooperationDeep2023","container-title":"Applied Intelligence","container-title-short":"Appl Intell","DOI":"10.1007/s10489-022-03208-w","ISSN":"1573-7497","issue":"6","issued":{"date-parts":[["2023",3,1]]},"language":"en","page":"6248-6265","source":"Springer Link","title":"Graph cooperation deep reinforcement learning for ecological urban traffic signal control","type":"article-journal","URL":"https://doi.org/10.1007/s10489-022-03208-w","volume":"53"},
  {"id":"yanMultiAgentSafe2023","abstract":"To realize real-time voltage/var control (VVC) in active distribution networks (ADNs), this paper proposes a new multi-agent safe graph reinforcement learning method to optimize reactive power output from PV inverters. The network is divided into several zones, and a decentralized framework is proposed for coordinated control of reactive power output in each zone to regulate voltage profiles and minimize network energy loss. The VVC problem is formulated as a multi-agent decentralized partially observable constrained Markov decision process. Each zone has a central control agent that embeds graph convolution networks (GCNs) in the policy network to improve the decision-making capability. The GCN extracts graph-structured features from the ADN topology, reflecting the relationship between VVC and grid topology, and can filter noise and impute missing data. The training process includes primal-dual policy optimization to rigorously satisfy voltage safety constraints. Simulations on a 141-bus distribution system demonstrate that the proposed method can effectively minimize network energy loss and reduce voltage deviations, even in the presence of noisy or incomplete input measurements.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Yan","given":"Rudai"},{"family":"Xing","given":"Qiang"},{"family":"Xu","given":"Yan"}],"citation-key":"yanMultiAgentSafe2023","container-title":"IEEE Transactions on Smart Grid","DOI":"10.1109/TSG.2023.3277087","ISSN":"1949-3061","issued":{"date-parts":[["2023"]]},"page":"1-1","source":"IEEE Xplore","title":"Multi Agent Safe Graph Reinforcement Learning for PV Inverter s Based Real-Time De centralized Volt/Var Control in Zoned Distribution Networks","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/10128717"},
  {"id":"yanNewHybridEnsemble2021","archive":"Scopus","author":[{"family":"Yan","given":"G."},{"family":"Yu","given":"C."},{"family":"Bai","given":"Y."}],"citation-key":"yanNewHybridEnsemble2021","container-title":"Machines","DOI":"10.3390/machines9120312","issue":"12","issued":{"date-parts":[["2021"]]},"title":"A new hybrid ensemble deep learning model for train axle temperature short term forecasting","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120668621&doi=10.3390%2fmachines9120312&partnerID=40&md5=7444ac5cd84e92c31a2e8d834db3df83","volume":"9"},
  {"id":"yuanXGNNModelLevelExplanations2020","abstract":"Graphs neural networks (GNNs) learn node features by aggregating and combining neighbor information, which have achieved promising performance on many graph tasks. However, GNNs are mostly treated as black-boxes and lack human intelligible explanations. Thus, they cannot be fully trusted and used in certain application domains if GNN models cannot be explained. In this work, we propose a novel approach, known as XGNN, to interpret GNNs at the model-level. Our approach can provide high-level insights and generic understanding of how GNNs work. In particular, we propose to explain GNNs by training a graph generator so that the generated graph patterns maximize a certain prediction of the model. We formulate the graph generation as a reinforcement learning task, where for each step, the graph generator predicts how to add an edge into the current graph. The graph generator is trained via a policy gradient method based on information from the trained GNNs. In addition, we incorporate several graph rules to encourage the generated graphs to be valid. Experimental results on both synthetic and real-world datasets show that our proposed methods help understand and verify the trained GNNs. Furthermore, our experimental results indicate that the generated graphs can provide guidance on how to improve the trained GNNs.","accessed":{"date-parts":[["2024",1,1]]},"author":[{"family":"Yuan","given":"Hao"},{"family":"Tang","given":"Jiliang"},{"family":"Hu","given":"Xia"},{"family":"Ji","given":"Shuiwang"}],"citation-key":"yuanXGNNModelLevelExplanations2020","collection-title":"KDD '20","container-title":"Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining","DOI":"10.1145/3394486.3403085","event-place":"New York, NY, USA","ISBN":"978-1-4503-7998-4","issued":{"date-parts":[["2020",8,20]]},"page":"430–438","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","source":"ACM Digital Library","title":"XGNN: Towards Model-Level Explanations of Graph Neural Networks","title-short":"XGNN","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/3394486.3403085"},
  {"id":"yueGraphQlearningAssisted2023","abstract":"Vehicle routing problem with time windows (VRPTW) is a typical class of constrained path planning problems in the field of combinatorial optimization. VRPTW considers a delivery task for a given set of customers with time windows, and the target is to find optimal routes for a group of vehicles that can minimize the total transportation cost. The traditional heuristics suffer from several limitations when solving VRPTW, such as poor scalability, sensitivity to hyperparameters and difficulty in handling complex constraints. Recent advance in machine learning makes it possible to enhance heuristic approaches via learned knowledge. In this paper, we propose a graph Q-learning assisted ant colony optimization algorithm named GQL-ACO to solve VRPTW. Compared to vanilla ant colony optimization (ACO), our proposed method first employs the learned heuristic values by using graph Q learning, instead of handcrafted ones, to define the hyperparameters of ACO. Second, we design a collaborative search strategy by combining ACO and Q-learning effectively, which can adaptively adjust the hyperparameters of ACO based on the search experiences.","accessed":{"date-parts":[["2023",11,13]]},"author":[{"family":"Yue","given":"Peng"},{"family":"Liu","given":"Shiqing"},{"family":"Jin","given":"Yaochu"}],"citation-key":"yueGraphQlearningAssisted2023","collection-title":"GECCO '23 Companion","container-title":"Proceedings of the Companion Conference on Genetic and Evolutionary Computation","DOI":"10.1145/3583133.3596423","event-place":"New York, NY, USA","ISBN":"9798400701207","issued":{"date-parts":[["2023",7,24]]},"page":"7–8","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","source":"ACM Digital Library","title":"Graph Q-learning Assisted Ant Colony Optimization for Vehicle Routing Problems with Time Windows","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/3583133.3596423"},
  {"id":"zengCombiningSubgoalGraphs2019","abstract":"In this paper, we present a hierarchical path planning framework called SG-RL (subgoal graphs-reinforcement learning), to plan rational paths for agents maneuvering in continuous and uncertain environments. By \"rational\", we mean (1) efficient path planning to eliminate first-move lags; (2) collision-free and smooth for agents with kinematic constraints satisfied. SG-RL works in a two-level manner. At the first level, SG-RL uses a geometric path-planning method, i.e., Simple Subgoal Graphs (SSG), to efficiently find optimal abstract paths, also called subgoal sequences. At the second level, SG-RL uses an RL method, i.e., Least-Squares Policy Iteration (LSPI), to learn near-optimal motion-planning policies which can generate kinematically feasible and collision-free trajectories between adjacent subgoals. The first advantage of the proposed method is that SSG can solve the limitations of sparse reward and local minima trap for RL agents; thus, LSPI can be used to generate paths in complex environments. The second advantage is that, when the environment changes slightly (i.e., unexpected obstacles appearing), SG-RL does not need to reconstruct subgoal graphs and replan subgoal sequences using SSGs, since LSPI can deal with uncertainties by exploiting its generalization ability to handle changes in environments. Simulation experiments in representative scenarios demonstrate that, compared with existing methods, SG-RL can work well on large-scale maps with relatively low action-switching frequencies and shorter path lengths, and SG-RL can deal with small changes in environments. We further demonstrate that the design of reward functions and the types of training environments are important factors for learning feasible policies. © 2019 by the authors.","archive":"Scopus","author":[{"family":"Zeng","given":"J."},{"family":"Qin","given":"L."},{"family":"Hu","given":"Y."},{"family":"Hu","given":"C."},{"family":"Yin","given":"Q."}],"citation-key":"zengCombiningSubgoalGraphs2019","container-title":"Applied Sciences (Switzerland)","DOI":"10.3390/app9020323","ISSN":"2076-3417","issue":"2","issued":{"date-parts":[["2019"]]},"language":"English","source":"Scopus","title":"Combining Subgoal Graphs with Reinforcement Learning to Build a Rational Pathfinder","type":"article-journal","volume":"9"},
  {"id":"zengCombiningSubgoalGraphs2019a","archive":"Scopus","author":[{"family":"Zeng","given":"J."},{"family":"Qin","given":"L."},{"family":"Hu","given":"Y."},{"family":"Hu","given":"C."},{"family":"Yin","given":"Q."}],"citation-key":"zengCombiningSubgoalGraphs2019a","container-title":"Applied Sciences (Switzerland)","DOI":"10.3390/app9020323","issue":"2","issued":{"date-parts":[["2019"]]},"title":"Combining Subgoal Graphs with Reinforcement Learning to Build a Rational Pathfinder","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060100131&doi=10.3390%2fapp9020323&partnerID=40&md5=72a0d2840e79283670f05434d6052fe5","volume":"9"},
  {"id":"zhangDeepLearningGraphs2022","abstract":"Deep learning has been shown to be successful in a number of domains, ranging from acoustics, images, to natural language processing. However, applying deep learning to the ubiquitous graph data is non-trivial because of the unique characteristics of graphs. Recently, substantial research efforts have been devoted to applying deep learning methods to graphs, resulting in beneficial advances in graph analysis techniques. In this survey, we comprehensively review the different types of deep learning methods on graphs. We divide the existing methods into five categories based on their model architectures and training strategies: graph recurrent neural networks, graph convolutional networks, graph autoencoders, graph reinforcement learning, and graph adversarial methods. We then provide a comprehensive overview of these methods in a systematic manner mainly by following their development history. We also analyze the differences and compositions of different methods. Finally, we briefly outline the applications in which they have been used and discuss potential future research directions.","accessed":{"date-parts":[["2023",11,27]]},"author":[{"family":"Zhang","given":"Ziwei"},{"family":"Cui","given":"Peng"},{"family":"Zhu","given":"Wenwu"}],"citation-key":"zhangDeepLearningGraphs2022","container-title":"IEEE Transactions on Knowledge and Data Engineering","DOI":"10.1109/TKDE.2020.2981333","ISSN":"1558-2191","issue":"1","issued":{"date-parts":[["2022",1]]},"page":"249-270","source":"IEEE Xplore","title":"Deep Learning on Graphs: A Survey","title-short":"Deep Learning on Graphs","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9039675","volume":"34"},
  {"id":"zhangGraphNeuralNetworks2023","abstract":"In this article, the utility of graph neural network (GNN) surrogates for Monte Carlo (MC) sampling-based risk quantification in daily operations of power grid is investigated. The MC simulation process necessitates solving a large number of optimal power flow (OPF) problems corresponding to the sample values of stochastic grid variables (power demand and renewable generation), which is computationally prohibitive. Computationally inexpensive surrogates of the OPF problem provide an attractive alternative for expedited MC simulation. GNN surrogates are especially suitable due to their superior ability to handle graph-structured data. Therefore, GNN surrogates of OPF problem are trained using supervised learning. They are then used to obtain Monte Carlo (MC) samples of the quantities of interest (operating reserve, transmission line flow) given the (hours-ahead) probabilistic wind generation and load forecast. The utility of GNN surrogates is evaluated by comparing OPF-based and GNN-based grid reliability and risk for IEEE Case118 synthetic grid. It is shown that the GNN surrogates are sufficiently accurate for predicting the (bus-level, branch-level and system-level) grid state and enable fast as well as accurate operational risk quantification for power grids. The article thus develops various tools for fast reliability and risk quantification for real-world power grids using GNNs.","accessed":{"date-parts":[["2023",11,20]]},"author":[{"family":"Zhang","given":"Yadong"},{"family":"Karve","given":"Pranav M."},{"family":"Mahadevan","given":"Sankaran"}],"citation-key":"zhangGraphNeuralNetworks2023","DOI":"10.48550/arXiv.2311.03661","issued":{"date-parts":[["2023",11,6]]},"number":"arXiv:2311.03661","publisher":"arXiv","source":"arXiv.org","title":"Graph Neural Networks for Power Grid Operational Risk Assessment","type":"article","URL":"http://arxiv.org/abs/2311.03661"},
  {"id":"zhangGraphReinforcementLearning2024","archive":"Scopus","author":[{"family":"Zhang","given":"B."},{"family":"Li","given":"C."},{"family":"Hu","given":"B."},{"family":"Li","given":"X."},{"family":"Wang","given":"R."},{"family":"Dong","given":"Z."}],"citation-key":"zhangGraphReinforcementLearning2024","DOI":"10.1007/978-981-99-8126-7_24","event-title":"Communications in Computer and Information Science","issued":{"date-parts":[["2024"]]},"page":"303-314","title":"Graph Reinforcement Learning for Securing Critical Loads by E-Mobility","type":"paper-conference","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178598408&doi=10.1007%2f978-981-99-8126-7_24&partnerID=40&md5=614a5735501c5aeb100dcedf1bc07c87","volume":"1961 CCIS"},
  {"id":"zhangLearningDispatchJob2020","abstract":"Priority dispatching rule (PDR) is widely used for solving real-world Job-shop scheduling problem (JSSP). However, the design of effective PDRs is a tedious task, requiring a myriad of specialized knowledge and often delivering limited performance. In this paper, we propose to automatically learn PDRs via an end-to-end deep reinforcement learning agent. We exploit the disjunctive graph representation of JSSP, and propose a Graph Neural Network based scheme to embed the states encountered during solving. The resulting policy network is size-agnostic, effectively enabling generalization on large-scale instances. Experiments show that the agent can learn high-quality PDRs from scratch with elementary raw features, and demonstrates strong performance against the best existing PDRs. The learned policies also perform well on much larger instances that are unseen in training.","accessed":{"date-parts":[["2024",1,2]]},"author":[{"family":"Zhang","given":"Cong"},{"family":"Song","given":"Wen"},{"family":"Cao","given":"Zhiguang"},{"family":"Zhang","given":"Jie"},{"family":"Tan","given":"Puay Siew"},{"family":"Chi","given":"Xu"}],"citation-key":"zhangLearningDispatchJob2020","container-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[["2020"]]},"page":"1621–1632","publisher":"Curran Associates, Inc.","source":"Neural Information Processing Systems","title":"Learning to Dispatch for Job Shop Scheduling via Deep Reinforcement Learning","type":"paper-conference","URL":"https://proceedings.neurips.cc/paper/2020/hash/11958dfee29b6709f48a9ba0387a2431-Abstract.html","volume":"33"},
  {"id":"zhangMVFStainMultipleVirtual2022","archive":"Scopus","author":[{"family":"Zhang","given":"R."},{"family":"Cao","given":"Y."},{"family":"Li","given":"Y."},{"family":"Liu","given":"Z."},{"family":"Wang","given":"J."},{"family":"He","given":"J."},{"family":"Zhang","given":"C."},{"family":"Sui","given":"X."},{"family":"Zhang","given":"P."},{"family":"Cui","given":"L."},{"family":"Li","given":"S."}],"citation-key":"zhangMVFStainMultipleVirtual2022","container-title":"Medical Image Analysis","DOI":"10.1016/j.media.2022.102520","issued":{"date-parts":[["2022"]]},"title":"MVFStain: Multiple virtual functional stain histopathology images generation based on specific domain mapping","type":"article-journal","URL":"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133900469&doi=10.1016%2fj.media.2022.102520&partnerID=40&md5=3357394a4b2136db42ab9a92ebc59c29","volume":"80"},
  {"id":"zhaoLargeScaleMachineLearning2022","abstract":"Efficient scheduling of distributed deep learning (DL) jobs in large GPU clusters is crucial for resource efficiency and job performance. While server sharing among jobs improves resource utilization, interference among co-located DL jobs occurs due to resource contention. Interference-aware job placement has been studied, with white-box approaches based on explicit interference modeling and black-box schedulers with reinforcement learning. In today’s clusters containing thousands of GPU servers, running a single scheduler to manage all arrival jobs in a timely and effective manner is challenging, due to the large workload scale. We adopt multiple schedulers in a large-scale cluster/data center, and propose a multi-agent reinforcement learning (MARL) scheduling framework to cooperatively learn fine-grained job placement policies, towards the objective of minimizing job completion time (JCT). To achieve topology-aware placements, our proposed framework uses hierarchical graph neural networks to encode the data center topology and server architecture. In view of a common lack of precise reward samples corresponding to different placements, a job interference model is further devised to predict interference levels in face of various co-locations, for training of the MARL schedulers. Testbed and trace-driven evaluations show that our scheduler framework outperforms representative scheduling schemes by more than 20% in terms of average JCT, and is adaptive to various machine learning cluster topologies.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Zhao","given":"Xiaoyang"},{"family":"Wu","given":"Chuan"}],"citation-key":"zhaoLargeScaleMachineLearning2022","container-title":"IEEE Transactions on Network and Service Management","DOI":"10.1109/TNSM.2021.3139607","ISSN":"1932-4537","issue":"4","issued":{"date-parts":[["2022",12]]},"page":"4962-4974","source":"IEEE Xplore","title":"Large-Scale Machine Learning Cluster Scheduling via Multi-Agent Graph Reinforcement Learning","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9667106","volume":"19"},
  {"id":"zhaoLearningSequentialDistribution2022","abstract":"A distribution service restoration algorithm as a fundamental resilient paradigm for system operators provides an optimally coordinated, resilient solution to enhance the restoration performance. The restoration problem is formulated to coordinate distribution generators and controllable switches optimally. A model-based control scheme is usually designed to solve this problem, relying on a precise model and resulting in low scalability. To tackle these limitations, this work proposes a graph-reinforcement learning framework for the restoration problem. We link the power system topology with a graph convolutional network, which captures the complex mechanism of network restoration in power networks and understands the mutual interactions among controllable devices. Latent features over graphical power networks produced by graph convolutional layers are exploited to learn the control policy for network restoration using deep reinforcement learning. The solution scalability is guaranteed by modeling distributed generators as agents in a multi-agent environment and a proper pre-training paradigm. Comparative studies on IEEE 123-node and 8500-node test systems demonstrate the performance of the proposed solution.","accessed":{"date-parts":[["2023",10,29]]},"author":[{"family":"Zhao","given":"Tianqiao"},{"family":"Wang","given":"Jianhui"}],"citation-key":"zhaoLearningSequentialDistribution2022","container-title":"IEEE Transactions on Power Systems","DOI":"10.1109/TPWRS.2021.3102870","ISSN":"1558-0679","issue":"2","issued":{"date-parts":[["2022",3]]},"page":"1601-1611","source":"IEEE Xplore","title":"Learning Sequential Distribution System Restoration via Graph-Reinforcement Learning","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9508140","volume":"37"}
]
