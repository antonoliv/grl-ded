mean_reward,mean_cost,mean_res_wasted,mean_length,training_iterations,trial_id,done,config/env_name,config/train_episodes,config/eval_episodes,config/path,config/seed,config/obs_step,config/obs_scaled_p,config/reward,config/res_term,config/learning_rate,config/gamma,config/ent_coef,config/gradient_steps,config/buffer_size,config/batch_size,config/tau,config/target_update_interval,config/learning_starts,config/train_freq,config/target_entropy,config/use_sde,config/sde_sample_freq,config/use_sde_at_warmup,config/action_noise,config/optimizer,config/activation_fn,config/num_units_layer,config/num_hidden_layers,config/gnn_in_channels,config/gnn_hidden_channels,config/gnn_out_channels,config/gnn_num_layers,config/gnn_dropout,logdir
88.95409846305847,493328.7451030819,15311.539600180336,125.0,1,d3108_00000,True,l2rpn_icaps_2021_small,3,3,/home/treeman/school/dissertation/src/grl/experiments/reward_test/gcn_sac/,142312345,False,True,<class 'environment.reward.res_penalty_reward.RESPenaltyReward'>,0.44273114474649444,0.0001,0.85,auto,1,1000000,256,0.001,1,1000,1,auto,False,-1,False,,<class 'torch.optim.adam.Adam'>,<class 'torch.nn.modules.activation.ReLU'>,128,6,6,36,36,3,0.0,d3108_00000
