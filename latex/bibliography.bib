@article{almasanDeepReinforcementLearning2022,
  title = {Deep Reinforcement Learning Meets Graph Neural Networks: {{Exploring}} a Routing Optimization Use Case},
  shorttitle = {Deep Reinforcement Learning Meets Graph Neural Networks},
  author = {Almasan, Paul and {Su{\'a}rez-Varela}, Jos{\'e} and Rusek, Krzysztof and {Barlet-Ros}, Pere and {Cabellos-Aparicio}, Albert},
  year = {2022},
  month = dec,
  journal = {Computer Communications},
  volume = {196},
  pages = {184--194},
  issn = {0140-3664},
  doi = {10.1016/j.comcom.2022.09.029},
  urldate = {2023-11-27},
  abstract = {Deep Reinforcement Learning (DRL) has shown a dramatic improvement in decision-making and automated control problems. Consequently, DRL represents a promising technique to efficiently solve many relevant optimization problems (e.g., routing) in self-driving networks. However, existing DRL-based solutions applied to networking fail to generalize, which means that they are not able to operate properly when applied to network topologies not observed during training. This lack of generalization capability significantly hinders the deployment of DRL technologies in production networks. This is because state-of-the-art DRL-based networking solutions use standard neural networks (e.g., fully connected, convolutional), which are not suited to learn from information structured as graphs. In this paper, we integrate Graph Neural Networks (GNN) into DRL agents and we design a problem specific action space to enable generalization. GNNs are Deep Learning models inherently designed to generalize over graphs of different sizes and structures. This allows the proposed GNN-based DRL agent to learn and generalize over arbitrary network topologies. We test our DRL+GNN agent in a routing optimization use case in optical networks and evaluate it on 180 and 232 unseen synthetic and real-world network topologies respectively. The results show that the DRL+GNN agent is able to outperform state-of-the-art solutions in topologies never seen during training.},
  keywords = {0 - DRL,0 - GNN,0 - RL,0 - Unmentioned,0 - Untagged,A - Smart Mobility,A - Vehicle Routing,DRL - DQL,DRL - DQN},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2022 - Almasan et al - Deep reinforcement learning meets graph neural networks.pdf;../../../documents/school/dissertation/zotero/storage/E5KUP45Y/S0140366422003784.html}
}

@misc{bahdanauNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2016},
  month = may,
  number = {arXiv:1409.0473},
  eprint = {1409.0473},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1409.0473},
  urldate = {2024-01-07},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arxiv},
  keywords = {T - Attention Mechanism,Type - Seminal},
  file = {../../../documents/school/dissertation/bibliography/Preprint/2016 - Bahdanau et al - Neural Machine Translation by Jointly Learning to Align and Translate.pdf;../../../documents/school/dissertation/zotero/storage/UYAAUKBS/1409.html}
}

@article{bayindirSmartGridTechnologies2016,
  title = {Smart Grid Technologies and Applications},
  author = {Bayindir, R. and Colak, I. and Fulli, G. and Demirtas, K.},
  year = {2016},
  month = dec,
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {66},
  pages = {499--516},
  issn = {1364-0321},
  doi = {10.1016/j.rser.2016.08.002},
  urldate = {2023-11-20},
  abstract = {Smart grid technologies can be defined as self-sufficient systems that can find solutions to problems quickly in an available system that reduces the workforce and targets sustainable, reliable, safe and quality electricity to all consumers. In this respect, different technological applications can be seen from the perspective of researchers and investors. Even though these technological application studies constitute an initial step for the structure of the smart grid, they have not been fully completed in many countries. Associations of initial studies for the next step in smart grid applications will provide an economic benefit for the authorities in the long term, and will help to establish standards to be compatible with every application so that all smart grid applications can be coordinated under the control of the same authorities. In this study, a review has been made of technological methods of data transmission and the energy efficiency in smart grids as well as smart grid applications. Therefore, this study is expected to be an important guiding source for researchers and engineers studying the smart grid. It also helps transmission and distribution system operators to follow the right path as they are transforming their classical grids to smart grids.},
  keywords = {0 - Unmentioned,0 - Untagged,A - Smart Grid},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2016 - Bayindir et al - Smart grid technologies and applications.pdf;../../../documents/school/dissertation/zotero/storage/2BKMC9VE/S1364032116304191.html}
}

@inproceedings{bihaniStriderNetGraphReinforcement2023,
  title = {{{StriderNet}}: {{A Graph Reinforcement Learning Approach}} to {{Optimize Atomic Structures}} on {{Rough Energy Landscapes}}},
  shorttitle = {{{StriderNet}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Bihani, Vaibhav and Manchanda, Sahil and Sastry, Srikanth and Ranu, Sayan and Krishnan, N. M. Anoop},
  year = {2023},
  month = jul,
  pages = {2431--2451},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-10-29},
  abstract = {Optimization of atomic structures presents a challenging problem, due to their highly rough and non-convex energy landscape, with wide applications in the fields of drug design, materials discovery, and mechanics. Here, we present a graph reinforcement learning approach, StriderNet, that learns a policy to displace the atoms towards low energy configurations. We evaluate the performance of StriderNet on three complex atomic systems, namely, binary Lennard-Jones particles, calcium silicate hydrates gel, and disordered silicon. We show that StriderNet outperforms all classical optimization algorithms and enables the discovery of a lower energy minimum. In addition, StriderNet exhibits a higher rate of reaching minima with energies, as confirmed by the average over multiple realizations. Finally, we show that StriderNet exhibits inductivity to unseen system sizes that are an order of magnitude different from the training system. All the codes and datasets are available at https://github.com/M3RG-IITD/StriderNET.},
  langid = {english},
  keywords = {0 - Unmentioned,0 - Untagged},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2023 - Bihani et al - StriderNet.pdf}
}

@inproceedings{bozkusEnsembleGraphQLearning2023,
  title = {Ensemble {{Graph Q-Learning}} for {{Large Scale Networks}}},
  booktitle = {{{ICASSP}} 2023 - 2023 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Bozkus, Talha and Mitra, Urbashi},
  year = {2023},
  month = jun,
  pages = {1--5},
  issn = {2379-190X},
  doi = {10.1109/ICASSP49357.2023.10094828},
  urldate = {2023-11-13},
  abstract = {The optimization of large-scale networks such as finding the optimal control strategies through cost minimization is challenged by large state spaces. For networks that can be modeled via Markov Decision Processes (MDP), a previously proposed graph reduction strategy is used in conjunction with a novel ensemble learning method based on Q-learning algorithm for policy optimization in unknown environments. By exploiting the structural properties of the network, several structurally related Markov chains are created and these multiple chains are sampled to learn multiple policies which are fused. The convergence of the learning approach is analyzed and the ensemble learning strategy is shown to inherit the properties of classical Q-learning. Numerical results show that the proposed algorithm achieves a reduction of 60\% with respect to the policy error and 80\% for the runtime versus other state-of-the-art Q-learning algorithms.},
  keywords = {0 - Unmentioned,0 - Untagged,GRL - Graph Q-Learning},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2023 - Bozkus-Mitra - Ensemble Graph Q-Learning for Large Scale Networks.pdf;../../../documents/school/dissertation/zotero/storage/DWPPW8GZ/10094828.html}
}

@unpublished{brunskillCS234ReinforcementLearning,
  type = {Lecture {{Slides}}},
  title = {{{CS234}}: {{Reinforcement Learning Winter}} 2023},
  author = {Brunskill, Emma},
  urldate = {2024-01-05},
  keywords = {0 - Cited,0 - RL,Type - Lecture},
  file = {../../../documents/school/dissertation/zotero/storage/8UJ4SDPB/modules.html}
}

@article{caoDataDrivenMultiAgentDeep2021,
  title = {Data-{{Driven Multi-Agent Deep Reinforcement Learning}} for {{Distribution System Decentralized Voltage Control With High Penetration}} of {{PVs}}},
  author = {Cao, Di and Zhao, Junbo and Hu, Weihao and Ding, Fei and Huang, Qi and Chen, Zhe and Blaabjerg, Frede},
  year = {2021},
  month = sep,
  journal = {IEEE Transactions on Smart Grid},
  volume = {12},
  number = {5},
  pages = {4137--4150},
  issn = {1949-3061},
  doi = {10.1109/TSG.2021.3072251},
  urldate = {2023-11-17},
  abstract = {This paper proposes a novel model-free/data-driven centralized training and decentralized execution multi-agent deep reinforcement learning (MADRL) framework for distribution system voltage control with high penetration of PVs. The proposed MADRL can coordinate both the real and reactive power control of PVs with existing static var compensators and battery storage systems. Unlike the existing DRL-based voltage control methods, our proposed method does not rely on a system model during both the training and execution stages. This is achieved by developing a new interaction scheme between the surrogate modeling of the original system and the multi-agent soft actor critic (MASAC) MADRL algorithm. In particular, the sparse pseudo-Gaussian process with a few-shots of measurements is utilized to construct the surrogate model of the original environment, i.e., power flow model. This is a data-driven process and no model parameters are needed. Furthermore, the MASAC enabled MADRL allows to achieve better scalability by dividing the original system into different voltage control regions with the aid of real and reactive power sensitivities to voltage, where each region is treated as an agent. This also serves as the foundation for the centralized training and decentralized execution, thus significantly reducing the communication requirements as only local measurements are required for control. Comparative results with other alternatives on the IEEE 123-nodes and 342-nodes systems demonstrate the superiority of the proposed method.},
  keywords = {0 - DRL,0 - RL,0 - Unmentioned,0 - Untagged,A - High Penetration of Distributed Generation,A - Smart Grid,DRL - Actor-Critic,RL - Multi-Agent,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2021 - Cao et al - Data-Driven Multi-Agent Deep Reinforcement Learning for Distribution System.pdf;../../../documents/school/dissertation/zotero/storage/CMWKT8QI/9399637.html}
}

@book{charniakIntroductionDeepLearning2018,
  title = {Introduction to Deep Learning},
  author = {Charniak, Eugene},
  year = {2018},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts London, England}},
  isbn = {978-0-262-03951-2},
  langid = {english},
  keywords = {0 - Neural Network,Type - Book},
  file = {../../../documents/school/dissertation/bibliography/Book/2018 - Charniak - Introduction to deep learning.pdf}
}

@inproceedings{chenAutonomousExplorationUncertainty2020,
  title = {Autonomous {{Exploration Under Uncertainty}} via {{Deep Reinforcement Learning}} on {{Graphs}}},
  booktitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Chen, Fanfei and Martin, John D. and Huang, Yewei and Wang, Jinkun and Englot, Brendan},
  year = {2020},
  month = oct,
  pages = {6140--6147},
  issn = {2153-0866},
  doi = {10.1109/IROS45743.2020.9341657},
  urldate = {2023-12-18},
  abstract = {We consider an autonomous exploration problem in which a range-sensing mobile robot is tasked with accurately mapping the landmarks in an a priori unknown environment efficiently in real-time; it must choose sensing actions that both curb localization uncertainty and achieve information gain. For this problem, belief space planning methods that forward- simulate robot sensing and estimation may often fail in real-time implementation, scaling poorly with increasing size of the state, belief and action spaces. We propose a novel approach that uses graph neural networks (GNNs) in conjunction with deep reinforcement learning (DRL), enabling decision-making over graphs containing exploration information to predict a robot's optimal sensing action in belief space. The policy, which is trained in different random environments without human intervention, offers a real-time, scalable decision-making process whose high-performance exploratory sensing actions yield accurate maps and high rates of information gain.},
  keywords = {0 - Cited,0 - DRL,0 - GNN,0 - GRL,0 - RL,A - Autonomous Exploration,DRL - A2C,DRL - Actor-Critic,DRL - DQL,DRL - DQN,GNN - GCN,GNN - GGNN,GNN - Graph U-Net,RL - Single Agent,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2020 - Chen et al - Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning on.pdf;../../../documents/school/dissertation/zotero/storage/RSHTCFMI/9341657.html}
}

@article{chenGraphRepresentationLearningbased2023,
  title = {Graph Representation Learning-Based Residential Electricity Behavior Identification and Energy Management},
  author = {Chen, Xinpei and Yu, Tao and Pan, Zhenning and Wang, Zihao and Yang, Shengchun},
  year = {2023},
  month = dec,
  journal = {Protection and Control of Modern Power Systems},
  volume = {8},
  number = {1},
  pages = {1--13},
  publisher = {{SpringerOpen}},
  issn = {2367-0983},
  doi = {10.1186/s41601-023-00305-x},
  urldate = {2023-10-29},
  abstract = {It is important to achieve an efficient home energy management system (HEMS) because of its role in promoting energy saving and emission reduction for end-users. Two critical issues in an efficient HEMS are identification of user behavior and energy management strategy. However, current HEMS methods usually assume perfect knowledge of user behavior or ignore the strong correlations of usage habits with different applications. This can lead to an insufficient description of behavior and suboptimal management strategy. To address these gaps, this paper proposes non-intrusive load monitoring (NILM) assisted graph reinforcement learning (GRL) for intelligent HEMS decision making. First, a behavior correlation graph incorporating NILM is introduced to represent the energy consumption behavior of users and a multi-label classification model is used to monitor the loads. Thus, efficient identification of user behavior and description of state transition can be achieved. Second, based on the online updating of the behavior correlation graph, a GRL model is proposed to extract information contained in the graph. Thus, reliable strategy under uncertainty of environment and behavior is available. Finally, the experimental results on several datasets verify the effectiveness of the proposed model.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {0 - Cited,0 - DRL,0 - GRL,0 - RL,A - Home Energy Management System,A - Multi-label Classification,A - Non-intrusive Load Monitoring,A - Smart Grid,D - REDD,D - REFIT,DRL - DQL,DRL - DQN,GNN - GCN,RL - Single Agent,T - Behavior Correlation Graph,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Chen et al - Graph representation learning-based residential electricity behavior.pdf}
}

@article{chenPhysicalassistedMultiagentGraph2023,
  title = {Physical-Assisted Multi-Agent Graph Reinforcement Learning Enabled Fast Voltage Regulation for {{PV-rich}} Active Distribution Network},
  author = {Chen, Yongdong and Liu, Youbo and Zhao, Junbo and Qiu, Gao and Yin, Hang and Li, Zhengbo},
  year = {2023},
  month = dec,
  journal = {Applied Energy},
  volume = {351},
  pages = {121743},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2023.121743},
  urldate = {2023-10-29},
  abstract = {Active distribution network is encountering serious voltage violations associated with the proliferation of distributed photovoltaic. Cutting-edge research has confirmed that voltage regulation techniques based on deep reinforcement learning manifest superior performance in addressing this issue. However, such techniques are typically applied to the specifically fixed network topologies and have insufficient learning efficiency. To address these challenges, a novel edge intelligence, featured by a multi-agent deep reinforcement learning algorithm with graph attention network and physical-assisted mechanism, is proposed. This novel method is unique in that it includes the graph attention network into reinforcement learning to capture spatial correlations and topological linkages among nodes, allowing agents to be ``aware'' of topology variations caused by reconfiguration real time. Furthermore, employing a relatively exact physical model to generate reference experiences and storing them in a replay buffer enables agents to identify effective actions faster during training and thus, greatly enhances the efficiency of learning voltage regulation laws. All agents are trained centralized to learn a coordinated voltage regulation strategy, which is then executed decentralized based solely on local observation for fast response. The proposed methodology is evaluated on the IEEE 33-node and 136-node systems, and it outperforms the previously implemented approaches in convergence and control performance.},
  keywords = {0 - Cited,0 - DRL,0 - GNN,0 - RL,A - Smart Grid,A - Voltage Regulation,D - IEEE136,D - IEEE33,D - No Source,DRL - Actor-Critic,DRL - SAC,GNN - GAT,RL - Multi-Agent,T - Attention Mechanism,T - Multi-head Attention,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Chen et al - Physical-assisted multi-agent graph reinforcement learning enabled fast voltage.pdf;../../../documents/school/dissertation/zotero/storage/KLXTS7PE/S0306261923011078.html}
}

@article{chenScalableGraphReinforcement2023,
  title = {A Scalable Graph Reinforcement Learning Algorithm Based Stochastic Dynamic Dispatch of Power System under High Penetration of Renewable Energy},
  author = {Chen, Junbin and Yu, Tao and Pan, Zhenning and Zhang, Mengyue and Deng, Bairong},
  year = {2023},
  month = oct,
  journal = {International Journal of Electrical Power \& Energy Systems},
  volume = {152},
  pages = {109212},
  issn = {0142-0615},
  doi = {10.1016/j.ijepes.2023.109212},
  urldate = {2023-10-29},
  abstract = {Due to the increasing penetration of renewable energy source, power system is facing significant uncertainties. How to fully consider those uncertainties in dynamic economic dispatch (DED) has become a crucial problem to the safe and economic operation of power system. Reinforcement learning (RL) based approaches can provide the dispatch policy in response to uncertainty. However, current RL uses traditional Euclidean data representation, which greatly reduces the scalability and computational efficiency of economic dispatch algorithms. To address such obstacle, this paper develops a novel graph reinforcement learning (GRL) method for DED of power system. Firstly, DED is formulated as a Markov decision process and formulated as a dynamic sequential decision problem. Secondly, a novel graph-based representation of system state is proposed. Using graph data to represent dispatch operation data with non-Euclidean characteristics can effectively capture the implicit correlation of the uncertainty corresponding to the system topology. Thirdly, a GRL algorithm is proposed to learn the optimal policy which maps graph represented system state to DED decision. Compared with the traditional deep reinforcement learning (DRL), the GRL proposed in this paper has certain generalization ability and scalability, and can achieve higher quality solutions in online operation. Case studies illustrate that the optimality of the proposed method is 98.04\%, which is 15.13\% higher than that of the existing learning methods. The algorithm is scalable and improves the sample efficiency.},
  keywords = {0 - Cited,0 - DRL,0 - GRL,0 - RL,A - Dynamic Economic Dispatch,A - High Penetration of Distributed Generation,A - Smart Grid,D - IEEE39,DRL - Actor-Critic,DRL - SAC,GNN - GCN,RL - Single Agent,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Chen et al - A scalable graph reinforcement learning algorithm based stochastic dynamic.pdf;../../../documents/school/dissertation/zotero/storage/I2DZJYCH/S0142061523002697.html}
}

@inproceedings{choLearningPhraseRepresentations2014,
  title = {Learning Phrase Representations Using {{RNN}} Encoder-Decoder for Statistical Machine Translation},
  booktitle = {{{EMNLP}} 2014 - 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}, {{Proceedings}} of the {{Conference}}},
  author = {Cho, K. and Van Merri{\"e}nboer, B. and Gulcehre, C. and Bahdanau, D. and Bougares, F. and Schwenk, H. and Bengio, Y.},
  year = {2014},
  pages = {1724--1734},
  doi = {10.3115/v1/d14-1179},
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases. {\textcopyright} 2014 Association for Computational Linguistics.},
  isbn = {978-1-937284-96-1},
  langid = {english},
  keywords = {0 - Unmentioned,0 - Untagged},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2014 - Cho et al - Learning phrase representations using RNN encoder-decoder for statistical.pdf;../../../documents/school/dissertation/zotero/storage/RRCTBHZK/display.html}
}

@misc{christiePowerSystemsTesta,
  title = {Power {{Systems Test Case Archive}}},
  author = {Christie, Rich},
  urldate = {2024-01-12},
  keywords = {0 - Data,0 - Unmentioned,A - Smart Grid,D - IEEE118}
}

@incollection{cuiGraphRepresentationLearning2022,
  title = {Graph {{Representation Learning}}},
  booktitle = {Graph {{Neural Networks}}: {{Foundations}}, {{Frontiers}}, and {{Applications}}},
  author = {Cui, Peng and Wu, Lingfei and Pei, Jian and Zhao, Liang and Wang, Xiao},
  editor = {Wu, Lingfei and Cui, Peng and Pei, Jian and Zhao, Liang},
  year = {2022},
  pages = {17--26},
  publisher = {{Springer Nature}},
  address = {{Singapore}},
  doi = {10.1007/978-981-16-6054-2_2},
  urldate = {2024-01-07},
  abstract = {Graph representation learning aims at assigning nodes in a graph to lowdimensional representations and effectively preserving the graph structures. Recently, a significant amount of progress has been made toward this emerging graph analysis paradigm. In this chapter, we first summarize the motivation of graph representation learning. Afterwards and primarily, we provide a comprehensive overview of a large number of graph representation learning methods in a systematic manner, covering the traditional graph representation learning, modern graph representation learning, and graph neural networks.},
  isbn = {9789811660542},
  langid = {english},
  file = {../../../documents/school/dissertation/bibliography/Book Section/2022 - Cui et al - Graph Representation Learning.pdf}
}

@article{devaillyIGRLInductiveGraph2022,
  title = {{{IG-RL}}: {{Inductive Graph Reinforcement Learning}} for {{Massive-Scale Traffic Signal Control}}},
  shorttitle = {{{IG-RL}}},
  author = {Devailly, F.-X. and Larocque, D. and Charlin, L.},
  year = {2022},
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {23},
  number = {7},
  pages = {7496--7507},
  issn = {1524-9050},
  doi = {10.1109/TITS.2021.3070835},
  abstract = {Scaling adaptive traffic signal control involves dealing with combinatorial state and action spaces. Multi-agent reinforcement learning attempts to address this challenge by distributing control to specialized agents. However, specialization hinders generalization and transferability, and the computational graphs underlying neural-network architectures - dominating in the multi-agent setting - do not offer the flexibility to handle an arbitrary number of entities which changes both between road networks, and over time as vehicles traverse the network. We introduce Inductive Graph Reinforcement Learning (IG-RL) based on graph-convolutional networks which adapts to the structure of any road network, to learn detailed representations of traffic signal controllers and their surroundings. Our decentralized approach enables learning of a transferable-adaptive-traffic-signal-control policy. After being trained on an arbitrary set of road networks, our model can generalize to new road networks and traffic distributions, with no additional training and a constant number of parameters, enabling greater scalability compared to prior methods. Furthermore, our approach can exploit the granularity of available data by capturing the (dynamic) demand at both the lane level and the vehicle level. The proposed method is tested on both road networks and traffic settings never experienced during training. We compare IG-RL to multi-agent reinforcement learning and domain-specific baselines. In both synthetic road networks and in a larger experiment involving the control of the 3,971 traffic signals of Manhattan, we show that different instantiations of IG-RL outperform baselines. {\textcopyright} 2000-2011 IEEE.},
  langid = {english},
  keywords = {0 - DRL,0 - GNN,0 - RL,0 - Unmentioned,A - Smart Mobility,A - Traffic Signal Control,DRL - DQL,GNN - GCN,RL - Multi-Agent,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2022 - Devailly et al - IG-RL.pdf;../../../documents/school/dissertation/zotero/storage/3CCDSWBT/display.html}
}

@article{dolatabadiEnhancedIEEE332021,
  title = {An {{Enhanced IEEE}} 33 {{Bus Benchmark Test System}} for {{Distribution System Studies}}},
  author = {Dolatabadi, Sarineh Hacopian and Ghorbanian, Maedeh and Siano, Pierluigi and Hatziargyriou, Nikos D.},
  year = {2021},
  month = may,
  journal = {IEEE Transactions on Power Systems},
  volume = {36},
  number = {3},
  pages = {2565--2572},
  issn = {1558-0679},
  doi = {10.1109/TPWRS.2020.3038030},
  urldate = {2023-11-20},
  abstract = {The transformation of passive distribution systems to more active ones thanks to the increased penetration of distributed energy resources, such as dispersed generators, flexible demand, distributed storage, and electric vehicles, creates the necessity of an enhanced test system for distribution systems planning and operation studies. The value of the proposed test system, is that it provides an appropriate and comprehensive benchmark for future researches concerning distribution systems. The proposed test system is developed by modifying and updating the well-known 33 bus distribution system from Baran \& Wu. It comprises both forms of balanced and unbalanced three-phase power systems, including new details on the integration of distributed and renewable generation units, reactive power compensation assets, reconfiguration infrastructures and appropriate datasets of load and renewable generation profiles for different case studies.},
  keywords = {0 - Data,0 - Unmentioned,A - Smart Grid,D - IEEE33},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2021 - Dolatabadi et al - An Enhanced IEEE 33 Bus Benchmark Test System for Distribution System Studies.pdf;../../../documents/school/dissertation/zotero/storage/TWAQGRUG/references.html}
}

@book{dorronsoroOptimizationLearning6th2023,
  title = {Optimization and {{Learning}}: 6th {{International Conference}}, {{OLA}} 2023, {{Malaga}}, {{Spain}}, {{May}} 3{\textendash}5, 2023, {{Proceedings}}},
  shorttitle = {Optimization and {{Learning}}},
  editor = {Dorronsoro, Bernab{\'e} and Chicano, Francisco and Danoy, Gregoire and Talbi, El-Ghazali},
  year = {2023},
  series = {Communications in {{Computer}} and {{Information Science}}},
  volume = {1824},
  publisher = {{Springer Nature Switzerland}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-34020-8},
  urldate = {2023-11-17},
  isbn = {978-3-031-34019-2 978-3-031-34020-8},
  langid = {english},
  keywords = {0 - Unmentioned,0 - Untagged},
  file = {../../../documents/school/dissertation/bibliography/Book/2023 - Dorronsoro et al - Optimization and Learning.pdf}
}

@article{fanAttentionBasedMultiAgentGraph2023,
  title = {Attention-{{Based Multi-Agent Graph Reinforcement Learning}} for {{Service Restoration}}},
  author = {Fan, Bangji and Liu, Xinghua and Xiao, Gaoxi and Kang, Yu and Wang, Dianhui and Wang, Peng},
  year = {2023},
  journal = {IEEE Transactions on Artificial Intelligence},
  pages = {1--15},
  issn = {2691-4581},
  doi = {10.1109/TAI.2023.3314395},
  urldate = {2023-10-29},
  abstract = {With the ongoing integration of distributed energy resources, modern distribution systems are getting sufficient generation capacity to perform active restoration after outages without transmission system support. The model-based approaches are widely used in resolving service restoration problems, relying on accurate system models. Deep reinforcement learning is believed as an alternative solution for problem solving, although it has not been sufficiently explored. In this paper, the service restoration process is described as a partially observable Markov decision process, and a multi-agent graph reinforcement learning approach based on attention is proposed to train multiple agents to co-achieve the restoration goal to reinforce the system resilience in coping with extreme events. To consider the connections and correlations between nodes during the service restoration, the state of the active distribution network is defined by graph data that contains features of both topology and nodes. The perceived ability of the agents is empowered by graph convolutional networks during the feature extraction, supplying agents with more comprehensive data to learn more reasonable restoration strategies. In addition, the centralized training with attention is developed for multi-agent systems, focusing on the relations between the agents to strengthen teamwork capability. The performance of the proposed method is verified by a set of comparative studies on the IEEE-118 system with dispatchable generators, rooftop photovoltaics, and energy storage systems simultaneously.},
  keywords = {0 - Cited,0 - DRL,0 - GRL,0 - RL,A - Service Restoration,A - Smart Grid,D - IEEE118,D - UW,DRL - Actor-Critic,GNN - GCN,RL - Multi-Agent,T - Attention Mechanism,T - Self-Attention,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Fan et al - Attention-Based Multi-Agent Graph Reinforcement Learning for Service Restoration.pdf;../../../documents/school/dissertation/zotero/storage/FBLLVM5I/10247632.html}
}

@misc{faramafoundationGymnasiumDocumentation,
  title = {Gymnasium {{Documentation}}},
  author = {Farama Foundation},
  urldate = {2024-02-02},
  abstract = {A standard API for reinforcement learning and a diverse set of reference environments (formerly Gym)},
  howpublished = {https://gymnasium.farama.org/index.html},
  langid = {english},
  file = {../../../documents/school/dissertation/zotero/storage/K9L4TMA6/gymnasium.farama.org.html}
}

@article{farhangiPathSmartGrid2010,
  title = {The Path of the Smart Grid},
  author = {Farhangi, Hassan},
  year = {2010},
  month = jan,
  journal = {IEEE Power and Energy Magazine},
  volume = {8},
  number = {1},
  pages = {18--28},
  issn = {1558-4216},
  doi = {10.1109/MPE.2009.934876},
  urldate = {2023-11-20},
  abstract = {Exciting yet challenging times lie ahead. The electrical power industry is undergoing rapid change. The rising cost of energy, the mass electrification of everyday life, and climate change are the major drivers that will determine the speed at which such transformations will occur. Regardless of how quickly various utilities embrace smart grid concepts, technologies, and systems, they all agree onthe inevitability of this massive transformation. It is a move that will not only affect their business processes but also their organization and technologies.},
  keywords = {0 - Unmentioned,0 - Untagged,A - Smart Grid},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2010 - Farhangi - The path of the smart grid.pdf;../../../documents/school/dissertation/zotero/storage/GJ5C5KEP/5357331.html}
}

@misc{FeedforwardNeuralNetwork,
  title = {Feedforward {{Neural Network}}},
  urldate = {2023-12-13},
  howpublished = {https://orgs.mines.edu/daa/wp-content/uploads/sites/38/2019/08/1\_Gh5PS4R\_A5drl5ebd\_gNrg@2x.jpg},
  keywords = {0 - Cited,0 - Neural Network,Type - Picture},
  file = {../../../documents/school/dissertation/zotero/storage/225F5JGA/1_Gh5PS4R_A5drl5ebd_gNrg@2x.html}
}

@phdthesis{gadelhoApplicationGraphNeural2023,
  title = {Application of {{Graph Neural Networks}} in {{Road Traffic Forecasting}} for {{Intelligent Transportation Systems}}},
  author = {Gadelho, Ana Clara Moreira},
  year = {2023},
  month = jul,
  urldate = {2023-12-28},
  copyright = {openAccess},
  langid = {english},
  school = {FEUP},
  keywords = {0 - GNN,Type - Thesis},
  annotation = {Accepted: 2023-12-18T03:30:17Z},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Gadelho - Application of Graph Neural Networks in Road Traffic Forecasting for.pdf}
}

@article{gaoFastAdaptiveTask2023,
  title = {Fast {{Adaptive Task Offloading}} and {{Resource Allocation}} in {{Large-Scale MEC Systems}} via {{Multi-Agent Graph Reinforcement Learning}}},
  author = {Gao, Zhen and Yang, Lei and Dai, Yu},
  year = {2023},
  journal = {IEEE Internet of Things Journal},
  pages = {1--1},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2023.3285950},
  urldate = {2023-10-29},
  abstract = {In multi-access edge computing, when many mobile devices (MDs) offload their tasks to an edge server (ES), its resources might become constrained. These tasks may take a long time to complete or even be thrown away. Since the unknown information of both the ESs and other MDs, it is difficult for each MD to determine its offloading policy independently. Furthermore, most offloading methods have poor generalization to new environment since they focus on model architecture with a fixed quantity of MDs and ESs, preventing trained models from transferring to other environments. In the paper, we provide a full decentralized offloading scheme based on the Curriculum Attention-weighted Graph Recurrent Network-based Multi-Agent Actor-Critic (CAGR-MAAC). First, we build MEC as a shared MD agents-ESs graph and an AGR-based message network is designed to enable each MD aggregate the information of ESs and other MDs and solve the partial observability of MD agents for MEC system. Second, a learnable differentiable encoder network is introduced to construct MD agent's local information encoding. Subsequently, the MD agent converts overall the information regarding the MEC system into a fixed-size embedding via an AGR Network to handle different quantity of MDs and ESs. Finally, we introduce curriculum learning to address the huge complexity of the MEC system and the training difficulties induced by the large amounts of MDs and ESs. Experiments demonstrate that compared with existing algorithms, CAGR-MAAC boosts task completion rates and decreases system costs by 13.01\% 15.03\% and 16.45\% 18.56\%, and can quickly adapt to the new environment.},
  keywords = {0 - Unmentioned,0 - Untagged},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Gao et al - Fast Adaptive Task Offloading and Resource Allocation in Large-Scale MEC.pdf;../../../documents/school/dissertation/zotero/storage/MJ47DPDY/10153416.html}
}

@inproceedings{gaoGraphUNets2019,
  title = {Graph {{U-Nets}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Gao, Hongyang and Ji, Shuiwang},
  year = {2019},
  month = may,
  pages = {2083--2092},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2024-01-05},
  abstract = {We consider the problem of representation learning for graph data. Convolutional neural networks can naturally operate on images, but have significant challenges in dealing with graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully applied on many image pixel-wise prediction tasks, similar methods are lacking for graph data. This is due to the fact that pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling (gPool) and unpooling (gUnpool) operations in this work. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values on a trainable projection vector. We further propose the gUnpool layer as the inverse operation of the gPool layer. The gUnpool layer restores the graph into its original structure using the position information of nodes selected in the corresponding gPool layer. Based on our proposed gPool and gUnpool layers, we develop an encoder-decoder model on graph, known as the graph U-Nets. Our experimental results on node classification and graph classification tasks demonstrate that our methods achieve consistently better performance than previous models.},
  langid = {english},
  keywords = {0 - GNN,GNN - Graph U-Net,Type - Seminal},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2019 - Gao-Ji - Graph U-Nets.pdf}
}

@inproceedings{gaoMultiVehiclesDecisionMakingInteractive2022,
  title = {Multi-{{Vehicles Decision-Making}} in {{Interactive Highway Exit}}: {{A Graph Reinforcement Learning Approach}}},
  shorttitle = {Multi-{{Vehicles Decision-Making}} in {{Interactive Highway Exit}}},
  booktitle = {2022 {{IEEE}} 17th {{Conference}} on {{Industrial Electronics}} and {{Applications}} ({{ICIEA}})},
  author = {Gao, Xin and Luan, Tian and Li, Xueyuan and Liu, Qi and Li, Zirui and Yang, Fan},
  year = {2022},
  month = dec,
  pages = {534--539},
  issn = {2158-2297},
  doi = {10.1109/ICIEA54703.2022.10005925},
  urldate = {2023-10-29},
  abstract = {In the research of driverless decision-making, most of the current research is aimed at following, changing lanes, overtaking, and other scenarios. In this paper, algorithms and reward functions are designed to solve decision-making problems in interactive environments. An interaction and stochastic high-speed exit scenario of human-driven vehicles and autonomous vehicles (AVs) is designed. The features of the graph are directly extracted from the surrounding environment information of AVs by using the graph convolutional neural network (GCN). The steering angle and longitudinal acceleration are output through the neural network module. In addition, an exponentially increasing reward function based on driving purpose, traffic efficiency, driving comfort, and safety is designed. Moreover, this paper uses two algorithms, Graph Convolutional Deep Q-learning Network (GCN-DQN) and Graph Convolutional Double Deep Q-learning Network (GCN-DDQN), to train the driverless decision model and compare them with each other. According to simulation results, by adjusting the weight value in the reward function, the system can realize different control targets. Meanwhile, the decision-making model trained by GCN-DDQN has better performance under strong interactive random scenes than the GCN-DQN method.},
  keywords = {0 - Unmentioned,0 - Untagged},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2022 - Gao et al - Multi-Vehicles Decision-Making in Interactive Highway Exit.pdf;../../../documents/school/dissertation/zotero/storage/FTIFX4MA/10005925.html}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  isbn = {978-0-262-03561-3},
  lccn = {Q325.5 .G66 2016},
  keywords = {0 - Neural Network,Type - Book},
  file = {../../../documents/school/dissertation/bibliography/Book/2016 - Goodfellow et al - Deep learning.pdf}
}

@misc{gunarathnaSolvingDynamicGraph2022,
  title = {Solving {{Dynamic Graph Problems}} with {{Multi-Attention Deep Reinforcement Learning}}},
  author = {Gunarathna, Udesh and {Borovica-Gajic}, Renata and Karunasekara, Shanika and Tanin, Egemen},
  year = {2022},
  month = jan,
  number = {arXiv:2201.04895},
  eprint = {2201.04895},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.04895},
  urldate = {2023-12-18},
  abstract = {Graph problems such as traveling salesman problem, or finding minimal Steiner trees are widely studied and used in data engineering and computer science. Typically, in real-world applications, the features of the graph tend to change over time, thus, finding a solution to the problem becomes challenging. The dynamic version of many graph problems are the key for a plethora of real-world problems in transportation, telecommunication, and social networks. In recent years, using deep learning techniques to find heuristic solutions for NP-hard graph combinatorial problems has gained much interest as these learned heuristics can find near-optimal solutions efficiently. However, most of the existing methods for learning heuristics focus on static graph problems. The dynamic nature makes NP-hard graph problems much more challenging to learn, and the existing methods fail to find reasonable solutions. In this paper, we propose a novel architecture named Graph Temporal Attention with Reinforcement Learning (GTA-RL) to learn heuristic solutions for graph-based dynamic combinatorial optimization problems. The GTA-RL architecture consists of an encoder capable of embedding temporal features of a combinatorial problem instance and a decoder capable of dynamically focusing on the embedded features to find a solution to a given combinatorial problem instance. We then extend our architecture to learn heuristics for the real-time version of combinatorial optimization problems where all input features of a problem are not known a prior, but rather learned in real-time. Our experimental results against several state-of-the-art learning-based algorithms and optimal solvers demonstrate that our approach outperforms the state-of-the-art learning-based approaches in terms of effectiveness and optimal solvers in terms of efficiency on dynamic and real-time graph combinatorial optimization.},
  archiveprefix = {arxiv},
  keywords = {0 - RL,0 - Unmentioned,A - Travelling Salesman Problem,A - Vehicle Routing,DRL - REINFORCE,GNN - Graph Temporal Attention,RL - Multi-Agent,T - Attention Mechanism,T - Multi-head Attention},
  file = {../../../documents/school/dissertation/bibliography/Preprint/2022 - Gunarathna et al - Solving Dynamic Graph Problems with Multi-Attention Deep Reinforcement Learning.pdf;../../../documents/school/dissertation/zotero/storage/43KFVWXZ/2201.html}
}

@inproceedings{guPowerSystemTransient2022,
  title = {Power {{System Transient Stability Assessment Based}} on {{Graph Neural Network}} with {{Interpretable Attribution Analysis}}},
  booktitle = {2022 {{4TH INTERNATIONAL CONFERENCE ON SMART POWER}} \& {{INTERNET ENERGY SYSTEMS}}, {{SPIES}}},
  author = {Gu, Sili and Qiao, Ji and Zhao, Zixuan and Zhu, Qiongfeng and Han, Fujia},
  year = {2022},
  pages = {1374--1379},
  publisher = {{IEEE}},
  address = {{New York}},
  doi = {10.1109/SPIES55999.2022.10082542},
  urldate = {2023-11-07},
  abstract = {The power transient stability analysis is one of the basis for determining the control strategy of power system security and stability. Considering the influence of power grid topology on the transient stability of power system, the transient stability evaluation model is constructed based on the graph attention neural network. The electrical components and their transient operation data are mapped to the graph data with the spatial topology characteristics of power system for model training, so as to improve the topological generalization performance of the model. The marginal contribution of input characteristics to the output of transient power angle stability evaluation model is quantitatively calculated based on Shapley additive explanation (SHAP), so as to improve the interpretability of data-driven method for transient power angle stability evaluation. The effectiveness of the proposed method is verified by the IEEE 39-bus system.},
  isbn = {978-1-66548-957-7},
  langid = {english},
  keywords = {0 - GNN,0 - Unmentioned,A - Smart Grid,A - Transient Stability Assessment,D - IEEE39,D - No Source,GNN - GAT,GNN - GCN,T - Shapley Additive Explanation,Type - Article},
  annotation = {Web of Science ID: WOS:000994420700246},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2022 - Gu et al - Power System Transient Stability Assessment Based on Graph Neural Network with.pdf}
}

@inproceedings{haarnojaSoftActorCriticOffPolicy2018,
  title = {Soft {{Actor-Critic}}: {{Off-Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  shorttitle = {Soft {{Actor-Critic}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  year = {2018},
  month = jul,
  pages = {1861--1870},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-12-16},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  langid = {english},
  keywords = {0 - DRL,0 - RL,DRL - Actor-Critic,DRL - SAC,RL - Single Agent,Type - Seminal},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2018 - Haarnoja et al - Soft Actor-Critic.pdf;../../../documents/school/dissertation/bibliography/Conference Paper/2018 - Haarnoja et al - Soft Actor-Critic2.pdf}
}

@article{hamiltonGraphRepresentationLearning,
  title = {Graph {{Representation Learning}}},
  author = {Hamilton, William L},
  abstract = {Graph-structured data is ubiquitous throughout the natural and social sciences, from telecommunication networks to quantum chemistry. Building relational inductive biases into deep learning architectures is crucial if we want systems that can learn, reason, and generalize from this kind of data. Recent years have seen a surge in research on graph representation learning, including techniques for deep graph embeddings, generalizations of convolutional neural networks to graph-structured data, and neural message-passing approaches inspired by belief propagation. These advances in graph representation learning have led to new state-of-the-art results in numerous domains, including chemical synthesis, 3D-vision, recommender systems, question answering, and social network analysis.},
  langid = {english},
  keywords = {0 - GNN,Type - Book},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/Hamilton - Graph Representation Learning.pdf}
}

@article{hanAutonomousControlTechnology2023,
  title = {An Autonomous Control Technology Based on Deep Reinforcement Learning for Optimal Active Power Dispatch},
  author = {Han, Xiaoyun and Mu, Chaoxu and Yan, Jun and Niu, Zeyuan},
  year = {2023},
  month = feb,
  journal = {International Journal of Electrical Power \& Energy Systems},
  volume = {145},
  pages = {108686},
  issn = {0142-0615},
  doi = {10.1016/j.ijepes.2022.108686},
  urldate = {2024-01-12},
  abstract = {The large-scale renewable energy integration has brought challenges to energy management in modern power systems. Due to the strong randomness and volatility of renewable energy, traditional model-based methods may become insufficient for optimal active power dispatch. To tackle the challenge, this paper proposes an autonomous control method based on soft actor{\textendash}critic (SAC), a deep-reinforcement learning (DRL) strategy recently developed, which provides an optimal solution for active power dispatch without a mathematical model while improving the renewable energy consumption rate under stable operation. A Lagrange multiplier is introduced to the SAC (LM-SAC) to promote algorithm performance in optimal active power dispatch. A pre-trained scheme based on imitation learning (IL-SAC) is also designed to further improve the training efficiency and robustness of the DRL agent. Simulations on the IEEE 118-bus system with the open platform Grid2Op verify that the proposed algorithm effectively achieves better renewable energy consumption rate and robustness compared with existing DRL algorithms.},
  keywords = {0 - DRL,0 - RL,A - Dynamic Economic Dispatch,A - Smart Grid,D - Grid2Op,DRL - Actor-Critic,DRL - SAC,RL - Single Agent,T - Imitation Learning,T - Lagrange Multiplier},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Han et al - An autonomous control technology based on deep reinforcement learning for.pdf;../../../documents/school/dissertation/zotero/storage/N7CLCMNL/S0142061522006822.html}
}

@inproceedings{hazraDynamicServiceDeployment2022,
  title = {Dynamic {{Service Deployment Strategy}} Using {{Reinforcement Learning}} in {{Edge Networks}}},
  booktitle = {2022 {{International Conference}} on {{Computing}}, {{Communication}}, {{Security}} and {{Intelligent Systems}} ({{IC3SIS}})},
  author = {Hazra, Abhishek and Adhikari, Mainak and Amgoth, Tarachand},
  year = {2022},
  month = jun,
  pages = {1--6},
  doi = {10.1109/IC3SIS54991.2022.9885498},
  urldate = {2023-10-29},
  abstract = {Edge computing has lately appeared as a practical entrance to intensify the capabilities of the industrial networks by transferring the computation data to the edge of the networks. However, the challenge here is to locate processing devices and provide services through the shortest path by making a proper trade-off between latency and energy of the processing. These demands can be solved using complex optimization problems within a confined deadline, which is barely feasible with traditional statistical techniques. Thus, we propose a heuristic-based task controlling mechanism that makes service deployment decisions by eliminating the need to solve combinatorial optimization problems. Moreover, to achieve the least possible delay and find the best possible path towards the goal, we adopt Graph Reinforcement Learning (GRL) technique. Extensive numerical results illustrate that our proposed strategy generates near-optimal solutions within the given time-bound.},
  keywords = {0 - GRL,0 - Unmentioned,0 - Untagged,A - Edge Computing,A - Internet of Things,A - Service Deployment,GRL - Graph Q-Learning,RL - Q-Learning},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2022 - Hazra et al - Dynamic Service Deployment Strategy using Reinforcement Learning in Edge.pdf;../../../documents/school/dissertation/zotero/storage/V88ZNUJJ/9885498.html}
}

@article{hesselRainbowCombiningImprovements2018,
  title = {Rainbow: {{Combining Improvements}} in {{Deep Reinforcement Learning}}},
  shorttitle = {Rainbow},
  author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  year = {2018},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468},
  doi = {10.1609/aaai.v32i1.11796},
  urldate = {2023-10-31},
  abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {0 - DRL,0 - RL,DRL - DQL,DRL - Rainbow,RL - Single Agent,Type - Seminal},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2018 - Hessel et al - Rainbow.pdf}
}

@article{huMultiAgentDeepReinforcement2022,
  title = {Multi-{{Agent Deep Reinforcement Learning}} for {{Voltage Control With Coordinated Active}} and {{Reactive Power Optimization}}},
  author = {Hu, Daner and Ye, Zhenhui and Gao, Yuanqi and Ye, Zuzhao and Peng, Yonggang and Yu, Nanpeng},
  year = {2022},
  month = nov,
  journal = {IEEE Transactions on Smart Grid},
  volume = {13},
  number = {6},
  pages = {4873--4886},
  issn = {1949-3061},
  doi = {10.1109/TSG.2022.3185975},
  urldate = {2023-11-17},
  abstract = {The increasing penetration of distributed renewable energy resources causes voltage fluctuations in distribution networks. The controllable active and reactive power resources such as energy storage (ES) systems and electric vehicles (EVs) in active distribution networks play an important role in mitigating the voltage excursions. This paper proposes a two-timescale hybrid voltage control strategy based on a mixed-integer optimization method and multi-agent reinforcement learning (MARL) to reduce power loss and mitigate voltage violations. In the slow-timescale, the active and reactive power optimization problem involving capacitor banks (CBs), on-load tap changers (OLTC), and ES systems is formulated as a mixed-integer second-order cone programming problem. In the fast-timescale, the reactive power of smart inverters connected to solar photovoltaic systems and active power of EVs are adjusted to mitigate short-term voltage fluctuations with a MARL algorithm. Specifically, we propose an experience augmented multi-agent actor-critic (EA-MAAC) algorithm with an attention mechanism to learn high-quality control policies. The control policies are executed online in a decentralized manner. The proposed hybrid voltage control strategy is validated on an IEEE testing distribution feeder. The numerical results show that our proposed control strategy is not only sample-efficient and robust but also effective in mitigating voltage fluctuations.},
  keywords = {0 - DRL,0 - GNN,0 - RL,0 - Unmentioned,A - Smart Grid,A - Voltage Regulation,D - IEEE123,D - IEEE33,D - Network Paper,D - PES Test Feeder,DRL - Actor-Critic,DRL - SAC,GNN - GAT,RL - Multi-Agent,T - Attention Mechanism,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2022 - Hu et al - Multi-Agent Deep Reinforcement Learning for Voltage Control With Coordinated.pdf;../../../documents/school/dissertation/zotero/storage/C8DXW5ET/9805763.html}
}

@article{huMultiagentGraphReinforcement2024,
  title = {Multi-Agent Graph Reinforcement Learning for Decentralized {{Volt-VAR}} Control in Power Distribution Systems},
  author = {Hu, Daner and Li, Zichen and Ye, Zhenhui and Peng, Yonggang and Xi, Wei and Cai, Tiantian},
  year = {2024},
  month = jan,
  journal = {International Journal of Electrical Power \& Energy Systems},
  volume = {155},
  pages = {109531},
  issn = {0142-0615},
  doi = {10.1016/j.ijepes.2023.109531},
  urldate = {2023-10-29},
  abstract = {Volt/Var control (VVC) is a crucial function in power distribution systems to minimize power loss and maintain voltages within allowable limits. However, incomplete and inaccurate information about the distribution network makes model-based VVC methods difficult to implement in practice. In this paper, we propose a novel multi-agent graph-based deep reinforcement learning (DRL) algorithm named MASAC-HGRN to address the VVC problem under partial observation constraints. Our proposed algorithm divides the power distribution system into several regions, each region treated as an agent. Unlike traditional model-based or global-observation-based DRL methods, our proposed method leverages a practical decentralized training and decentralized execution (DTDE) paradigm to address the partial observation constraints. The well-trained agents gather information only from their interconnected neighbors and realize decentralized local control. Numerical studies with IEEE 33-bus and 123-bus distribution test feeders demonstrate that our proposed MASAC-HGRN algorithm outperforms the state-of-art RL algorithms and traditional model-based approaches in terms of VVC performance. Moreover, the DTDE framework exhibits flexibility and robustness in extensive robustness experiments.},
  keywords = {0 - DRL,0 - GNN,0 - RL,0 - Unmentioned,0 - Untagged,A - High Penetration of Distributed Generation,A - Smart Grid,A - Volt/VAR control,DRL - DQL,DRL - DQN,GNN - GAT,RL - Multi-Agent},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2024 - Hu et al - Multi-agent graph reinforcement learning for decentralized Volt-VAR control in.pdf;../../../documents/school/dissertation/zotero/storage/4SYPSTI8/S0142061523005884.html}
}

@inproceedings{jiangCooperativePlanningMultiUAV2023,
  title = {Cooperative Planning of Multi-{{UAV}} Logistics Delivery by Multi-Graph Reinforcement Learning},
  booktitle = {International {{Conference}} on {{Computer Application}} and {{Information Security}} ({{ICCAIS}} 2022)},
  author = {Jiang, Zhiling and Chen, Yining and Song, Guanghua and Yang, Bowei and Jiang, Xiaohong},
  year = {2023},
  month = mar,
  volume = {12609},
  pages = {129--137},
  publisher = {{SPIE}},
  doi = {10.1117/12.2671868},
  urldate = {2023-10-29},
  abstract = {In this paper, we design a deep reinforcement learning algorithm based on graph neural network to solve the problem of cooperation control of multiple UAVs. Our algorithm can control multiple UAV swarms to complete package delivery tasks in an unexplored area under partial observation. Since each UAV has only a limited observation space and a small range of communication, we propose a reinforcement learning algorithm based on graph neural network, which can process multiple graphs simultaneously time and aggregate the feature vectors of the neighbor agents to address the cooperative issue of heterogeneous multi-agent coordination. We conduct a couple of ablation experiments to prove the effectiveness and performance characteristics of our algorithm.},
  keywords = {0 - GRL,0 - Unmentioned,0 - Untagged,A - Smart Mobility,RL - Multi-Agent,Type - Article}
}

@misc{jiangGraphConvolutionalReinforcement2020,
  title = {Graph {{Convolutional Reinforcement Learning}}},
  author = {Jiang, Jiechuan and Dun, Chen and Huang, Tiejun and Lu, Zongqing},
  year = {2020},
  month = feb,
  number = {arXiv:1810.09202},
  eprint = {1810.09202},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.09202},
  urldate = {2023-12-18},
  abstract = {Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.},
  archiveprefix = {arxiv},
  keywords = {0 - DRL,0 - RL,0 - Unmentioned,DRL - DQL,DRL - DQN,GNN - GCN,RL - Multi-Agent,T - Attention Mechanism,T - Multi-head Attention,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Preprint/2020 - Jiang et al - Graph Convolutional Reinforcement Learning.pdf;../../../documents/school/dissertation/zotero/storage/45KNEMFV/1810.html}
}

@inproceedings{jiangPathSpuriousnessawareReinforcement2023,
  title = {Path {{Spuriousness-aware Reinforcement Learning}} for {{Multi-Hop Knowledge Graph Reasoning}}},
  booktitle = {Proceedings of the 17th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Jiang, Chunyang and Zhu, Tianchen and Zhou, Haoyi and Liu, Chang and Deng, Ting and Hu, Chunming and Li, Jianxin},
  year = {2023},
  pages = {3181--3192},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dubrovnik, Croatia}},
  doi = {10.18653/v1/2023.eacl-main.232},
  urldate = {2023-10-29},
  abstract = {Multi-hop reasoning, a prevalent approach for query answering, aims at inferring new facts along reasonable paths over a knowledge graph. Reinforcement learning (RL) methods can be adopted by formulating the problem into a Markov decision process. However, common suffering within RL-based reasoning models is that the agent can be biased to spurious paths which coincidentally lead to the correct answer with poor explanation. In this work, we take a deep dive into this phenomenon and define a metric named Path Spuriousness (PS), to quantitatively estimate to what extent a path is spurious. Guided by the definition of PS, we design a model with a new reward that considers both answer accuracy and path reasonableness. We test our method on five datasets and experiments reveal that our method considerably enhances the agent's capacity to prevent spurious paths while keeping comparable to state-of-the-art performance.},
  langid = {english},
  keywords = {0 - Unmentioned,0 - Untagged},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2023 - Jiang et al - Path Spuriousness-aware Reinforcement Learning for Multi-Hop Knowledge Graph.pdf}
}

@inproceedings{johnnGRAPHReinforcementLearning2023,
  title = {{{GRAPH Reinforcement Learning}} for~{{Operator Selection}} in~the~{{ALNS Metaheuristic}}},
  booktitle = {Optimization and {{Learning}}},
  author = {Johnn, Syu-Ning and Darvariu, Victor-Alexandru and Handl, Julia and Kalcsics, Joerg},
  editor = {Dorronsoro, Bernab{\'e} and Chicano, Francisco and Danoy, Gregoire and Talbi, El-Ghazali},
  year = {2023},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {200--212},
  publisher = {{Springer Nature Switzerland}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-34020-8_15},
  abstract = {ALNS is a popular metaheuristic with renowned efficiency in solving combinatorial optimisation problems. However, despite 16 years of intensive research into ALNS, whether the embedded adaptive layer can efficiently select operators to improve the incumbent remains an open question. In this work, we formulate the choice of operators as a Markov Decision Process, and propose a practical approach based on Deep Reinforcement Learning and Graph Neural Networks. The results show that our proposed method achieves better performance than the classic ALNS adaptive layer due to the choice of operator being conditioned on the current solution. We also discuss important considerations such as the size of the operator portfolio and the impact of the choice of operator scales. Notably, our approach can also save significant time and labour costs for handcrafting problem-specific operator portfolios.},
  isbn = {978-3-031-34020-8},
  langid = {english},
  keywords = {0 - GNN,0 - Unmentioned,0 - Untagged,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2023 - Johnn et al - GRAPH Reinforcement Learning for Operator Selection in the ALNS Metaheuristic.pdf}
}

@article{kaelblingReinforcementLearningSurvey1996,
  title = {Reinforcement {{Learning}}: {{A Survey}}},
  shorttitle = {Reinforcement {{Learning}}},
  author = {Kaelbling, L. P. and Littman, M. L. and Moore, A. W.},
  year = {1996},
  month = may,
  journal = {Journal of Artificial Intelligence Research},
  volume = {4},
  pages = {237--285},
  issn = {1076-9757},
  doi = {10.1613/jair.301},
  urldate = {2023-09-21},
  abstract = {This paper surveys the field of reinforcement learning from    a computer-science perspective. It is written to be accessible to    researchers familiar with machine learning.  Both the historical basis    of the field and a broad selection of current work are summarized.    Reinforcement learning is the problem faced by an agent that learns    behavior through trial-and-error interactions with a dynamic    environment.  The work described here has a resemblance to work in    psychology, but differs considerably in the details and in the use of    the word ``reinforcement.''  The paper discusses central issues of    reinforcement learning, including trading off exploration and    exploitation, establishing the foundations of the field via Markov    decision theory, learning from delayed reinforcement, constructing    empirical models to accelerate learning, making use of generalization    and hierarchy, and coping with hidden state.  It concludes with a    survey of some implemented systems and an assessment of the practical    utility of current methods for reinforcement learning.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {0 - RL,0 - Unmentioned,Type - Survey},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/1996 - Kaelbling et al - Reinforcement Learning.pdf}
}

@inproceedings{kerstingRadialDistributionTest2001,
  title = {Radial Distribution Test Feeders},
  booktitle = {2001 {{IEEE Power Engineering Society Winter Meeting}}. {{Conference Proceedings}} ({{Cat}}. {{No}}.{{01CH37194}})},
  author = {Kersting, W.H.},
  year = {2001},
  month = jan,
  volume = {2},
  pages = {908-912 vol.2},
  doi = {10.1109/PESW.2001.916993},
  urldate = {2024-01-12},
  abstract = {Many computer programs are available for the analysis of radial distribution feeders. In 1992 a paper was published that presented the complete data for three four-wire wye and one three-wire delta radial distribution test feeders. The purpose of publishing the data was to make available a common set of data that could be used by program developers and users to verify the correctness of their solutions. This paper presents an updated version of the same test feeders along with a simple system that can be used to test three-phase transformer models.},
  keywords = {0 - Data,A - Smart Grid,D - PES Test Feeder},
  annotation = {668 citations (Crossref) [2024-01-12]},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2001 - Kersting - Radial distribution test feeders.pdf;../../../documents/school/dissertation/zotero/storage/4ZGSK335/916993.html}
}

@article{khoslaDeepReinforcementLearning2020,
  title = {Deep {{Reinforcement Learning}} with {{Graph-based State Representations}}},
  author = {Khosla, Megha},
  year = {2020},
  month = apr,
  urldate = {2023-11-20},
  abstract = {Deep RL approaches build much of their success on the ability of the deep neural network to generate useful internal representations. Nevertheless, they suffer from a high sample-complexity and starting with a good input representation can have a},
  keywords = {0 - Unmentioned,0 - Untagged},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2020 - Khosla - Deep Reinforcement Learning with Graph-based State Representations.pdf;../../../documents/school/dissertation/zotero/storage/EUPSJ8WU/Deep_Reinforcement_Learning_with_Graph_based_State_Representations.html}
}

@misc{kipfSemiSupervisedClassificationGraph2017,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2017},
  month = feb,
  number = {arXiv:1609.02907},
  eprint = {1609.02907},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1609.02907},
  urldate = {2023-11-06},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  archiveprefix = {arxiv},
  keywords = {0 - GNN,GNN - GCN,Type - Seminal},
  file = {../../../documents/school/dissertation/bibliography/Preprint/2017 - Kipf-Welling - Semi-Supervised Classification with Graph Convolutional Networks.pdf;../../../documents/school/dissertation/zotero/storage/SH3Y8XZX/1609.html}
}

@article{kolterREDDPublicData,
  title = {{{REDD}}: {{A Public Data Set}} for {{Energy Disaggregation Research}}},
  author = {Kolter, J Zico and Johnson, Matthew J},
  abstract = {Energy and sustainability issues raise a large number of problems that can be tackled using approaches from data mining and machine learning, but traction of such problems has been slow due to the lack of publicly available data. In this paper we present the Reference Energy Disaggregation Data Set (REDD), a freely available data set containing detailed power usage information from several homes, which is aimed at furthering research on energy disaggregation (the task of determining the component appliance contributions from an aggregated electricity signal). We discuss past approaches to disaggregation and how they have influenced our design choices in collecting data, we describe the hardware and software setups for the data collection, and we present initial benchmark disaggregation results using a well-known Factorial Hidden Markov Model (FHMM) technique.},
  langid = {english},
  keywords = {0 - Data,A - Smart Grid,D - REDD},
  file = {../../../documents/school/dissertation/zotero/storage/HU49H4SW/Kolter and Johnson - REDD A Public Data Set for Energy Disaggregation .pdf}
}

@inproceedings{koolAttentionLearnSolve2018,
  title = {Attention, {{Learn}} to {{Solve Routing Problems}}!},
  booktitle = {{{arXiv}}.Org},
  author = {Kool, Wouter and {van Hoof}, Herke and Welling, Max},
  year = {2018},
  month = mar,
  urldate = {2024-01-02},
  abstract = {The recently presented idea to learn heuristics for combinatorial optimization problems is promising as it can save costly development. However, to push this idea towards practical implementation, we need better models and better ways of training. We contribute in both directions: we propose a model based on attention layers with benefits over the Pointer Network and we show how to train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which we find is more efficient than using a value function. We significantly improve over recent learned heuristics for the Travelling Salesman Problem (TSP), getting close to optimal results for problems up to 100 nodes. With the same hyperparameters, we learn strong heuristics for two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP), outperforming a wide range of baselines and getting results close to highly optimized and specialized algorithms.},
  langid = {english},
  keywords = {0 - Unmentioned,0 - Untagged},
  file = {../../../documents/school/dissertation/bibliography/Web Page/2018 - Kool et al - Attention, Learn to Solve Routing Problems.pdf}
}

@book{krieselBriefIntroductionNeural2017,
  title = {A {{Brief Introduction}} to {{Neural Networks}}},
  author = {Kriesel, David},
  year = {2017},
  month = sep,
  urldate = {2023-11-06},
  abstract = {A Brief Introduction to Neural Networks  Manuscript Download - Zeta2 Version  Filenames are subject to change. Thus, if you place links, please do so with this subpage as target.      Original version    eBookReader optimized   English   [PDF], 6.2MB, 244 pages},
  chapter = {2014-12-18T12:57:47+01:00},
  langid = {english},
  keywords = {0 - Neural Network,Type - Book},
  file = {../../../documents/school/dissertation/bibliography/Book/2017 - Kriesel - A Brief Introduction to Neural Networks.pdf;../../../documents/school/dissertation/zotero/storage/RNEWJEHX/neural_networks.html}
}

@book{kumarConceptsTechniquesGraph2023,
  title = {Concepts and {{Techniques}} of {{Graph Neural Networks}}},
  author = {Kumar, Vinod and Rajput, Dharmendra S.},
  year = {2023},
  langid = {english},
  keywords = {0 - GNN,GNN - GAT,GNN - GCN,Type - Book},
  file = {../../../documents/school/dissertation/bibliography/Book/2023 - Kumar-Rajput - Concepts and Techniques of Graph Neural Networks.pdf}
}

@incollection{kumarFundamentalsGraphGraph2023,
  title = {Fundamentals of {{Graph}} for {{Graph Neural Network}}},
  booktitle = {Concepts and {{Techniques}} of {{Graph Neural Networks}}},
  author = {Kumar, Vinod and Prajapati, Himanshu and Ponnusamy, Sasikala},
  year = {2023},
  pages = {1--18},
  publisher = {{IGI Global}},
  doi = {10.4018/978-1-6684-6903-3.ch001},
  urldate = {2023-12-19},
  abstract = {The vertices, which are also known as nodes or points, and the edges, which are responsible for connecting the vertices to one another, are the two primary components that make up a graph. Graph theory is the mathematical study of graphs, which are structures that are used to depict relations betwee...},
  copyright = {Access limited to members},
  isbn = {978-1-66846-903-3},
  langid = {english},
  keywords = {0 - GNN,Type - Book}
}

@article{leiDynamicEnergyDispatch2021,
  title = {Dynamic {{Energy Dispatch Based}} on {{Deep Reinforcement Learning}} in {{IoT-Driven Smart Isolated Microgrids}}},
  author = {Lei, Lei and Tan, Yue and Dahlenburg, Glenn and Xiang, Wei and Zheng, Kan},
  year = {2021},
  month = may,
  journal = {IEEE Internet of Things Journal},
  volume = {8},
  number = {10},
  pages = {7938--7953},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2020.3042007},
  urldate = {2024-01-30},
  abstract = {Microgrids (MGs) are small, local power grids that can operate independently from the larger utility grid. Combined with the Internet of Things (IoT), a smart MG can leverage the sensory data and machine learning techniques for intelligent energy management. This article focuses on deep reinforcement learning (DRL)-based energy dispatch for IoT-driven smart isolated MGs with diesel generators (DGs), photovoltaic (PV) panels, and a battery. A finite-horizon partial observable Markov decision process (POMDP) model is formulated and solved by learning from historical data to capture the uncertainty in future electricity consumption and renewable power generation. In order to deal with the instability problem of DRL algorithms and unique characteristics of finite-horizon models, two novel DRL algorithms, namely, finite-horizon deep deterministic policy gradient (FH-DDPG) and finite-horizon recurrent deterministic policy gradient (FH-RDPG), are proposed to derive energy dispatch policies with and without fully observable state information. A case study using real isolated MG data is performed, where the performance of the proposed algorithms are compared with the other baseline DRL and non-DRL algorithms. Moreover, the impact of uncertainties on MG performance is decoupled into two levels and evaluated, respectively.},
  keywords = {A - Dynamic Economic Dispatch},
  annotation = {42 citations (Crossref) [2024-01-30]},
  file = {../../../documents/dissertation/bibliography/Journal Article/2021 - Lei et al - Dynamic Energy Dispatch Based on Deep Reinforcement Learning in IoT-Driven.pdf;../../../documents/school/dissertation/zotero/storage/3LSK3AQT/9277511.html}
}

@article{lengGraphConvolutionalNetworkbased2021,
  title = {Graph Convolutional Network-Based Reinforcement Learning for Tasks Offloading in Multi-Access Edge Computing},
  author = {Leng, Lixiong and Li, Jingchen and Shi, Haobin and Zhu, Yi'an},
  year = {2021},
  month = aug,
  journal = {Multimedia Tools and Applications},
  volume = {80},
  number = {19},
  pages = {29163--29175},
  issn = {1573-7721},
  doi = {10.1007/s11042-021-11130-5},
  urldate = {2024-01-05},
  abstract = {To achieve high quality of service for computation-intensive applications, multi-access edge computing (MEC) is proposed for offloading tasks to MEC servers. The emerging reinforcement learning-based task offloading strategies have attracted attention of researchers, but the incomplete Markov models in them result in limited improvements. This work proposes a graph convolutional network-based reinforcement learning (GRL-based) method to enhance the reinforcement learning-based task offloading in MEC. The Graph Convolutional Network is introduced to extract features from tasks through regarding the task set as a directed acyclic graph. Then we construct a complete Markov model for the offloading strategy. In the proposed GRL-based method, the decision process is deployed in the user layer, while the training process is deployed in the cloud layer. An off-policy reinforcement learning method, soft actor-critic, is used to train the offloading strategy, by which the sampling and training can be implemented separately. Several simulation experiments show the proposed GRL-based method performs better than baseline methods, and it can achieve continuous decisions for task offloading efficiently.},
  langid = {english},
  keywords = {0 - Cited,0 - DRL,0 - GRL,0 - RL,A - MEC,A - Task Offloading,DRL - Actor-Critic,DRL - SAC,GNN - GCN,RL - Single Agent,T - Long Short-Term Memory,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2021 - Leng et al - Graph convolutional network-based reinforcement learning for tasks offloading.pdf}
}

@misc{liangAcceleratedPrimalDualPolicy2018,
  title = {Accelerated {{Primal-Dual Policy Optimization}} for {{Safe Reinforcement Learning}}},
  author = {Liang, Qingkai and Que, Fanyu and Modiano, Eytan},
  year = {2018},
  month = feb,
  number = {arXiv:1802.06480},
  eprint = {1802.06480},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.06480},
  urldate = {2023-11-16},
  abstract = {Constrained Markov Decision Process (CMDP) is a natural framework for reinforcement learning tasks with safety constraints, where agents learn a policy that maximizes the long-term reward while satisfying the constraints on the long-term cost. A canonical approach for solving CMDPs is the primal-dual method which updates parameters in primal and dual spaces in turn. Existing methods for CMDPs only use on-policy data for dual updates, which results in sample inefficiency and slow convergence. In this paper, we propose a policy search method for CMDPs called Accelerated Primal-Dual Optimization (APDO), which incorporates an off-policy trained dual variable in the dual update procedure while updating the policy in primal space with on-policy likelihood ratio gradient. Experimental results on a simulated robot locomotion task show that APDO achieves better sample efficiency and faster convergence than state-of-the-art approaches for CMDPs.},
  archiveprefix = {arxiv},
  keywords = {0 - RL,0 - Unmentioned,0 - Untagged,DRL - Primal-Dual Policy Optimization,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Preprint/2018 - Liang et al - Accelerated Primal-Dual Policy Optimization for Safe Reinforcement Learning.pdf;../../../documents/school/dissertation/zotero/storage/QVK6G8VM/1802.html}
}

@article{lienertGMTakesTesla2022,
  title = {{{GM}} Takes on {{Tesla}} in Home and Commercial Energy Storage, Management},
  author = {Lienert, Paul},
  year = {2022},
  month = oct,
  journal = {Reuters},
  urldate = {2024-02-03},
  abstract = {It is expanding beyond car making, with plans to offer energy storage and management services through its new GM Energy unit.},
  chapter = {Autos \& Transportation},
  langid = {english},
  file = {../../../documents/school/dissertation/zotero/storage/3Z2PMRQI/gm-takes-tesla-home-commercial-energy-storage-management-2022-10-11.html}
}

@inproceedings{liGatedGraphSequence2016,
  title = {Gated Graph Sequence Neural Networks},
  booktitle = {4th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2016 - {{Conference Track Proceedings}}},
  author = {Li, Y. and Zemel, R. and Brockschmidt, M. and Tarlow, D.},
  year = {2016},
  abstract = {Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be described as abstract data structures. {\textcopyright} ICLR 2016: San Juan, Puerto Rico. All Rights Reserved.},
  langid = {english},
  keywords = {0 - GNN,GNN - GGNN,Type - Seminal},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2016 - Li et al - Gated graph sequence neural networks.pdf;../../../documents/school/dissertation/zotero/storage/TTFRF4JP/display.html}
}

@inproceedings{liGraphReinforcementLearningbased2022,
  title = {Graph {{Reinforcement Learning-based CNN Inference Offloading}} in {{Dynamic Edge Computing}}},
  booktitle = {{{GLOBECOM}} 2022 - 2022 {{IEEE Global Communications Conference}}},
  author = {Li, Nan and Iosifidis, Alexandros and Zhang, Qi},
  year = {2022},
  month = dec,
  pages = {982--987},
  doi = {10.1109/GLOBECOM48099.2022.10001067},
  urldate = {2023-10-29},
  abstract = {This paper studies the computational offloading of CNN inference in dynamic multi-access edge computing (MEC) networks. To address the uncertainties in communication time and Edge servers' available capacity, we use early-exit mechanism to terminate the computation earlier to meet the deadline of inference tasks. We design a reward function to trade off the communication, computation and inference accuracy, and formu-late the offloading problem of CNN inference as a maximization problem with the goal of maximizing the average inference accuracy and throughput in long term. To solve the maxi-mization problem, we propose a graph reinforcement learning-based early-exit mechanism (GRLE), which outperforms the state-of-the-art work, deep reinforcement learning-based online offloading (DROO) and its enhanced method, DROO with early-exit mechanism (DROOE), under different dynamic scenarios. The experimental results show that G RLE achieves the average accuracy up to 3.41 x over graph reinforcement learning (GRL) and 1.45x over DROOE, which shows the advantages of GRLE for offloading decision-making in dynamic MEC.},
  keywords = {0 - DRL,0 - RL,0 - Unmentioned,0 - Untagged,A - Edge Computing,A - MEC,DRL - Actor-Critic,T - CNN},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2022 - Li et al - Graph Reinforcement Learning-based CNN Inference Offloading in Dynamic Edge.pdf;../../../documents/school/dissertation/zotero/storage/FQHCI7XC/10001067.html}
}

@misc{lillicrapContinuousControlDeep2019,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  year = {2019},
  month = jul,
  number = {arXiv:1509.02971},
  eprint = {1509.02971},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1509.02971},
  urldate = {2023-12-16},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  archiveprefix = {arxiv},
  keywords = {0 - Unmentioned,0 - Untagged},
  file = {../../../documents/school/dissertation/bibliography/Preprint/2019 - Lillicrap et al - Continuous control with deep reinforcement learning2.pdf;../../../documents/school/dissertation/zotero/storage/NFG58TB6/1509.html}
}

@inproceedings{linEmergencyControlPower2022,
  title = {Emergency {{Control}} of {{Power Grid}} under {{Topology Changes Based}} on {{Graph Reinforcement Learning}}},
  booktitle = {2022 12th {{International Conference}} on {{Power}} and {{Energy Systems}} ({{ICPES}})},
  author = {Lin, Hanxing and Chen, Jinyu and Chen, Wenxin and Chen, Zihan},
  year = {2022},
  month = dec,
  pages = {498--503},
  issn = {2767-732X},
  doi = {10.1109/ICPES56491.2022.10073160},
  urldate = {2023-10-29},
  abstract = {The current power grid topology changes frequently, and power angle instability and voltage instability often occur at the same time. Emergency control methods based on physical characteristics are usually difficult to model in complex power systems, and have poor adaptability to power system structural changes. The traditional analysis methods based on the physical characteristics of the power grid can no longer meet the requirements of power grid control, and new methods need to be found to solve this problem. In this paper, an emergency control method based on graph reinforcement learning (GRL) is proposed for the emergency control problem of AC/DC hybrid grid. The graph neural network is used to extract the environmental features, and the state space, action space and reward function of reinforcement learning are designed for the emergency control task. Combined with the transient stability analysis, the emergency control strategy framework is constructed, and the effective emergency control decision after failure is realized. The effectiveness of the method in the topology change scenario is verified.},
  keywords = {0 - DRL,0 - RL,0 - Unmentioned,0 - Untagged,A - Emergency Control,A - Smart Grid,A - Transient Stability Assessment,DRL - DQL,DRL - DQN,GNN - GraphSAGE},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2022 - Lin et al - Emergency Control of Power Grid under Topology Changes Based on Graph.pdf;../../../documents/school/dissertation/zotero/storage/HUEEM3UE/10073160.html}
}

@inproceedings{liNovelGraphReinforcement2022,
  title = {A {{Novel Graph Reinforcement Learning Approach}} for {{Stochastic Dynamic Economic Dispatch}} under {{High Penetration}} of {{Renewable Energy}}},
  booktitle = {2022 4th {{Asia Energy}} and {{Electrical Engineering Symposium}} ({{AEEES}})},
  author = {Li, Peng and Huang, Wenqi and Dai, Zhen and Hou, Jiaxuan and Cao, Shang and Zhang, Jiayu and Chen, Junbin},
  year = {2022},
  month = mar,
  pages = {498--503},
  doi = {10.1109/AEEES54426.2022.9759565},
  urldate = {2023-10-29},
  abstract = {Due to the fast development of new-type power systems, the power grid is facing increasing uncertainties brought by high penetration of distributed generations. How to improve the decision quality of economic dispatch under such conditions becomes a very crucial task. Therefore, a novel graph reinforcement learning (GRL) approach for dynamic economic dispatch under high penetration of renewable energy is proposed. Compared with other reinforcement learning method, a novel graph-based representation of system state is adopted. Thus, the implicit correlations of uncertainties considering system topology can be more effectively captured. As a fully model-free method, the proposed methodology does not rely on the explicit models of physical system and uncertainty distributions. The asymptotic optimal policy can be obtained by continuous interaction with the environment. Case simulations illustrate that the proposed method achieves a better performance in terms of optimality and efficiency compared with existing learning methods.},
  keywords = {0 - Cited,0 - DRL,0 - GRL,0 - RL,A - Dynamic Economic Dispatch,A - High Penetration of Distributed Generation,A - Smart Grid,D - IEEE39,DRL - Actor-Critic,DRL - SAC,GNN - GCN,RL - Single Agent,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2022 - Li et al - A Novel Graph Reinforcement Learning Approach for Stochastic Dynamic Economic.pdf;../../../documents/school/dissertation/zotero/storage/A66PUPRC/9759565.html}
}

@article{liuDecomposingSharedNetworks2023,
  title = {Decomposing Shared Networks for Separate Cooperation with Multi-Agent Reinforcement Learning},
  author = {Liu, Weiwei and Peng, Linpeng and Wen, Licheng and Yang, Jian and Liu, Yong},
  year = {2023},
  month = sep,
  journal = {Information Sciences},
  volume = {641},
  pages = {119085},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2023.119085},
  urldate = {2023-11-07},
  abstract = {Sharing network parameters between agents is an essential and typical operation to improve the scalability of multi-agent reinforcement learning algorithms. However, agents with different tasks sharing the same network parameters are not conducive to distinguishing the agents' skills. In addition, the importance of communication between agents undertaking the same task is much higher than that with external agents. Therefore, we propose Dual Cooperation Networks (DCN). In order to distinguish whether agents undertake the same task, all agents are grouped according to their status through the graph neural network instead of the traditional proximity. The agent communicates within the group to achieve strong cooperation. After that, the global value function is decomposed by groups to facilitate cooperation between groups. Finally, we have verified it in simulation and physical hardware, and the algorithm has achieved excellent performance.},
  keywords = {0 - GNN,0 - Unmentioned,A - Decomposing Shared Networks,GNN - Graph Autoencoder,T - Group Cooperation Network},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Liu et al - Decomposing shared networks for separate cooperation with multi-agent.pdf;../../../documents/school/dissertation/zotero/storage/QBJ6FDMB/S0020025523006709.html}
}

@article{liuDistributedEconomicDispatch2018b,
  title = {Distributed {{Economic Dispatch}} in {{Microgrids Based}} on {{Cooperative Reinforcement Learning}}},
  author = {Liu, Weirong and Zhuang, Peng and Liang, Hao and Peng, Jun and Huang, Zhiwu},
  year = {2018},
  month = jun,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {29},
  number = {6},
  pages = {2192--2203},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2018.2801880},
  urldate = {2024-01-30},
  abstract = {Microgrids incorporated with distributed generation (DG) units and energy storage (ES) devices are expected to play more and more important roles in the future power systems. Yet, achieving efficient distributed economic dispatch in microgrids is a challenging issue due to the randomness and nonlinear characteristics of DG units and loads. This paper proposes a cooperative reinforcement learning algorithm for distributed economic dispatch in microgrids. Utilizing the learning algorithm can avoid the difficulty of stochastic modeling and high computational complexity. In the cooperative reinforcement learning algorithm, the function approximation is leveraged to deal with the large and continuous state spaces. And a diffusion strategy is incorporated to coordinate the actions of DG units and ES devices. Based on the proposed algorithm, each node in microgrids only needs to communicate with its local neighbors, without relying on any centralized controllers. Algorithm convergence is analyzed, and simulations based on real-world meteorological and load data are conducted to validate the performance of the proposed algorithm.},
  keywords = {0 - RL,A - Dynamic Economic Dispatch,RL - Multi-Agent},
  annotation = {155 citations (Crossref) [2024-01-30]},
  file = {../../../documents/dissertation/bibliography/Journal Article/2018 - Liu et al - Distributed Economic Dispatch in Microgrids Based on Cooperative Reinforcement.pdf;../../../documents/school/dissertation/zotero/storage/UHNWJEXL/8306311.html}
}

@inproceedings{liuGraphConvolutionBasedDeep2022,
  title = {Graph {{Convolution-Based Deep Reinforcement Learning}} for {{Multi-Agent Decision-Making}} in {{Interactive Traffic Scenarios}}},
  booktitle = {2022 {{IEEE}} 25th {{International Conference}} on {{Intelligent Transportation Systems}} ({{ITSC}})},
  author = {Liu, Qi and Li, Zirui and Li, Xueyuan and Wu, Jingda and Yuan, Shihua},
  year = {2022},
  month = oct,
  pages = {4074--4081},
  doi = {10.1109/ITSC55140.2022.9922001},
  urldate = {2023-10-29},
  abstract = {A reliable multi-agent decision-making system is highly demanded for safe and efficient operations of connected and autonomous vehicles (CAVs). In order to represent the mutual effects between vehicles and model the dynamic traffic environments, this research proposes an integrated and open-source framework to realize different Graph Reinforcement Learning (GRL) methods for better decision-making in interactive driving scenarios. Firstly, an interactive driving scenario on the highway with two ramps is constructed. The vehicles in this scenario are modeled by graph representation, and features are extracted via Graph Neural Network (GNN). Secondly, several GRL approaches are implemented and compared in detail. Finally, The simulation in the SUMO platform is carried out to evaluate the performance of different G RL approaches. Results are analyzed from multiple perspectives to compare the performance of different G RL methods in intelligent transportation scenarios. Experiments show that the implementation of GNN can well model the interactions between vehicles, and the proposed framework can improve the overall performance of multi-agent decision-making. The source code of our work can be found at https://github.com/Jacklinkk/TorchGRL.},
  keywords = {0 - Unmentioned,0 - Untagged},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2022 - Liu et al - Graph Convolution-Based Deep Reinforcement Learning for Multi-Agent.pdf;../../../documents/school/dissertation/zotero/storage/TYDETASY/9922001.html}
}

@misc{liuGraphReinforcementLearning2022,
  title = {Graph {{Reinforcement Learning Application}} to {{Co-operative Decision-Making}} in {{Mixed Autonomy Traffic}}: {{Framework}}, {{Survey}}, and {{Challenges}}},
  shorttitle = {Graph {{Reinforcement Learning Application}} to {{Co-operative Decision-Making}} in {{Mixed Autonomy Traffic}}},
  author = {Liu, Qi and Li, Xueyuan and Li, Zirui and Wu, Jingda and Du, Guodong and Gao, Xin and Yang, Fan and Yuan, Shihua},
  year = {2022},
  month = nov,
  number = {arXiv:2211.03005},
  eprint = {2211.03005},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.03005},
  urldate = {2023-11-20},
  abstract = {Proper functioning of connected and automated vehicles (CAVs) is crucial for the safety and efficiency of future intelligent transport systems. Meanwhile, transitioning to fully autonomous driving requires a long period of mixed autonomy traffic, including both CAVs and human-driven vehicles. Thus, collaboration decision-making for CAVs is essential to generate appropriate driving behaviors to enhance the safety and efficiency of mixed autonomy traffic. In recent years, deep reinforcement learning (DRL) has been widely used in solving decision-making problems. However, the existing DRL-based methods have been mainly focused on solving the decision-making of a single CAV. Using the existing DRL-based methods in mixed autonomy traffic cannot accurately represent the mutual effects of vehicles and model dynamic traffic environments. To address these shortcomings, this article proposes a graph reinforcement learning (GRL) approach for multi-agent decision-making of CAVs in mixed autonomy traffic. First, a generic and modular GRL framework is designed. Then, a systematic review of DRL and GRL methods is presented, focusing on the problems addressed in recent research. Moreover, a comparative study on different GRL methods is further proposed based on the designed framework to verify the effectiveness of GRL methods. Results show that the GRL methods can well optimize the performance of multi-agent decision-making for CAVs in mixed autonomy traffic compared to the DRL methods. Finally, challenges and future research directions are summarized. This study can provide a valuable research reference for solving the multi-agent decision-making problems of CAVs in mixed autonomy traffic and can promote the implementation of GRL-based methods into intelligent transportation systems. The source code of our work can be found at https://github.com/Jacklinkk/Graph\_CAVs.},
  archiveprefix = {arxiv},
  keywords = {0 - Unmentioned,0 - Untagged},
  file = {../../../documents/school/dissertation/bibliography/Preprint/2022 - Liu et al - Graph Reinforcement Learning Application to Co-operative Decision-Making in.pdf;../../../documents/school/dissertation/zotero/storage/ILHPJFIX/2211.html}
}

@article{liuGraphReinforcementLearningBased2023,
  title = {Graph {{Reinforcement Learning-Based Decision-Making Technology}} for {{Connected}} and {{Autonomous Vehicles}}: {{Framework}}, {{Review}}, and {{Future Trends}}},
  shorttitle = {Graph {{Reinforcement Learning-Based Decision-Making Technology}} for {{Connected}} and {{Autonomous Vehicles}}},
  author = {Liu, Qi and Li, Xueyuan and Tang, Yujie and Gao, Xin and Yang, Fan and Li, Zirui},
  year = {2023},
  month = jan,
  journal = {Sensors},
  volume = {23},
  number = {19},
  pages = {8229},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1424-8220},
  doi = {10.3390/s23198229},
  urldate = {2023-10-29},
  abstract = {The proper functioning of connected and autonomous vehicles (CAVs) is crucial for the safety and efficiency of future intelligent transport systems. Meanwhile, transitioning to fully autonomous driving requires a long period of mixed autonomy traffic, including both CAVs and human-driven vehicles. Thus, collaborative decision-making technology for CAVs is essential to generate appropriate driving behaviors to enhance the safety and efficiency of mixed autonomy traffic. In recent years, deep reinforcement learning (DRL) methods have become an efficient way in solving decision-making problems. However, with the development of computing technology, graph reinforcement learning (GRL) methods have gradually demonstrated the large potential to further improve the decision-making performance of CAVs, especially in the area of accurately representing the mutual effects of vehicles and modeling dynamic traffic environments. To facilitate the development of GRL-based methods for autonomous driving, this paper proposes a review of GRL-based methods for the decision-making technologies of CAVs. Firstly, a generic GRL framework is proposed in the beginning to gain an overall understanding of the decision-making technology. Then, the GRL-based decision-making technologies are reviewed from the perspective of the construction methods of mixed autonomy traffic, methods for graph representation of the driving environment, and related works about graph neural networks (GNN) and DRL in the field of decision-making for autonomous driving. Moreover, validation methods are summarized to provide an efficient way to verify the performance of decision-making methods. Finally, challenges and future research directions of GRL-based decision-making methods are summarized.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {0 - GRL,0 - Unmentioned,A - Autonomous Vehicles,Type - Survey},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Liu et al - Graph Reinforcement Learning-Based Decision-Making Technology for Connected and.pdf}
}

@book{liuIntroductionGraphNeural2020,
  title = {Introduction to {{Graph Neural Networks}}},
  author = {Liu, Zhiyuan and Zhou, Jie},
  year = {2020},
  series = {Synthesis {{Lectures}} on {{Artificial Intelligence}} and {{Machine Learning}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-01587-8},
  urldate = {2023-12-10},
  isbn = {978-3-031-00459-9 978-3-031-01587-8},
  langid = {english},
  keywords = {0 - GNN,Type - Book},
  file = {../../../documents/school/dissertation/bibliography/Book/2020 - Liu-Zhou - Introduction to Graph Neural Networks.pdf}
}

@inproceedings{liuMultiAgentsInteractionApproach2022,
  title = {Multi-{{Agents Interaction Approach}} Based on {{Graph Network}} and {{Reinforcement Learning}}},
  booktitle = {2022 9th {{International Conference}} on {{Dependable Systems}} and {{Their Applications}} ({{DSA}})},
  author = {Liu, Lian and Wang, Zimeng},
  year = {2022},
  month = aug,
  pages = {710--715},
  issn = {2767-6684},
  doi = {10.1109/DSA56465.2022.00101},
  urldate = {2023-10-29},
  abstract = {Due to the development of artificial intelligence technology, the research of unmanned equipment has gained great progress. However, how to make multi-agents in unmanned clusters to be reasonable for interaction is the key problem to be solved at present. And the characteristics of unmanned clusters can be derived from a variety of work modes. At the same time these mode are inseparable from the interaction and collaboration mechanism. Graph reinforcement learning is a new algorithm that combines graph neural network and reinforcement learning. This paper is based on graph reinforcement learning to solve the problem of interaction between various nodes in unmanned clusters. And the approach provides the technical guidance for the future development of unmanned cluster technology.},
  keywords = {0 - Unmentioned,0 - Untagged},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2022 - Liu-Wang - Multi-Agents Interaction Approach based on Graph Network and Reinforcement.pdf;../../../documents/school/dissertation/zotero/storage/B2JVMVNG/9914496.html}
}

@article{liuNewMultidatadrivenSpatiotemporal2021,
  title = {A New Multi-Data-Driven Spatiotemporal {{PM2}}.5 Forecasting Model Based on an Ensemble Graph Reinforcement Learning Convolutional Network},
  author = {Liu, Xinwei and Qin, Muchuan and He, Yue and Mi, Xiwei and Yu, Chengqing},
  year = {2021},
  month = oct,
  journal = {Atmospheric Pollution Research},
  volume = {12},
  number = {10},
  pages = {101197},
  issn = {1309-1042},
  doi = {10.1016/j.apr.2021.101197},
  urldate = {2023-10-29},
  abstract = {Spatiotemporal PM2.5 forecasting technology plays an important role in urban traffic environment management and planning. In order to establish a satisfactory high-precision PM2.5 prediction model, a new multidata-driven spatiotemporal PM2.5 forecasting model is proposed in this paper. The overall modelling framework consists of three main parts. In part I, the graph convolutional network uses an adjacency matrix to effectively aggregate spatiotemporal pollutant data from different nodes and extract the most valuable feature information for target point modeling from the original data. In part II, the extracted feature information is used as the input of the gated recursive unit and the long short-term memory network to construct the prediction model. In part III, the Q-learning algorithm builds the best ensemble PM2.5 forecasting model by analyzing the processing ability and analysis ability of different predictors. Based on the analysis of multiple cases, the following conclusions can be drawn: (1) Graphic convolutional networks can effectively analyze the spatiotemporal correlation of PM2.5 data and achieve better performance than traditional convolutional neural networks. (2) Q-learning can adaptively optimize the ensemble weight coefficient and achieve better results than the traditional optimization algorithm. (3) The proposed GCN-LSTM-GRU-Q model can achieve better results than the 24 benchmark models.},
  keywords = {0 - Unmentioned,A - PM2.5 Forecasting,A - Smart Mobility,A - Spatio-Temporal Forecasting,GNN - GCN,RL - Q-Learning,T - GRU,T - Long Short-Term Memory,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2021 - Liu et al - A new multi-data-driven spatiotemporal PM2.pdf;../../../documents/school/dissertation/zotero/storage/TP3WB3VH/S1309104221002622.html}
}

@article{liuOnlineMultiAgentReinforcement2021,
  title = {Online {{Multi-Agent Reinforcement Learning}} for {{Decentralized Inverter-Based Volt-VAR Control}}},
  author = {Liu, Haotian and Wu, Wenchuan},
  year = {2021},
  month = jul,
  journal = {IEEE Transactions on Smart Grid},
  volume = {12},
  number = {4},
  pages = {2980--2990},
  issn = {1949-3061},
  doi = {10.1109/TSG.2021.3060027},
  urldate = {2023-11-17},
  abstract = {The distributed Volt/Var control (VVC) methods have been widely studied for active distribution networks(ADNs), which is based on perfect model and real-time P2P communication. However, the model is always incomplete with significant parameter errors and such P2P communication system is hard to maintain. In this article, we propose an online multi-agent reinforcement learning and decentralized control framework (OLDC) for VVC. In this framework, the VVC problem is formulated as a constrained Markov game and we propose a novel multi-agent constrained soft actor-critic (MACSAC) reinforcement learning algorithm. MACSAC is used to train the control agents online, so the accurate ADN model is no longer needed. Then, the trained agents can realize decentralized optimization using local measurements without real-time P2P communication. The OLDC with MACSAC has shown extraordinary flexibility, efficiency and robustness to various computing and communication conditions. Numerical simulations on IEEE test cases not only demonstrate that the proposed MACSAC outperforms the state-of-art learning algorithms, but also support the superiority of our OLDC framework in the online application.},
  keywords = {0 - Unmentioned,0 - Untagged},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2021 - Liu-Wu - Online Multi-Agent Reinforcement Learning for Decentralized Inverter-Based.pdf;../../../documents/school/dissertation/zotero/storage/T7ABFN57/9356806.html}
}

@inproceedings{luoMultiAgentCollaborativeExploration2019,
  title = {Multi-{{Agent Collaborative Exploration}} through {{Graph-based Deep Reinforcement Learning}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Agents}} ({{ICA}})},
  author = {Luo, Tianze and Subagdja, Budhitama and Wang, Di and Tan, Ah-Hwee},
  year = {2019},
  month = oct,
  pages = {2--7},
  doi = {10.1109/AGENTS.2019.8929168},
  urldate = {2023-11-13},
  abstract = {Autonomous exploration by a single or multiple agents in an unknown environment leads to various applications in automation, such as cleaning, search and rescue, etc. Traditional methods normally take frontier locations and segmented regions of the environment into account to efficiently allocate target locations to different agents to visit. They may employ ad hoc solutions to allocate the task to the agents, but the allocation may not be efficient. In the literature, few studies focused on enhancing the traditional methods by applying machine learning models for agent performance improvement. In this paper, we propose a graph-based deep reinforcement learning approach to effectively perform multi-agent exploration. Specifically, we first design a hierarchical map segmentation method to transform the environment exploration problem to the graph domain, wherein each node of the graph corresponds to a segmented region in the environment and each edge indicates the distance between two nodes. Subsequently, based on the graph structure, we apply a Graph Convolutional Network (GCN) to allocate the exploration target to each agent. Our experiments show that our proposed model significantly improves the efficiency of map explorations across varying sizes of collaborative agents over the traditional methods.},
  keywords = {0 - DRL,0 - RL,0 - Unmentioned,A - Autonomous Exploration,D - ROS Room Dataset,DRL - DQL,DRL - DQN,GNN - GCN,RL - Multi-Agent,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2019 - Luo et al - Multi-Agent Collaborative Exploration through Graph-based Deep Reinforcement.pdf;../../../documents/school/dissertation/zotero/storage/LFUG6DE7/8929168.html}
}

@phdthesis{luzMultiAgentDeepReinforcement2022,
  title = {Multi-{{Agent Deep Reinforcement Learning}} for {{Resource Management}} in {{Earth Observation Satellites Constellations}}},
  author = {da Luz, Jo{\~a}o Henrique Afonso Marques Reguengo},
  year = {2022},
  month = jul,
  urldate = {2023-12-28},
  copyright = {openAccess},
  langid = {english},
  school = {FEUP},
  keywords = {0 - DRL,0 - RL,RL - Multi-Agent,Type - Thesis},
  annotation = {Accepted: 2023-01-23T03:35:39Z},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2022 - Luz - Multi-Agent Deep Reinforcement Learning for Resource Management in Earth.pdf}
}

@article{lvReinforcementLearningList2023,
  title = {A {{Reinforcement Learning List Recommendation Model Fused}} with {{Graph Neural Networks}}},
  author = {Lv, Zhongming and Tong, Xiangrong},
  year = {2023},
  month = jan,
  journal = {Electronics},
  volume = {12},
  number = {18},
  pages = {3748},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2079-9292},
  doi = {10.3390/electronics12183748},
  urldate = {2023-11-14},
  abstract = {Existing list recommendation methods present a list consisting of multiple items for feedback recommendation to user requests, which has the advantages of high flexibility and direct user feedback. However, the structured representation of state data limits the embedding of users and items, making them isolated from each other, missing some useful infomation for recommendation. In addition, the traditional non-end-to-end learning series takes a long time and accumulates errors. During the model training process, the results of each task can easily affect the next calculation, thus affecting the entire training effect. Aiming at the above problems, this paper proposes a Reinforcement Learning List Recommendation Model Fused with a Graph Neural Network, GNLR. The goal of this model is to maximize the recommendation effect while ensuring that the list recommendation system accurately analyzes user preferences to improve user experience. To this end, firstly, we use an user{\textendash}item bipartite graph and Graph Neural Network to aggregate neighborhood information for users and items to generate graph structured representation; secondly, we adopt an attention mechanism to assign corresponding weights to neighborhood information to reduce the influence of noise nodes in heterogeneous information networks; finally, we alleviate the problems of traditional non-end-to-end methods through end-to-end training methods. The experimental results show that the method proposed in this paper can alleviate the above problems, and the recommendation hit rate and accuracy rate increase by about 10\%.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {0 - DRL,0 - GNN,0 - GRL,0 - RL,0 - Unmentioned,A - Recommendation Systems,DRL - Actor-Critic,GNN - GAT,Type - Article,Type - Survey},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Lv-Tong - A Reinforcement Learning List Recommendation Model Fused with Graph Neural.pdf}
}

@phdthesis{martinsLastMileDeliveryProblem2023,
  title = {Last-{{Mile Delivery Problem Representation}} in {{Reinforcement Learning}}},
  author = {Martins, Clara Alves},
  year = {2023},
  month = jul,
  urldate = {2023-12-28},
  copyright = {openAccess},
  langid = {english},
  school = {FEUP},
  keywords = {0 - DRL,0 - GNN,0 - GRL,0 - RL,RL - Single Agent,Type - Thesis},
  annotation = {Accepted: 2023-12-15T00:09:42Z},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Martins - Last-Mile Delivery Problem Representation in Reinforcement Learning.pdf}
}

@article{mcfarlaneGiantBatteriesDrain2023,
  title = {Giant Batteries Drain Economics of Gas Power Plants},
  author = {Mcfarlane, Sarah and Twidale, Susanna},
  year = {2023},
  month = nov,
  journal = {Reuters},
  urldate = {2024-01-07},
  abstract = {Giant batteries that ensure stable power supply by offsetting intermittent renewable supplies are becoming cheap enough to make developers abandon scores of projects for gas-fired generation world-wide.},
  chapter = {Energy},
  langid = {english},
  file = {../../../documents/school/dissertation/zotero/storage/4HY6HLER/giant-batteries-drain-economics-gas-power-plants-2023-11-21.html}
}

@article{mckennaHighresolutionStochasticIntegrated2016,
  title = {High-Resolution Stochastic Integrated Thermal{\textendash}Electrical Domestic Demand Model},
  author = {McKenna, Eoghan and Thomson, Murray},
  year = {2016},
  month = mar,
  journal = {Applied Energy},
  volume = {165},
  pages = {445--461},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2015.12.089},
  urldate = {2024-01-12},
  abstract = {This paper describes the extension of CREST's existing electrical domestic demand model into an integrated thermal{\textendash}electrical demand model. The principle novelty of the model is its integrated structure such that the timing of thermal and electrical output variables are appropriately correlated. The model has been developed primarily for low-voltage network analysis and the model's ability to account for demand diversity is of critical importance for this application. The model, however, can also serve as a basis for modelling domestic energy demands within the broader field of urban energy systems analysis. The new model includes the previously published components associated with electrical demand and generation (appliances, lighting, and photovoltaics) and integrates these with an updated occupancy model, a solar thermal collector model, and new thermal models including a low-order building thermal model, domestic hot water consumption, thermostat and timer controls and gas boilers. The paper reviews the state-of-the-art in high-resolution domestic demand modelling, describes the model, and compares its output with three independent validation datasets. The integrated model remains an open-source development in Excel VBA and is freely available to download for users to configure and extend, or to incorporate into other models.},
  keywords = {0 - Data,0 - Smart Grid,D - CREST},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2016 - McKenna-Thomson - High-resolution stochastic integrated thermal–electrical domestic demand model.pdf;../../../documents/school/dissertation/zotero/storage/4DHTXMYA/S0306261915016621.html}
}

@inproceedings{mnihAsynchronousMethodsDeep2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  year = {2016},
  month = jun,
  pages = {1928--1937},
  publisher = {{PMLR}},
  issn = {1938-7228},
  urldate = {2024-01-05},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  langid = {english},
  keywords = {0 - DRL,0 - RL,DRL - A2C,DRL - A3C,DRL - Actor-Critic,RL - Single Agent,Type - Seminal},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2016 - Mnih et al - Asynchronous Methods for Deep Reinforcement Learning.pdf}
}

@article{mnihHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  urldate = {2024-01-05},
  abstract = {An artificial agent is developed that learns to play~a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a~performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  copyright = {2015 Springer Nature Limited},
  langid = {english},
  keywords = {0 - DRL,0 - RL,DRL - DQL,DRL - DQN,RL - Single Agent,Type - Seminal},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2015 - Mnih et al - Human-level control through deep reinforcement learning.pdf}
}

@inproceedings{mondenChargingDischargingControl2021,
  title = {Charging and Discharging Control of a Hybrid Battery Energy Storage System Using Different Battery Types in Order to Avoid Degradation},
  booktitle = {2021 {{IEEE International Future Energy Electronics Conference}} ({{IFEEC}})},
  author = {Monden, Yukitaka and Mizutani, Mami and Yamazaki, Shuji and Kobayashi, Takenori},
  year = {2021},
  month = nov,
  pages = {1--6},
  doi = {10.1109/IFEEC53238.2021.9662026},
  urldate = {2024-02-04},
  abstract = {Recently, there has been a rapid increase of renewable energy resources connected to power grids, so that power quality such as frequency variation has become a growing concern. Therefore, battery energy storage systems (BESSs) have been put into practical use to balance demand and supply power and to regulate the grid frequency. On the other hand, a service life of a batteries becomes shorter due to degradation as the number of charging and discharging cycles increases. This paper presents a hybrid battery energy storage system (HESS), where large energy batteries are used together with high power batteries. The system configuration and the control scheme of the HESS are then proposed for frequency regulation applications. Simulation results show that the proposed control scheme effectively prevent degradation of the HESS compared to a BESS using large energy batteries only.},
  keywords = {Degradation,high power battery system,hybrid battery energy storage system,hybrid control,Hybrid power systems,large energy battery system,Power grids,Power quality,Renewable energy sources,Resistance,Simulation},
  annotation = {2 citations (Crossref) [2024-02-04]},
  file = {../../../documents/dissertation/bibliography/Conference Paper/2021 - Monden et al - Charging and discharging control of a hybrid battery energy storage system.pdf;../../../documents/school/dissertation/zotero/storage/IMPKG474/9662026.html}
}

@book{moralesGrokkingDeepReinforcement2020,
  title = {Grokking {{Deep Reinforcement Learning}}},
  author = {Morales, Miguel},
  year = {2020},
  publisher = {{Manning}},
  address = {{Shelter Island [New York]}},
  abstract = {Grokking Deep Reinforcement Learning uses engaging exercises to teach you how to build deep learning systems. This book combines annotated Python code with intuitive explanations to explore DRL techniques. You'll see how algorithms function and learn to develop your own DRL agents using evaluative feedback},
  isbn = {978-1-61729-545-4},
  langid = {english},
  keywords = {0 - DRL,0 - RL,RL - Multi-Agent,RL - Single Agent,Type - Book},
  annotation = {OCLC: 1228577398},
  file = {../../../documents/school/dissertation/bibliography/Book/2020 - Morales - Grokking Deep Reinforcement Learning.pdf}
}

@article{morenoMultiAgentDeepReinforcement2020,
  title = {Multi-{{Agent Deep Reinforcement Learning}} for Autonomous Driving},
  author = {Moreno, Gon{\c c}alo Vasconcelos Cunha Miranda},
  year = {2020},
  month = oct,
  urldate = {2023-11-20},
  copyright = {openAccess},
  langid = {english},
  keywords = {0 - Unmentioned,0 - Untagged},
  annotation = {Accepted: 2022-10-14T23:08:05Z},
  file = {../../../documents/school/biblio/undefined/2020/2020 - Moreno - Multi-Agent Deep Reinforcement Learning for autonomous driving.pdf}
}

@incollection{murrayDataManagementPlatform2015,
  title = {A Data Management Platform for Personalised Real-Time Energy Feedback},
  booktitle = {8th {{International Conference}} on {{Energy Efficiency}} in {{Domestic Appliances}} and {{Lighting}}},
  author = {Murray, David and Liao, Jing and Stankovic, Lina and Stankovic, Vladimir and {Hauxwell-Baldwin}, Richard and Wilson, Charlie and Coleman, Michael and Kane, Tom and Firth, Steven},
  year = {2015},
  month = aug,
  publisher = {{IET}},
  address = {{GBR}},
  urldate = {2024-01-12},
  abstract = {This paper presents a data collection and energy fe edback platform for smart homes to enhance the value of information given by smart energy meter da ta by providing user-tailored real-time energy consumption feedback and advice that can be easily accessed and acted upon by the household. Our data management platform consists of an SQL server back-end which collects data, namely, aggregate power consumption as well as consumption of major appliances, temperature, humidity, light, and motion data. These data streams allow us to infer information about the household's appliance usage and domestic activities, which in t urn enables meaningful and useful energy feedback. The platform developed has been rolled ou t in 20 UK households over a period of just over 21 months. As well as the data streams mentioned, q ualitative data such as appliance survey, tariff, house construction type and occupancy information a re also included. The paper presents a review of publically available smart home datasets and a desc ription of our own smart home set up and monitoring platform. We then provide examples of th e types of feedback that can be generated, looking at the suitability of electricity tariffs a nd appliance specific feedback.},
  langid = {english},
  keywords = {0 - Data,0 - Smart Grid,D - REFIT},
  file = {../../../documents/school/dissertation/bibliography/Book Section/2015 - Murray et al - A data management platform for personalised real-time energy feedback.pdf;../../../documents/school/dissertation/zotero/storage/PXF9NWVP/54966.html}
}

@article{nguyenStochasticOptimizationRenewableBased2016a,
  title = {Stochastic {{Optimization}} of {{Renewable-Based Microgrid Operation Incorporating Battery Operating Cost}}},
  author = {Nguyen, Tu A. and Crow, M. L.},
  year = {2016},
  month = may,
  journal = {IEEE Transactions on Power Systems},
  volume = {31},
  number = {3},
  pages = {2289--2296},
  issn = {1558-0679},
  doi = {10.1109/TPWRS.2015.2455491},
  urldate = {2024-01-30},
  abstract = {Integration of renewable energy resources in microgrids has been increasing in recent decades. Due to the randomness in renewable resources such as solar and wind, the power generated can deviate from forecasted values. This variation may cause increased operating costs for committing costly reserve units or penalty costs for shedding load. In addition, it is often desired to charge/discharge and coordinate the energy storage units in an efficient and economical way. To address these problems, a novel battery operation cost model is proposed which considers a battery as an equivalent fuel-run generator to enable it to be incorporated into a unit commitment problem. A probabilistic constrained approach is used to incorporate the uncertainties of the renewable sources and load demands into the unit commitment (UC) and economic dispatch problems.},
  keywords = {0 - Smart Grid,A - Dynamic Economic Dispatch},
  annotation = {162 citations (Crossref) [2024-01-30]},
  file = {../../../documents/dissertation/bibliography/Journal Article/2016 - Nguyen-Crow - Stochastic Optimization of Renewable-Based Microgrid Operation Incorporating.pdf;../../../documents/school/dissertation/zotero/storage/CMRQ3UNP/7172561.html}
}

@book{nicaBriefIntroductionSpectral2018,
  title = {A {{Brief Introduction}} to {{Spectral Graph Theory}}},
  author = {Nica, Bogdan},
  year = {2018},
  month = may,
  edition = {1},
  publisher = {{EMS Press}},
  doi = {10.4171/188},
  urldate = {2023-12-24},
  isbn = {978-3-03719-188-0 978-3-03719-688-5},
  langid = {english},
  keywords = {0 - Graph Theory,Type - Book},
  file = {../../../documents/school/dissertation/bibliography/Book/2018 - Nica - A Brief Introduction to Spectral Graph Theory.pdf}
}

@article{nieReinforcementLearningGraphs2023,
  title = {Reinforcement {{Learning}} on {{Graphs}}: {{A Survey}}},
  shorttitle = {Reinforcement {{Learning}} on {{Graphs}}},
  author = {Nie, Mingshuo and Chen, Dongming and Wang, Dongqi},
  year = {2023},
  month = aug,
  journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume = {7},
  number = {4},
  pages = {1065--1082},
  issn = {2471-285X},
  doi = {10.1109/TETCI.2022.3222545},
  urldate = {2023-11-27},
  abstract = {Graph mining tasks arise from many different application domains, including social networks, biological networks, transportation, and E-commerce, which have been receiving great attention from the theoretical and algorithmic design communities in recent years, and there has been some pioneering work employing the research-rich Reinforcement Learning (RL) techniques to address graph mining tasks. However, these fusion works are dispersed in different research domains, which makes them difficult to compare. In this survey, we provide a comprehensive overview of these fusion works and generalize these works to Graph Reinforcement Learning (GRL) as a unified formulation. We further discuss the applications of GRL methods across various domains, and simultaneously propose the key challenges and advantages of integrating graph mining and RL methods. Furthermore, we propose important directions and challenges to be solved in the future. To our knowledge, this is the latest work on a comprehensive survey of GRL, this work provides a global view and a learning resource for scholars. Based on our review, we create a collection of papers for both interested scholars who want to enter this rapidly developing domain and experts who would like to compare GRL methods.},
  keywords = {0 - DRL,0 - GNN,0 - GRL,0 - RL,0 - Unmentioned,GNN - GCN,Type - Survey},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Nie et al - Reinforcement Learning on Graphs.pdf;../../../documents/school/dissertation/zotero/storage/9JQMLZEK/10121200.html}
}

@misc{openaiSpinningDocumentation,
  title = {Spinning {{Up Documentation}}},
  author = {OpenAI},
  urldate = {2023-12-26},
  howpublished = {https://spinningup.openai.com/en/latest/index.html},
  keywords = {0 - Cited,0 - DRL,0 - RL,DRL - Actor-Critic,DRL - DDPG,DRL - PPO,DRL - SAC,Type - Documentation},
  file = {../../../documents/school/dissertation/zotero/storage/FJG6IDG4/index.html}
}

@inproceedings{paulEfficientPlanningMultiRobot2023,
  title = {Efficient {{Planning}} of {{Multi-Robot Collective Transport}} Using {{Graph Reinforcement Learning}} with {{Higher Order Topological Abstraction}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Paul, Steve and Li, Wenyuan and Smyth, Brian and Chen, Yuzhou and Gel, Yulia and Chowdhury, Souma},
  year = {2023},
  month = may,
  pages = {5779--5785},
  doi = {10.1109/ICRA48891.2023.10161517},
  urldate = {2023-10-29},
  abstract = {Efficient multi-robot task allocation (MRTA) is fundamental to various time-sensitive applications such as disaster response, warehouse operations, and construction. This paper tackles a particular class of these problems that we call MRTA-collective transport or MRTA-CT - here tasks present varying workloads and deadlines, and robots are subject to flight range, communication range, and payload constraints. For large instances of these problems involving 100s-1000's of tasks and 10s-100s of robots, traditional non-learning solvers are often time-inefficient, and emerging learning-based policies do not scale well to larger-sized problems without costly retraining. To address this gap, we use a recently proposed encoder-decoder graph neural network involving Capsule networks and multi-head attention mechanism, and innovatively add topological descriptors (TD) as new features to improve transferability to unseen problems of similar and larger size. Persistent homology is used to derive the TD, and proximal policy optimization is used to train our TD-augmented graph neural network. The resulting policy model compares favorably to state-of-the-art non-learning baselines while being much faster. The benefit of using TD is readily evident when scaling to test problems of size larger than those used in training.},
  keywords = {0 - Unmentioned,0 - Untagged,A - Smart Mobility},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2023 - Paul et al - Efficient Planning of Multi-Robot Collective Transport using Graph.pdf;../../../documents/school/dissertation/zotero/storage/7IQCEW2X/10161517.html}
}

@article{peiEmergencyControlStrategy2023,
  title = {An Emergency Control Strategy for Undervoltage Load Shedding of Power System: {{A}} Graph Deep Reinforcement Learning Method},
  shorttitle = {An Emergency Control Strategy for Undervoltage Load Shedding of Power System},
  author = {Pei, Yangzhou and Yang, Jun and Wang, Jundong and Xu, Peidong and Zhou, Ting and Wu, Fuzhang},
  year = {2023},
  month = may,
  journal = {IET Generation, Transmission \& Distribution},
  volume = {17},
  number = {9},
  pages = {2130--2141},
  issn = {1751-8687, 1751-8695},
  doi = {10.1049/gtd2.12795},
  urldate = {2023-11-13},
  abstract = {Undervoltage load shedding (UVLS) is the last line of defense to ensure the safe and stable operation of the power system. The existing UVLS technique has difficulty adapting and generalizing to new topology variation scenarios of the power network, which greatly affects the reliability of the control strategy. This paper proposes a UVLS emergency control scheme based on a graph deep reinforcement learning method named GraphSAGE-D3QN (graph sample and aggregate-double dueling deep q network). During offline training, a GraphSAGE-based feature extraction mechanism of the power grid with topology variation is designed that can better capture the changes in system state characteristics. A novel reinforcement learning framework based on D3QN is developed for UVLS modeling, which reduces overestimations of control action and leads to a better control effect. Then, online emergency decision-making is achieved. The simulation results on the modified IEEE 39-bus system and IEEE 300-bus power system show that the proposed UVLS scheme can always provide more economical and reliable control strategies for power networks with topology variations and achieves better benefits in both adaptability and generalization performances for previously unseen topology variation scenarios.},
  langid = {english},
  keywords = {0 - Cited,0 - DRL,0 - GRL,0 - RL,A - Smart Grid,A - Undervoltage Load Shedding,D - IEEE300,D - IEEE39,D - RLGC,DRL - D3QN,DRL - DQL,DRL - DQN,GNN - GraphSAGE,RL - Single Agent,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Pei et al - An emergency control strategy for undervoltage load shedding of power system.pdf}
}

@article{penaExtendedIEEE118Bus2018,
  title = {An {{Extended IEEE}} 118-{{Bus Test System With High Renewable Penetration}}},
  author = {Pe{\~n}a, Ivonne and {Martinez-Anido}, Carlo Brancucci and Hodge, Bri-Mathias},
  year = {2018},
  month = jan,
  journal = {IEEE Transactions on Power Systems},
  volume = {33},
  number = {1},
  pages = {281--289},
  issn = {1558-0679},
  doi = {10.1109/TPWRS.2017.2695963},
  urldate = {2024-02-04},
  abstract = {This article describes a new publicly available version of the IEEE 118-bus test system, named NREL-118. The database is based on the transmission representation (buses and lines) of the IEEE 118-bus test system, with a reconfigured generation representation using three regions of the US Western Interconnection from the latest Western Electricity Coordination Council (WECC) 2024 Common Case [Transmission expansion planning home and GridView WECC database]. Time-synchronous hourly load, wind, and solar time series are provided for one year. The public database presented and described in this manuscript will allow researchers to model a test power system using detailed transmission, generation, load, wind, and solar data. This database includes key additional features that add to the current IEEE 118-bus test model, such as the inclusion of ten generation technologies with different heat rate functions, minimum stable levels and ramping rates, GHG emissions rates, regulation and contingency reserves, and hourly time series data for one full year for load, wind, and solar generation.},
  keywords = {Data models,Databases,Economics,Electric grid database,Generators,load forecasts,Load modeling,Power transmission lines,renewable energy data,renewable forecasts,test power system},
  annotation = {139 citations (Crossref) [2024-02-04]},
  file = {../../../documents/dissertation/bibliography/Journal Article/2018 - Peña et al - An Extended IEEE 118-Bus Test System With High Renewable Penetration.pdf;../../../documents/school/dissertation/zotero/storage/LGTDXGHQ/7904729.html}
}

@article{pereraApplicationsReinforcementLearning2021,
  title = {Applications of Reinforcement Learning in Energy Systems},
  author = {Perera, A. T. D. and Kamalaruban, Parameswaran},
  year = {2021},
  month = mar,
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {137},
  pages = {110618},
  issn = {1364-0321},
  doi = {10.1016/j.rser.2020.110618},
  urldate = {2024-01-30},
  abstract = {Energy systems undergo major transitions to facilitate the large-scale penetration of renewable energy technologies and improve efficiencies, leading to the integration of many sectors into the energy system domain. As the complexities in this domain increase, it becomes challenging to control energy flows using existing techniques based on physical models. Moreover, although data-driven models, such as reinforcement learning (RL), have gained considerable attention in many fields, a direct shift into RL is not feasible in the energy domain irrespective of the ongoing complexities. To this end, a top-down approach is used to understand this behavior by reviewing the current state of the art. We classified RL papers in the literature into seven categories based on their area of application. Subsequently, publications under each category were further examined relative to problem diversity, RL technique employed, performance improvement (compared with other white and gray box models), verification, and reproducibility; many of the articles reported a 10{\textendash}20\% performance improvement with the use of RL. In most studies, however, deep learning techniques and state-of-the-art actor-critic methods (e.g., twin delayed deep deterministic policy gradient and soft actor-critic) were not applied. This has remarkably hindered performance improvements and problems related to complex energy flows have not been considered. Approximately half of the publications reported the use of Q-learning. Furthermore, despite the availability of historical data in the energy system domain, batch RL algorithms have not been exploited. Emerging multi-agent RL applications may be considered as a positive development that can enable the management of complex interactions among multiple parties. Most studies lack proper benchmarking compared to model-based approaches or gray-box models, and a majority cover energy dispatch problems and building energy management. Although RL can adequately solve problems that are considerably integrated in several sectors, only a limited number of publications have discussed its broad application. The present study clearly demonstrates that even without the full utilization of RL capacity, this technique has a considerable potential in resolving the continuously increasing complexity within the energy system domain.},
  keywords = {A - Dynamic Economic Dispatch},
  annotation = {138 citations (Crossref) [2024-01-30]},
  file = {../../../documents/dissertation/bibliography/Journal Article/2021 - Perera-Kamalaruban - Applications of reinforcement learning in energy systems.pdf;../../../documents/school/dissertation/zotero/storage/NHBCVUIL/S1364032120309023.html}
}

@misc{pygteamPyGPytorch_geometric,
  title = {{{PyG}} - Pytorch\_geometric},
  author = {PyG Team},
  urldate = {2024-02-02},
  howpublished = {https://pyg.org/},
  file = {../../../documents/school/dissertation/zotero/storage/PYX73CP3/latest.html}
}

@misc{pythonPython2024,
  title = {Python},
  author = {Python},
  year = {2024},
  month = jan,
  journal = {Python.org},
  urldate = {2024-02-02},
  abstract = {The official home of the Python Programming Language},
  howpublished = {https://www.python.org/},
  langid = {english},
  file = {../../../documents/school/dissertation/zotero/storage/XXPRKPWL/www.python.org.html}
}

@misc{pytorchPyTorch,
  title = {{{PyTorch}}},
  author = {PyTorch},
  journal = {PyTorch},
  urldate = {2024-02-02},
  howpublished = {https://pytorch.org/},
  langid = {english},
  file = {../../../documents/school/dissertation/zotero/storage/XC924C6V/pytorch.org.html}
}

@article{qianDeepReinforcementLearning2020,
  title = {Deep {{Reinforcement Learning}} for {{EV Charging Navigation}} by {{Coordinating Smart Grid}} and {{Intelligent Transportation System}}},
  author = {Qian, Tao and Shao, Chengcheng and Wang, Xiuli and Shahidehpour, Mohammad},
  year = {2020},
  month = mar,
  journal = {IEEE Transactions on Smart Grid},
  volume = {11},
  number = {2},
  pages = {1714--1723},
  issn = {1949-3061},
  doi = {10.1109/TSG.2019.2942593},
  urldate = {2023-12-18},
  abstract = {A coordinated operation of smart grid (SG) and intelligent transportation system (ITS) provides electric vehicle (EV) owners with a myriad of power and transportation network data for EV charging navigation. However, the optimal charging navigation would be a challenging task owing to the randomness of traffic conditions, charging prices and waiting time at EV charging station (EVCS). In this paper, we propose a deep reinforcement learning (DRL)-based EV charging navigation, aiming at minimizing the total travel time and the charging cost at EVCS. First, we utilize the deterministic shortest charging route model (DSCRM) to extract feature states out of collected stochastic data and then formulate EV charging navigation as a Markov Decision Process (MDP) with an unknown transition probability. The proposed DRL-based approach will approximate the solution, which can adaptively learn the optimal strategy without any prior knowledge of uncertainties. Case studies are carried out within a practical zone in Xi'an city, China. Numerous experimental results verity the effectiveness of the proposed approach and illustrate its adaptation to EV driver preferences. The coordination effect of SG and ITS on reducing the waiting time and the charging cost in EV charging navigations is also analyzed.},
  keywords = {0 - Unmentioned,0 - Untagged},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2020 - Qian et al - Deep Reinforcement Learning for EV Charging Navigation by Coordinating Smart.pdf;../../../documents/school/dissertation/zotero/storage/5AMVGANU/8845652.html}
}

@article{RenewablesSupplied882023,
  title = {Renewables Supplied 88\% of {{Portugal}}'s Electricity Consumption in {{January}}},
  year = {2023},
  month = feb,
  journal = {Reuters},
  urldate = {2023-11-25},
  abstract = {Renewable utilities supplied 88\% of Portugal's electricity consumption in January, as heavy rains coupled with good wind and solar conditions allowed to sharply reduce the use of gas-fired power plants, grid operator REN said on Wednesday.},
  chapter = {Americas},
  langid = {english},
  keywords = {0 - Cited,Type - News Article},
  file = {../../../documents/school/dissertation/zotero/storage/62E3VMNI/renewables-supplied-88-portugals-electricity-consumption-january-2023-02-01.html}
}

@misc{rtefranceGrid2OpDocumentation,
  title = {{{Grid2Op}}'s {{Documentation}}},
  author = {RTE France},
  urldate = {2024-02-02},
  howpublished = {https://grid2op.readthedocs.io/en/latest/index.html},
  file = {../../../documents/school/dissertation/zotero/storage/ENHUEN7H/index.html}
}

@article{scarselliGraphNeuralNetwork2009,
  title = {The {{Graph Neural Network Model}}},
  author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  year = {2009},
  month = jan,
  journal = {IEEE Transactions on Neural Networks},
  volume = {20},
  number = {1},
  pages = {61--80},
  issn = {1941-0093},
  doi = {10.1109/TNN.2008.2005605},
  urldate = {2023-10-31},
  abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
  keywords = {0 - GNN,Type - Seminal},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2009 - Scarselli et al - The Graph Neural Network Model.pdf;../../../documents/school/dissertation/zotero/storage/I9L4JRJQ/4700287.html}
}

@misc{schulmanProximalPolicyOptimization2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = {2017},
  month = aug,
  number = {arXiv:1707.06347},
  eprint = {1707.06347},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1707.06347},
  urldate = {2023-12-16},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arxiv},
  keywords = {0 - DRL,0 - RL,DRL - PPO,RL - Single Agent,Type - Seminal},
  file = {../../../documents/school/dissertation/bibliography/Preprint/2017 - Schulman et al - Proximal Policy Optimization Algorithms.pdf;../../../documents/school/dissertation/zotero/storage/CNHCBRWE/1707.html}
}

@article{shakyaReinforcementLearningAlgorithms2023,
  title = {Reinforcement Learning Algorithms: {{A}} Brief Survey},
  shorttitle = {Reinforcement Learning Algorithms},
  author = {Shakya, Ashish Kumar and Pillai, Gopinatha and Chakrabarty, Sohom},
  year = {2023},
  month = nov,
  journal = {Expert Systems with Applications},
  volume = {231},
  pages = {120495},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2023.120495},
  urldate = {2023-11-14},
  abstract = {Reinforcement Learning (RL) is a machine learning (ML) technique to learn sequential decision-making in complex problems. RL is inspired by trial-and-error based human/animal learning. It can learn an optimal policy autonomously with knowledge obtained by continuous interaction with a stochastic dynamical environment. Problems considered virtually impossible to solve, such as learning to play video games just from pixel information, are now successfully solved using deep reinforcement learning. Without human intervention, RL agents can surpass human performance in challenging tasks. This review gives a broad overview of RL, covering its fundamental principles, essential methods, and illustrative applications. The authors aim to develop an initial reference point for researchers commencing their research work in RL. In this review, the authors cover some fundamental model-free RL algorithms and pathbreaking function approximation-based deep RL (DRL) algorithms for complex uncertain tasks with continuous action and state spaces, making RL useful in various interdisciplinary fields. This article also provides a brief review of model-based and multi-agent RL approaches. Finally, some promising research directions for RL are briefly presented.},
  keywords = {0 - DRL,0 - RL,0 - Unmentioned,Type - Survey},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Shakya et al - Reinforcement learning algorithms.pdf;../../../documents/school/dissertation/zotero/storage/CEX9P7IW/S0957417423009971.html}
}

@article{shangNewEnsembleDeep2022,
  title = {A New Ensemble Deep Graph Reinforcement Learning Network for Spatio-Temporal Traffic Volume Forecasting in a Freeway Network},
  author = {Shang, Pan and Liu, Xinwei and Yu, Chengqing and Yan, Guangxi and Xiang, Qingqing and Mi, Xiwei},
  year = {2022},
  month = apr,
  journal = {Digital Signal Processing},
  volume = {123},
  pages = {103419},
  issn = {1051-2004},
  doi = {10.1016/j.dsp.2022.103419},
  urldate = {2023-10-29},
  abstract = {Spatio-temporal traffic volume forecasting technologies can effectively improve freeway traffic efficiency and the travel comfort of humans. To construct a high-precision traffic volume forecasting model, this study proposed a new ensemble deep graph reinforcement learning network. The modeling process of the spatio-temporal prediction model mainly included three steps. In step I, raw spatiotemporal traffic network datasets (traffic volumes, traffic speeds, weather, and holidays) were preprocessed and the adjacency matrix was constructed. In step II, a graph attention network (GAT) and graph convolution network (GCN) were used as the main predictors to build the spatio-temporal traffic volume forecasting model and obtain the forecasting results, respectively. In step III, deep reinforcement learning was used to effectively analyze the correlations between the forecasting results from these two neural networks and the final results, so as to optimize the weight coefficient. The final result of the proposed model was obtained by combining the forecasting results from the GAT and GCN with the weight coefficient. Based on summarizing and analyzing the experimental results, it can be concluded that: (1) deep reinforcement learning can effectively integrate the two different graph neural networks and achieve better results than traditional ensemble methods; and (2) the presented ensemble model performs better than twenty-one models proposed by other researchers for all studied cases.},
  keywords = {0 - DRL,0 - GNN,0 - RL,0 - Unmentioned,A - Smart Mobility,A - Spatio-Temporal Forecasting,A - Traffic Volume Forecasting,DRL - DQL,DRL - DQN,GNN - GAT,GNN - GCN,RL - Single Agent,T - Attention Mechanism,T - GRU,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2022 - Shang et al - A new ensemble deep graph reinforcement learning network for spatio-temporal.pdf;../../../documents/school/dissertation/zotero/storage/HD4IPKJX/S1051200422000367.html}
}

@article{silverDeterministicPolicyGradient,
  title = {Deterministic {{Policy Gradient Algorithms}}},
  author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.},
  langid = {english},
  keywords = {0 - RL,DRL - DPG,RL - Single Agent,Type - Seminal},
  file = {../../../documents/school/dissertation/zotero/storage/2B6HHJ4N/Silver et al. - Deterministic Policy Gradient Algorithms.pdf}
}

@misc{silverMasteringChessShogi2017,
  title = {Mastering {{Chess}} and {{Shogi}} by {{Self-Play}} with a {{General Reinforcement Learning Algorithm}}},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year = {2017},
  month = dec,
  number = {arXiv:1712.01815},
  eprint = {1712.01815},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1712.01815},
  urldate = {2023-12-30},
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  archiveprefix = {arxiv},
  keywords = {0 - DRL,0 - RL,DRL - AlphaZero,RL - Single Agent,Type - Seminal},
  file = {../../../documents/school/dissertation/bibliography/Preprint/2017 - Silver et al - Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning.pdf;../../../documents/school/dissertation/zotero/storage/AV9F55RK/1712.html}
}

@inproceedings{siregarNetworkReconfigurationDistributed2021,
  title = {Network {{Reconfiguration}} of {{Distributed Generation}} for {{Reduced Power Loss}} and {{Increasing Voltage Profile}} by {{Using Artificial Bee Colony}}},
  booktitle = {2021 {{IEEE}} 5th {{International Conference}} on {{Information Technology}}, {{Information Systems}} and {{Electrical Engineering}} ({{ICITISEE}})},
  author = {Siregar, Yulianta and Pane, Zulkarnen and Astianta Bukit, Ferry Rahmat and Gunanta Sembiring, Enda},
  year = {2021},
  month = nov,
  pages = {235--240},
  doi = {10.1109/ICITISEE53823.2021.9655826},
  urldate = {2024-01-12},
  abstract = {The addition of distributed generation (DG) to the distribution network is essential with increasing load demand. It makes more complex problems, so it is necessary to create ideal conditions for the electrical network system, such as the smallest possible power loss value and the best possible voltage profile. Network reconfiguration can optimize the existing system. Various techniques have previously optimized network reconfiguration on a system connected to DG. This research reconfigures the DG-connected IEEE 33 bus distribution network by using the artificial bee colony algorithm for balanced and unbalanced load conditions. The results satisfy the desired requirement by reducing power losses in the system from 167.4 kW to 107.3 kW, increasing the minimum voltage profile to 94.847\% on bus 32.},
  keywords = {0 - Data,A - Smart Grid},
  annotation = {0 citations (Crossref) [2024-01-12]},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2021 - Siregar et al - Network Reconfiguration of Distributed Generation for Reduced Power Loss and.pdf;../../../documents/school/dissertation/zotero/storage/EW9P7EXF/9655826.html}
}

@article{stokerEnergyStorageOutranks2023,
  title = {Energy Storage Outranks Solar in Company Investment Plans},
  author = {Stoker, Liam and Stoker, Liam},
  year = {2023},
  month = dec,
  journal = {Reuters},
  urldate = {2024-02-03},
  abstract = {Energy storage is set to overtake solar as the leading technology for energy transition investments in the next three years, a new industry survey by Reuters Events shows.},
  chapter = {Energy},
  langid = {american},
  file = {../../../documents/school/dissertation/zotero/storage/B46GK2TG/energy-storage-outranks-solar-company-investment-plans-2023-12-11.html}
}

@article{suEvolutionStrategiesbasedOptimized2023,
  title = {Evolution Strategies-Based Optimized Graph Reinforcement Learning for Solving Dynamic Job Shop Scheduling Problem},
  author = {Su, Chupeng and Zhang, Cong and Xia, Dan and Han, Baoan and Wang, Chuang and Chen, Gang and Xie, Longhan},
  year = {2023},
  month = sep,
  journal = {Applied Soft Computing},
  volume = {145},
  pages = {110596},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2023.110596},
  urldate = {2023-10-29},
  abstract = {The job shop scheduling problem (JSSP) with dynamic events and uncertainty is a strongly NP-hard combinatorial optimization problem (COP) with extensive applications in the manufacturing system. Recently, growing interest has been aroused in utilizing machine learning techniques to solve the JSSP. However, most prior arts cannot handle dynamic events and barely consider uncertainties. To close this gap, this paper proposes a framework to solve a dynamic JSSP (DJSP) with machine breakdown and stochastic processing time based on Graph Neural Network (GNN) and deep reinforcement learning (DRL). To this end, we first formulate the DJSP as a Markov Decision Process (MDP), where disjunctive graph represent the states. Secondly, we propose a GNN-based model to effectively extract the embeddings of the state by considering the features of the dynamic events and the stochasticity of the problem, e.g., the machine breakdown and stochastic processing time. Then, the model constructs solutions by dispatching optimal operations to machines based on the learned embeddings. Notably, we propose to use the evolution strategies (ES) to find optimal policies that are more stable and robust than conventional DRL algorithms. The extensive experiments show that our method substantially outperforms existing reinforcement learning-based and traditional methods on multiple classic benchmarks.},
  keywords = {0 - GNN,0 - GRL,0 - RL,0 - Unmentioned,0 - Untagged,A - Evolution strategies,A - Job Shop Scheduling},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Su et al - Evolution strategies-based optimized graph reinforcement learning for solving.pdf;../../../documents/school/dissertation/zotero/storage/CBCSYUF6/S1568494623006142.html}
}

@article{sunGraphReinforcementLearningBasedTaskOffloading2023,
  title = {Graph-{{Reinforcement-Learning-Based Task Offloading}} for {{Multiaccess Edge Computing}}},
  author = {Sun, Zhenchuan and Mo, Yijun and Yu, Chen},
  year = {2023},
  month = feb,
  journal = {IEEE Internet of Things Journal},
  volume = {10},
  number = {4},
  pages = {3138--3150},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2021.3123822},
  urldate = {2023-10-29},
  abstract = {Network applications involve massive heterogeneous data fusion and analysis. Artificial intelligence can significantly improve the convenience and user experience, but it requires a lot of storage, bandwidth, and computing resources. Multiaccess edge computing (MEC) extends intelligence services to IoT devices through offloading approaches and joint processing, which solves the resource bottleneck. However, designing advanced collaboration technology to offload tasks to MEC servers is still challenging. Heuristic algorithms and deep reinforcement learning (DRL)-based approaches have been proposed to offload tasks and minimize application latency. However, heuristic algorithms heavily depend on accurate mathematical models for the MEC system, and DRL does not make fair use of the relationship between devices in the MEC graph. To solve this, we propose a task offloading mechanism based on graph neural network (GNN), which can directly learn on graph data with messages passing and aggregation. We propose a graph reinforcement learning-based offloading (GRLO) framework, which models MEC as an acyclic graph and the offloading policy by graph state migration. GRLO combines GNN with the actor-critic network and trains offloading decision makers without labels. To efficiently train the GRLO, we propose a method that quickly explores action space and approaches the optimal solution. The numerical results show that the GRLO has lower latency compared to baselines while having generalization ability to new environments and topologies. Moreover, we verified the effectiveness of GRLO on a prototype.},
  keywords = {0 - DRL,0 - RL,0 - Unmentioned,0 - Untagged,A - Edge Computing,A - MEC,A - Task Offloading,DRL - Actor-Critic},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Sun et al - Graph-Reinforcement-Learning-Based Task Offloading for Multiaccess Edge.pdf;../../../documents/school/dissertation/zotero/storage/4WBW68MP/9592681.html}
}

@article{suttonMDPsSemiMDPsFramework1999,
  title = {Between {{MDPs}} and Semi-{{MDPs}}: {{A}} Framework for Temporal Abstraction in Reinforcement Learning},
  shorttitle = {Between {{MDPs}} and Semi-{{MDPs}}},
  author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
  year = {1999},
  month = aug,
  journal = {Artificial Intelligence},
  volume = {112},
  number = {1-2},
  pages = {181--211},
  issn = {00043702},
  doi = {10.1016/S0004-3702(99)00052-1},
  urldate = {2023-09-23},
  abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options{\textemdash}closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macroutility problem. {\textcopyright} 1999 Published by Elsevier Science B.V. All rights reserved.},
  langid = {english},
  keywords = {0 - RL,0 - Unmentioned,0 - Untagged,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/1999 - Sutton et al - Between MDPs and semi-MDPs.pdf}
}

@book{suttonReinforcementLearningIntroduction2014,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew},
  year = {2014},
  series = {Adaptive Computation and Machine Learning},
  edition = {Nachdruck},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  isbn = {978-0-262-19398-6},
  langid = {english},
  keywords = {0 - RL,RL - Single Agent,Type - Book},
  file = {../../../documents/school/dissertation/bibliography/Book/2014 - Sutton-Barto - Reinforcement learning.pdf}
}

@inproceedings{tangDependentTaskOffloading2020,
  title = {Dependent {{Task Offloading}} for {{Multiple Jobs}} in {{Edge Computing}}},
  booktitle = {2020 29th {{International Conference}} on {{Computer Communications}} and {{Networks}} ({{ICCCN}})},
  author = {Tang, Zhiqing and Lou, Jiong and Zhang, Fuming and Jia, Weijia},
  year = {2020},
  month = aug,
  pages = {1--9},
  issn = {2637-9430},
  doi = {10.1109/ICCCN49398.2020.9209593},
  urldate = {2024-01-05},
  abstract = {The dependent task offloading problem for one single job in edge computing (EC) has drawn attention widely. Unlike most existing approaches that only focus on a single job, we aim to solve the dependent task offloading problem for multiple jobs, which is more general in the real world. To solve this problem, we propose a deep reinforcement learning (DRL) based multi-job dependent task offloading algorithm. Specifically, 1) we model edge nodes, jobs, and tasks in a resource-limited EC scenario, where the dependent tasks of multiple jobs are offloaded to the nodes to be processed. Then we model the task offloading decision as a Markov decision process (MDP) problem to minimize the transmission cost and computation cost. 2) To represent the state space of MDP and to accelerate decision-making in EC, we propose a DRL-based algorithm with the aid of graph convolutional network (GCN) to extract the dependency information of different tasks and then improve the action selection process. 3) We conduct experiments with real-world trace, demonstrating our algorithm outperforms the baseline algorithms 13.78\% on average in regarding to offloading cost.},
  keywords = {0 - Cited,0 - DRL,0 - RL,A - Edge Computing,A - Task Offloading,DRL - DQL,GNN - GCN,RL - Multi-Agent,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2020 - Tang et al - Dependent Task Offloading for Multiple Jobs in Edge Computing.pdf;../../../documents/school/dissertation/zotero/storage/C7G5N54K/9209593.html}
}

@incollection{tangGraphNeuralNetworks2022,
  title = {Graph {{Neural Networks}} for {{Node Classification}}},
  booktitle = {Graph {{Neural Networks}}: {{Foundations}}, {{Frontiers}}, and {{Applications}}},
  author = {Tang, Jian and Liao, Renjie},
  editor = {Wu, Lingfei and Cui, Peng and Pei, Jian and Zhao, Liang},
  year = {2022},
  pages = {41--61},
  publisher = {{Springer Nature}},
  address = {{Singapore}},
  doi = {10.1007/978-981-16-6054-2_4},
  urldate = {2024-01-07},
  abstract = {Graph Neural Networks are neural architectures specifically designed for graph-structured data, which have been receiving increasing attention recently and applied to different domains and applications. In this chapter, we focus on a fundamental task on graphs: node classification.We will give a detailed definition of node classification and also introduce some classical approaches such as label propagation. Afterwards, we will introduce a few representative architectures of graph neural networks for node classification. We will further point out the main difficulty{\textemdash} the oversmoothing problem{\textemdash}of training deep graph neural networks and present some latest advancement along this direction such as continuous graph neural networks.},
  isbn = {9789811660542},
  langid = {english},
  file = {../../../documents/school/dissertation/bibliography/Book Section/2022 - Tang-Liao - Graph Neural Networks for Node Classification.pdf}
}

@book{traskGrokkingDeepLearning2019,
  title = {Grokking {{Deep Learning}}},
  author = {Trask, Andrew W.},
  year = {2019},
  publisher = {{Manning}},
  address = {{Shelter Island}},
  abstract = {"Grokking Deep Learning teaches you to build deep learning neural networks from scratch! In his engaging style, seasoned deep learning expert Andrew Trask shows you the science under the hood, so you grok for yourself every detail of training neural networks. Using only Python and its math-supporting library, NumPy, you'll train your own neural networks to see and understand images, translate text into different languages, and even write like Shakespeare!"--},
  isbn = {978-1-61729-370-2},
  lccn = {QA76.87 .T73 2019},
  keywords = {0 - Neural Network,Type - Book},
  annotation = {OCLC: on1084981313},
  file = {../../../documents/school/dissertation/bibliography/Book/2019 - Trask - Grokking Deep Learning.pdf}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2024-01-02},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  keywords = {0 - Unmentioned,0 - Untagged},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2017 - Vaswani et al - Attention is All you Need.pdf}
}

@inproceedings{velickovicGraphAttentionNetworks2018,
  title = {Graph {{Attention Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2018},
  month = feb,
  urldate = {2023-10-31},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of computationally intensive matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  langid = {english},
  keywords = {0 - GNN,GNN - GAT,Type - Seminal},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2018 - Veličković et al - Graph Attention Networks.pdf}
}

@article{vijayapriyaSmartGridOverview2011,
  title = {Smart {{Grid}}: {{An Overview}}},
  shorttitle = {Smart {{Grid}}},
  author = {Vijayapriya, Tamilmaran and Kothari, Dwarkadas Pralhadas},
  year = {2011},
  journal = {Smart Grid and Renewable Energy},
  volume = {02},
  number = {04},
  pages = {305--311},
  issn = {2151-481X, 2151-4844},
  doi = {10.4236/sgre.2011.24035},
  urldate = {2023-11-20},
  abstract = {This paper briefly discusses evolution of Smart Grid development. Smart Grid is important as it will take us towards energy independence and environmentally sustainable economic growth. Growth of Smart Power Grid in India will slowly but surely take us towards fulfilling the dreams of former President Dr. A.P.J. Abdul Kalam, ``Energy for all and Energy forever''.},
  langid = {english},
  keywords = {0 - Unmentioned,0 - Untagged,A - Smart Grid},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2011 - Vijayapriya-Kothari - Smart Grid.pdf}
}

@article{wangCacheAidedMECIoT2023,
  title = {Cache-{{Aided MEC}} for {{IoT}}: {{Resource Allocation Using Deep Graph Reinforcement Learning}}},
  shorttitle = {Cache-{{Aided MEC}} for {{IoT}}},
  author = {Wang, Dan and Bai, Yalu and Huang, Gang and Song, Bin and Yu, F. Richard},
  year = {2023},
  month = jul,
  journal = {IEEE Internet of Things Journal},
  volume = {10},
  number = {13},
  pages = {11486--11496},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2023.3244909},
  urldate = {2023-10-29},
  abstract = {With the growing demand for latency-sensitive and compute-intensive services in the Internet of Things (IoT), multiaccess edge computing (MEC)-enabled IoT is envisioned as a promising technique that allows network nodes to have computing and caching capabilities. In this article, we propose a cache-aided MEC (CA-MEC) offloading framework for joint optimization of communication, computing, and caching (3C) resources in the MEC-enabled IoT. Our goal is to optimize the offloading decision and resource allocation strategy to minimize the system latency subject to dynamic cache capacities and computing resource constraints. We first formulate this optimization problem as a multiagent decision problem, a partially observable Markov decision process (POMDP). Then, the deep graph convolution reinforcement learning (DGRL) method is applied to motivate the agents to learn optimal strategies cooperatively in a highly dynamic environment. Simulations show that our method is highly effective for computation offloading and resource allocation and performs superior results in a large-scale network.},
  keywords = {0 - DRL,0 - RL,0 - Unmentioned,A - Internet of Things,A - MEC,A - Task Offloading,DRL - DQL,DRL - DQN,GNN - GCN,RL - Multi-Agent,T - Attention Mechanism,T - Multi-head Attention,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Wang et al - Cache-Aided MEC for IoT.pdf;../../../documents/school/dissertation/zotero/storage/WPVZ9X4I/10044184.html}
}

@inproceedings{wangGCNRLCircuitDesigner2020,
  title = {{{GCN-RL Circuit Designer}}: {{Transferable Transistor Sizing}} with {{Graph Neural Networks}} and {{Reinforcement Learning}}},
  shorttitle = {{{GCN-RL Circuit Designer}}},
  booktitle = {2020 57th {{ACM}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  author = {Wang, Hanrui and Wang, Kuan and Yang, Jiacheng and Shen, Linxiao and Sun, Nan and Lee, Hae-Seung and Han, Song},
  year = {2020},
  month = jul,
  pages = {1--6},
  issn = {0738-100X},
  doi = {10.1109/DAC18072.2020.9218757},
  urldate = {2023-12-18},
  abstract = {Automatic transistor sizing is a challenging problem in circuit design due to the large design space, complex performance tradeoffs, and fast technology advancements. Although there have been plenty of work on transistor sizing targeting on one circuit, limited research has been done on transferring the knowledge from one circuit to another to reduce the re-design overhead. In this paper, we present GCN-RL Circuit Designer, leveraging reinforcement learning (RL) to transfer the knowledge between different technology nodes and topologies. Moreover, inspired by the simple fact that circuit is a graph, we learn on the circuit topology representation with graph convolutional neural networks (GCN). The GCN-RL agent extracts features of the topology graph whose vertices are transistors, edges are wires. Our learning-based optimization consistently achieves the highest Figures of Merit (FoM) on four different circuits compared with conventional black box optimization methods (Bayesian Optimization, Evolutionary Algorithms), random search and human expert designs. Experiments on transfer learning between five technology nodes and two circuit topologies demonstrate that RL with transfer learning can achieve much higher FoMs than methods without knowledge transfer. Our transferable optimization method makes transistor sizing and design porting more effective and efficient.},
  keywords = {0 - Cited,0 - DRL,0 - GRL,0 - RL,A - Automatic Transistor Sizing,DRL - Actor-Critic,DRL - DDPG,GNN - GCN,RL - Single Agent,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2020 - Wang et al - GCN-RL Circuit Designer.pdf;../../../documents/school/dissertation/zotero/storage/RMCQBFT8/9218757.html}
}

@misc{waradpandeGraphbasedStateRepresentation2021,
  title = {Graph-Based {{State Representation}} for {{Deep Reinforcement Learning}}},
  author = {Waradpande, Vikram and Kudenko, Daniel and Khosla, Megha},
  year = {2021},
  month = feb,
  number = {arXiv:2004.13965},
  eprint = {2004.13965},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2004.13965},
  urldate = {2023-12-17},
  abstract = {Deep RL approaches build much of their success on the ability of the deep neural network to generate useful internal representations. Nevertheless, they suffer from a high sample-complexity and starting with a good input representation can have a significant impact on the performance. In this paper, we exploit the fact that the underlying Markov decision process (MDP) represents a graph, which enables us to incorporate the topological information for effective state representation learning. Motivated by the recent success of node representations for several graph analytical tasks we specifically investigate the capability of node representation learning methods to effectively encode the topology of the underlying MDP in Deep RL. To this end we perform a comparative analysis of several models chosen from 4 different classes of representation learning algorithms for policy learning in grid-world navigation tasks, which are representative of a large class of RL problems. We find that all embedding methods outperform the commonly used matrix representation of grid-world environments in all of the studied cases. Moreoever, graph convolution based methods are outperformed by simpler random walk based methods and graph linear autoencoders.},
  archiveprefix = {arxiv},
  keywords = {0 - DRL,0 - GRL,0 - RL,0 - Unmentioned,Type - Survey},
  file = {../../../documents/school/dissertation/bibliography/Preprint/2021 - Waradpande et al - Graph-based State Representation for Deep Reinforcement Learning.pdf;../../../documents/school/dissertation/zotero/storage/E9E768QI/2004.html}
}

@misc{weiGraphLearningIts2023,
  title = {Graph {{Learning}} and {{Its Applications}}: {{A Holistic Survey}}},
  shorttitle = {Graph {{Learning}} and {{Its Applications}}},
  author = {Wei, Shaopeng and Zhao, Yu and Chen, Xingyan and Li, Qing and Zhuang, Fuzhen and Liu, Ji and Kou, Gang},
  year = {2023},
  month = jun,
  number = {arXiv:2212.08966},
  eprint = {2212.08966},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-11-17},
  abstract = {Graph learning aims to learn complex relationships among nodes and the topological structure of graphs, such as social networks, academic networks and e-commerce networks, which are common in the real world. Those relationships make graphs special compared with traditional tabular data in which nodes are dependent on non-Euclidean space and contain rich information to explore. Graph learning developed from graph theory to graph data mining and now is empowered with representation learning, making it achieve great performances in various scenarios, even including text, image, chemistry, and biology. Due to the broad application prospects in the real world, graph learning has become a popular and promising area in machine learning. Thousands of works have been proposed to solve various kinds of problems in graph learning and is appealing more and more attention in academic community, which makes it pivotal to survey previous valuable works. Although some of the researchers have noticed this phenomenon and finished impressive surveys on graph learning. However, they failed to link related objectives, methods and applications in a more logical way and cover current ample scenarios as well as challenging problems due to the rapid expansion of the graph learning.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {0 - GNN,0 - Unmentioned,Type - Survey},
  file = {../../../documents/school/dissertation/bibliography/Preprint/2023 - Wei et al - Graph Learning and Its Applications.pdf}
}

@article{wuComprehensiveSurveyGraph2021,
  title = {A {{Comprehensive Survey}} on {{Graph Neural Networks}}},
  author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
  year = {2021},
  month = jan,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {1},
  pages = {4--24},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2020.2978386},
  urldate = {2023-12-17},
  abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial-temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.},
  keywords = {0 - GNN,0 - Unmentioned,Type - Survey},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2021 - Wu et al - A Comprehensive Survey on Graph Neural Networks.pdf;../../../documents/school/dissertation/zotero/storage/2DWQGBJ7/9046288.html}
}

@book{wuGraphNeuralNetworks2022,
  title = {Graph {{Neural Networks}}: {{Foundations}}, {{Frontiers}}, and {{Applications}}},
  shorttitle = {Graph {{Neural Networks}}},
  editor = {Wu, Lingfei and Cui, Peng and Pei, Jian and Zhao, Liang},
  year = {2022},
  publisher = {{Springer Nature Singapore}},
  address = {{Singapore}},
  doi = {10.1007/978-981-16-6054-2},
  urldate = {2023-11-28},
  isbn = {9789811660535 9789811660542},
  langid = {english},
  keywords = {0 - GNN,Type - Book},
  file = {../../../documents/school/dissertation/bibliography/Book/2022 - Wu et al - Graph Neural Networks.pdf}
}

@incollection{wuGraphNeuralNetworks2022a,
  title = {Graph {{Neural Networks}}},
  booktitle = {Graph {{Neural Networks}}: {{Foundations}}, {{Frontiers}}, and {{Applications}}},
  author = {Wu, Lingfei and Cui, Peng and Pei, Jian and Zhao, Liang and Song, Le},
  editor = {Wu, Lingfei and Cui, Peng and Pei, Jian and Zhao, Liang},
  year = {2022},
  pages = {27--37},
  publisher = {{Springer Nature}},
  address = {{Singapore}},
  doi = {10.1007/978-981-16-6054-2_3},
  urldate = {2024-01-07},
  abstract = {Deep Learning has become one of the most dominant approaches in Artificial Intelligence research today. Although conventional deep learning techniques have achieved huge successes on Euclidean data such as images, or sequence data such as text, there are many applications that are naturally or best represented with a graph structure. This gap has driven a tide in research for deep learning on graphs, among them Graph Neural Networks (GNNs) are the most successful in coping with various learning tasks across a large number of application domains. In this chapter, we will systematically organize the existing research of GNNs along three axes: foundations, frontiers, and applications. We will introduce the fundamental aspects of GNNs ranging from the popular models and their expressive powers, to the scalability, interpretability and robustness of GNNs. Then, we will discuss various frontier research, ranging from graph classification and link prediction, to graph generation and transformation, graph matching and graph structure learning. Based on them, we further summarize the basic procedures which exploit full use of various GNNs for a large number of applications. Finally, we provide the organization of our book and summarize the roadmap of the various research topics of GNNs.},
  isbn = {9789811660542},
  langid = {english},
  file = {../../../documents/school/dissertation/bibliography/Book Section/2022 - Wu et al - Graph Neural Networks.pdf}
}

@article{xingBilevelGraphReinforcement2023,
  title = {A {{Bilevel Graph Reinforcement Learning Method}} for {{Electric Vehicle Fleet Charging Guidance}}},
  author = {Xing, Qiang and Xu, Yan and Chen, Zhong},
  year = {2023},
  month = jul,
  journal = {IEEE Transactions on Smart Grid},
  volume = {14},
  number = {4},
  pages = {3309--3312},
  issn = {1949-3061},
  doi = {10.1109/TSG.2023.3240580},
  urldate = {2023-10-29},
  abstract = {This letter proposes a bilevel graph reinforcement learning method for electric vehicle (EV) fleet charging guidance, achieving collaborative optimization of the transportation-electrification coupled system. A dual-agent architecture is first constructed, where the upper-level is used for charging and the lower-level is used for routing. The EV traveling and charging behavior is characterized as a graph-structured interaction process. A graph attention network (GAT) is leveraged to extract the topology correlation and feature information. Then the extracted topology embedded with knowledge, as intermediate latent environment states, is fed into the underlying network of deep reinforcement learning (DRL). A DRL-based sequential scheduling pattern is developed to realize the guidance of multiple EVs. Extensive experimental results verify the superiority and adaptability of our proposed methodology.},
  keywords = {0 - Cited,0 - DRL,0 - GNN,0 - GRL,0 - RL,A - EV Charging Guidance,A - Smart Grid,A - Smart Mobility,A - Vehicle Routing,DRL - DQL,DRL - Rainbow,GNN - GAT,RL - Single Agent,T - Attention Mechanism,T - Multi-head Attention},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Xing et al - A Bilevel Graph Reinforcement Learning Method for Electric Vehicle Fleet.pdf;../../../documents/school/dissertation/zotero/storage/UXYA7TKW/10029881.html}
}

@article{xingGraphReinforcementLearningBased2023,
  title = {A {{Graph Reinforcement Learning-Based Decision-Making Platform}} for {{Real-Time Charging Navigation}} of {{Urban Electric Vehicles}}},
  author = {Xing, Qiang and Xu, Yan and Chen, Zhong and Zhang, Ziqi and Shi, Zhao},
  year = {2023},
  month = mar,
  journal = {IEEE Transactions on Industrial Informatics},
  volume = {19},
  number = {3},
  pages = {3284--3295},
  issn = {1941-0050},
  doi = {10.1109/TII.2022.3210264},
  urldate = {2023-10-29},
  abstract = {To provide efficient charging behavior decision-making for urban electric vehicles (EVs), this article proposes a new platform for real-time EV charging navigation (EVCN) based on graph reinforcement learning. Considering the interaction of EVs with charging stations (CSs) and traffic networks, the navigation goal of the ``vehicle-station-network'' coupled system is to minimize the charging cost and traveling time of EV owners. Specifically, to realize data acquisition and decision-making output, we first characterize the EV charging and traveling behavior as the dynamic interaction process of graph-structured networks. A graph convolutional network is used to extract the environment information required for EVCN, and the generated environment feature is fed into the underlying network of deep reinforcement learning (DRL), which can help the agent better understand massive graph-structured data. Then, the real-time navigation problem is duly formulated as a finite Markov decision process. A sequential scheduling pattern is built according to the sorting of EV charging urgency and solved by a Rainbow-based DRL algorithm. It achieves the sequential recommendation of CSs and planning of traveling routes for multiple EVs. Case studies are conducted within a practical zone in Nanjing, China. Simulation results verify the developed platform and the solving method.},
  keywords = {0 - Cited,0 - DRL,0 - GRL,0 - RL,A - EV Charging Guidance,A - Smart Grid,A - Smart Mobility,A - Vehicle Routing,DRL - DQL,DRL - DQN,DRL - Rainbow,GNN - GCN,RL - Single Agent,T - Dropout Layer Technology,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Xing et al - A Graph Reinforcement Learning-Based Decision-Making Platform for Real-Time.pdf;../../../documents/school/dissertation/zotero/storage/9JWVZ2FS/9906438.html}
}

@article{xingModellingDrivingCharging2021,
  title = {Modelling Driving and Charging Behaviours of Electric Vehicles Using a Data-Driven Approach Combined with Behavioural Economics Theory},
  author = {Xing, Qiang and Chen, Zhong and Zhang, Ziqi and Wang, Ruisheng and Zhang, Tian},
  year = {2021},
  month = nov,
  journal = {Journal of Cleaner Production},
  volume = {324},
  pages = {129243},
  issn = {0959-6526},
  doi = {10.1016/j.jclepro.2021.129243},
  urldate = {2023-12-18},
  abstract = {With the popularization and promotion of electric vehicles (EVs), their interactions with power grids and traffic networks have increasingly deepened. Accurate modelling of EV behaviour can faithfully depict the characteristics of EV driving and charging. However, most existing modelling researches fail to adopt real-world travel data and consider realistic perceptual decision-making psychology of owners. Thus, this paper proposes a novel behavioural modelling for EVs based on a data-driven approach combined with behavioural economics theory. To characterize the driving behaviour of owners using actual data, a systematic data mining and modelling approach is firstly proposed based on the open-source `Didi' traffic travel data set, which obtains the traffic operation rules and the regenerative behaviour characteristics data. According to the subjective perceptual characteristics of social economic man, a Cumulative Prospect Theory-based modelling framework is further developed to quantify the uncertain and stochastic charging decision-making behaviour of EV users. Moreover, the user's preferences and attitudes are evaluated by calculating their cumulative prospect value when choosing charging stations. Finally, the most suitable charging station is recommended for EVs with charging requirements. Case studies are conducted within a practical zone in Nanjing, China. The results demonstrate that the traffic travel rules of vehicle owners have typical date types and functional area distribution characteristics. And the travel time and space of private and commercial vehicles are relatively regular, whereas the travel rules of public vehicles are random. Besides, this proposed methodology can not only effectively capture the irrational decision-making characteristics of EV users' charging behaviour, but also achieve promising performance in terms of reducing the charging waiting cost. The user's decision-making regarding charging behaviour exhibits a higher risk-seeking preference than a risk-aversion preference.},
  keywords = {0 - Unmentioned,0 - Untagged},
  file = {../../../documents/school/dissertation/zotero/storage/CFBEIQ7N/S0959652621034296.html}
}

@article{xingRealtimeOptimalScheduling2023,
  title = {Real-Time Optimal Scheduling for Active Distribution Networks: {{A}} Graph Reinforcement Learning Method},
  shorttitle = {Real-Time Optimal Scheduling for Active Distribution Networks},
  author = {Xing, Qiang and Chen, Zhong and Zhang, Tian and Li, Xu and Sun, KeHui},
  year = {2023},
  month = feb,
  journal = {International Journal of Electrical Power \& Energy Systems},
  volume = {145},
  pages = {108637},
  issn = {0142-0615},
  doi = {10.1016/j.ijepes.2022.108637},
  urldate = {2023-11-20},
  abstract = {To improve the economy and safety of the active distribution network (ADN) operation, this paper proposes a real-time optimal scheduling strategy based on graph reinforcement learning (GRL), achieving online collaborative optimization of controllable equipment. For realizing the perception of inborn graph-structured features of the ADN and the formulation of the real-time scheduling scheme, the active-reactive power coordination process is firstly characterized as a dynamic graph-structured network interaction. A graph attention network (GAT) block is leveraged to extract and aggregate the topology branch correlation and node power information. Then the obtained topology information embedded with node features, as intermediate latent environment states, is fed into the underlying network architecture of deep reinforcement learning (DRL). Moreover, the power regulation problem of controllable equipment is duly formulated as a finite Markov decision process (FMDP). And the complex decision-making is solved by a deep deterministic policy gradient (DDPG) algorithm. Specially, under the electrical fault scenarios with variations in topology structures and feature information, GRL helps the agent efficiently learn and capture new graph-structured knowledge. Case studies are conducted within a modified IEEE 33-bus system. Extensive experimental results verify the superiority and adaptability of our proposed methodology.},
  keywords = {0 - Cited,0 - DRL,0 - GNN,0 - GRL,0 - RL,A - Dynamic Economic Dispatch,A - Smart Grid,D - IEEE33,D - No Source,DRL - Actor-Critic,DRL - DDPG,GNN - GAT,RL - Single Agent,T - Attention Mechanism,T - Multi-head Attention,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Xing et al - Real-time optimal scheduling for active distribution networks.pdf;../../../documents/school/dissertation/zotero/storage/DNLQ6SQ8/S0142061522006330.html}
}

@misc{xuHowPowerfulAre2019,
  title = {How {{Powerful}} Are {{Graph Neural Networks}}?},
  author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  year = {2019},
  month = feb,
  number = {arXiv:1810.00826},
  eprint = {1810.00826},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.00826},
  urldate = {2023-12-17},
  abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  archiveprefix = {arxiv},
  keywords = {0 - GNN,GNN - GIN,Type - Seminal},
  file = {../../../documents/school/dissertation/bibliography/Preprint/2019 - Xu et al - How Powerful are Graph Neural Networks.pdf;../../../documents/school/dissertation/zotero/storage/PSFK72QS/1810.html}
}

@article{xuRealtimeFastCharging2022,
  title = {Real-Time Fast Charging Station Recommendation for Electric Vehicles in Coupled Power-Transportation Networks: {{A}} Graph Reinforcement Learning Method},
  shorttitle = {Real-Time Fast Charging Station Recommendation for Electric Vehicles in Coupled Power-Transportation Networks},
  author = {Xu, Peidong and Zhang, Jun and Gao, Tianlu and Chen, Siyuan and Wang, Xiaohui and Jiang, Huaiguang and Gao, Wenzhong},
  year = {2022},
  month = oct,
  journal = {International Journal of Electrical Power \& Energy Systems},
  volume = {141},
  pages = {108030},
  issn = {0142-0615},
  doi = {10.1016/j.ijepes.2022.108030},
  urldate = {2023-10-29},
  abstract = {With the increasing penetration rate of electric vehicles, the fast charging demands of electric vehicles will have a significant influence on the operation of coupled power-transportation networks. To promote the interests of the coupled system, fast charging stations, and electric vehicle users, in this paper, a multi-objective system-level fast charging station recommendation method is proposed to dynamically allocate electric vehicles to suitable stations. The recommendation problem is formulated as a sequential decision-making problem and a deep reinforcement learning method is adopted. To deal with the network-structure coupled system states, graph attention networks are introduced. Considering the heterogeneity between entities, we propose a physical connection-based graph formulation method with feature projection to integrate multi-dimensional information from charging stations, traffic nodes, and power grid buses into a graph. The graph convolution of coupled system states can then be realized to promote environment perception. Besides, to address the long time-delay action execution in recommendation problem, a double-prioritized DQN({$\lambda$}) training mechanism is developed to update the guidance strategy, where an attention-prioritized cache construction method is proposed to improve the training efficiency cooperated with prioritized experience replay. The proposed graph reinforcement learning method is trained and evaluated in a joint power-transportation simulation platform. Simulation results show that the proposed strategy can promote the interest of multiple facets in coupled power-transportation networks by handling the requests in a real-time manner. Its feasibility and robustness in the urban transportation systems are also demonstrated.},
  keywords = {0 - Cited,0 - DRL,0 - GNN,0 - RL,0 - Unmentioned,0 - Untagged,A - EV Charging Guidance,A - Vehicle Routing,DRL - DQL,DRL - DQN,GNN - GAT},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2022 - Xu et al - Real-time fast charging station recommendation for electric vehicles in coupled.pdf;../../../documents/school/dissertation/zotero/storage/W9SG9WFJ/S0142061522000746.html}
}

@article{yanAutomaticVirtualNetwork2020,
  title = {Automatic {{Virtual Network Embedding}}: {{A Deep Reinforcement Learning Approach With Graph Convolutional Networks}}},
  shorttitle = {Automatic {{Virtual Network Embedding}}},
  author = {Yan, Zhongxia and Ge, Jingguo and Wu, Yulei and Li, Liangxiong and Li, Tong},
  year = {2020},
  month = jun,
  journal = {IEEE Journal on Selected Areas in Communications},
  volume = {38},
  number = {6},
  pages = {1040--1057},
  issn = {1558-0008},
  doi = {10.1109/JSAC.2020.2986662},
  urldate = {2024-01-06},
  abstract = {Virtual network embedding arranges virtual network services onto substrate network components. The performance of embedding algorithms determines the effectiveness and efficiency of a virtualized network, making it a critical part of the network virtualization technology. To achieve better performance, the algorithm needs to automatically detect the network status which is complicated and changes in a time-varying manner, and to dynamically provide solutions that can best fit the current network status. However, most existing algorithms fail to provide automatic embedding solutions in an acceptable running time. In this paper, we combine deep reinforcement learning with a novel neural network structure based on graph convolutional networks, and propose a new and efficient algorithm for automatic virtual network embedding. In addition, a parallel reinforcement learning framework is used in training along with a newly-designed multi-objective reward function, which has proven beneficial to the proposed algorithm for automatic embedding of virtual networks. Extensive simulation results under different scenarios show that our algorithm achieves best performance on most metrics compared with the existing state-of-the-art solutions, with upto 39.6\% and 70.6\% improvement on acceptance ratio and average revenue, respectively. Moreover, the results also demonstrate that the proposed solution possesses good robustness.},
  keywords = {0 - Cited,0 - DRL,0 - GRL,0 - RL,A - Automatic Virtual Network Embedding,DRL - A3C,DRL - Actor-Critic,GNN - GCN,RL - Single Agent,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2020 - Yan et al - Automatic Virtual Network Embedding.pdf;../../../documents/school/dissertation/zotero/storage/GDATSKAJ/9060910.html}
}

@article{yangDynamicEnergyDispatch2021,
  title = {Dynamic Energy Dispatch Strategy for Integrated Energy System Based on Improved Deep Reinforcement Learning},
  author = {Yang, Ting and Zhao, Liyuan and Li, Wei and Zomaya, Albert Y.},
  year = {2021},
  month = nov,
  journal = {Energy},
  volume = {235},
  pages = {121377},
  issn = {03605442},
  doi = {10.1016/j.energy.2021.121377},
  urldate = {2024-01-12},
  abstract = {Dynamic energy dispatch is an integral part of the operation optimization of integrated energy systems (IESs). Most existing dynamic dispatch schemes depend heavily on explicit forecast or mathematical models of the future uncertainties. Due to the randomness of renewable energy generation and energy demands, these approaches are limited by the accuracy of forecasting or model. A novel model-free dynamic dispatch strategy for IES based on improved deep reinforcement learning (DRL) is proposed to solve the problem. The IES dynamic dispatch problem is formulated as a Markov decision process (MDP), in which the uncertainties of renewable generation, electric load and heat load are considered. For solving the MDP, an improved deep deterministic policy gradient (DDPG) algorithm using prioritized experience replay mechanism and L2 regularization is developed, so as to improve the policy quality and learning efficiency of the dispatch strategy. The proposed approach does not require any forecast information or distribution knowledge, and can adaptively respond to the stochastic fluctuations of the supply and demands. Simulation results show the proposed dispatch strategy has faster convergence and lower operating costs than original DDPG-based strategy. In addition, the advantages of the proposed approach in terms of cost-effectiveness and stochastic environmental adaptation are validated.},
  langid = {english},
  keywords = {0 - DRL,0 - RL,A - Dynamic Economic Dispatch,D - CREST,DRL - Actor-Critic,DRL - DDPG,RL - Single Agent,T - L2 Regularization,T - Prioritized Experience Replay},
  file = {../../../documents/dissertation/bibliography/Journal Article/2021 - Yang et al - Dynamic energy dispatch strategy for integrated energy system based on improved.pdf}
}

@article{yangGAAPPONovelGraph2023,
  title = {{{GAA-PPO}}: {{A}} Novel Graph Adversarial Attack Method by Incorporating Proximal Policy Optimization},
  shorttitle = {{{GAA-PPO}}},
  author = {Yang, Shuxin and Chang, Xiaoyang and Zhu, Guixiang and Cao, Jie and Qin, Weiping and Wang, Youquan and Wang, Zhendong},
  year = {2023},
  month = nov,
  journal = {Neurocomputing},
  volume = {557},
  pages = {126707},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2023.126707},
  urldate = {2023-10-29},
  abstract = {The Graph Convolutional Network (GCN) has demonstrated impressive performance in processing graph structured data. However recent studies have revealed that GCN is vulnerable to adversarial attacks, where a small amount of data modification can significantly affect the performance of the GCN models. While most existing studies node injection attacks with graph reinforcement learning by considering gradient information, they still suffer from the problems that the step size of the policy gradient is difficult to determine, and the attack effect needs to be further improved. In light of the above issues, this paper proposes a Graph Adversarial Attack method by incorporating Proximal Policy Optimization named GAA-PPO, which fills subtasks of sequentially generating features and links for injected nodes without modifying existing nodes or edges. GAA-PPO comprises two main components: node injection attack network (actor network) and value prediction network (critic network). Specifically, the actor network leverages a node generator and an edge sampler to generate appropriate features and edges for the injected nodes. Notably, a novel edge sampler that incorporates Approximation Personalized Propagation of Neural Prediction (APPNP) is introduced to effectively propagate malicious features of the injected nodes. On the other hand, the critic network evaluates the performance of the perturbed graph at each stage. To enhance the stability of the algorithm, GAA-PPO employs the importance sampling technique of Proximal Policy Optimization (PPO) during the training process. Extensive experiments on three publicly benchmark datasets show that GAA-PPO yields significant performance advantages over the state-of-the-art method.},
  keywords = {0 - GNN,0 - RL,0 - Unmentioned,0 - Untagged},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Yang et al - GAA-PPO.pdf;../../../documents/school/dissertation/zotero/storage/YDQPXDKD/S0925231223008305.html}
}

@article{yangGeneralizedSingleVehicleBasedGraph2022,
  title = {Generalized {{Single-Vehicle-Based Graph Reinforcement Learning}} for {{Decision-Making}} in {{Autonomous Driving}}},
  author = {Yang, Fan and Li, Xueyuan and Liu, Qi and Li, Zirui and Gao, Xin},
  year = {2022},
  month = jan,
  journal = {Sensors},
  volume = {22},
  number = {13},
  pages = {4935},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1424-8220},
  doi = {10.3390/s22134935},
  urldate = {2023-10-29},
  abstract = {In the autonomous driving process, the decision-making system is mainly used to provide macro-control instructions based on the information captured by the sensing system. Learning-based algorithms have apparent advantages in information processing and understanding for an increasingly complex driving environment. To incorporate the interactive information between agents in the environment into the decision-making process, this paper proposes a generalized single-vehicle-based graph neural network reinforcement learning algorithm (SGRL algorithm). The SGRL algorithm introduces graph convolution into the traditional deep neural network (DQN) algorithm, adopts the training method for a single agent, designs a more explicit incentive reward function, and significantly improves the dimension of the action space. The SGRL algorithm is compared with the traditional DQN algorithm (NGRL) and the multi-agent training algorithm (MGRL) in the highway ramp scenario. Results show that the SGRL algorithm has outstanding advantages in network convergence, decision-making effect, and training efficiency.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {0 - DRL,0 - RL,0 - Unmentioned,0 - Untagged,A - Autonomous Vehicles},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2022 - Yang et al - Generalized Single-Vehicle-Based Graph Reinforcement Learning for.pdf}
}

@article{yanGraphCooperationDeep2023,
  title = {Graph Cooperation Deep Reinforcement Learning for Ecological Urban Traffic Signal Control},
  author = {Yan, Liping and Zhu, Lulong and Song, Kai and Yuan, Zhaohui and Yan, Yunjuan and Tang, Yue and Peng, Chan},
  year = {2023},
  month = mar,
  journal = {Applied Intelligence},
  volume = {53},
  number = {6},
  pages = {6248--6265},
  issn = {1573-7497},
  doi = {10.1007/s10489-022-03208-w},
  urldate = {2023-11-13},
  abstract = {Cooperation between intersections in large-scale road networks is critical in traffic congestion. Currently, most traffic signals cooperate via pre-defined timing phases, which is extremely inefficient in real-time traffic scenarios. Most existing studies on multi-agent reinforcement learning (MARL) traffic signal control have focused on designing efficient communication methods, but have ignored the importance of how agents interact in cooperative communication. To achieve more efficient cooperation among traffic signals and alleviate urban traffic congestion, this study constructs a Graph Cooperation Q-learning Network Traffic Signal Control (GCQN-TSC) model, which is a graph cooperation network with an embedded self-attention mechanism that enables agents to adjust their attention in real time according to the dynamic traffic flow information, perceive the traffic environment quickly and effectively in a larger range, and help agents achieve more effective collaboration. Moreover, the Deep Graph Q-learning (DGQ) algorithm is proposed in this model to optimize the traffic signal control strategy according to the spatio-temporal characteristics of different traffic scenes and provide the optimal signal phase for each intersection. This study also integrates the ecological traffic concept into MARL traffic signal control, which aims to reduce traffic exhaust emissions. Finally, the proposed GCQN-TSC is experimentally validated both in a synthetic traffic grid and a real-world traffic network using the SUMO simulator. The experimental results show that GCQN-TSC outperforms other traffic signal control methods in almost all performance metrics, including average queue length and waiting time, as it can aggregate information acquired from collaborative agents and make network-level signal optimization decisions.},
  langid = {english},
  keywords = {0 - DRL,0 - RL,0 - Unmentioned,A - Smart Mobility,A - Traffic Signal Control,DRL - DQL,DRL - DQN,GNN - GCN,RL - Multi-Agent,T - Attention Mechanism,T - GRU,T - Multi-head Attention,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Yan et al - Graph cooperation deep reinforcement learning for ecological urban traffic.pdf}
}

@article{yanMultiAgentSafe2023,
  title = {Multi {{Agent Safe Graph Reinforcement Learning}} for {{PV Inverter}} s {{Based Real-Time De}} Centralized {{Volt}}/{{Var Control}} in {{Zoned Distribution Networks}}},
  author = {Yan, Rudai and Xing, Qiang and Xu, Yan},
  year = {2023},
  journal = {IEEE Transactions on Smart Grid},
  pages = {1--1},
  issn = {1949-3061},
  doi = {10.1109/TSG.2023.3277087},
  urldate = {2023-10-29},
  abstract = {To realize real-time voltage/var control (VVC) in active distribution networks (ADNs), this paper proposes a new multi-agent safe graph reinforcement learning method to optimize reactive power output from PV inverters. The network is divided into several zones, and a decentralized framework is proposed for coordinated control of reactive power output in each zone to regulate voltage profiles and minimize network energy loss. The VVC problem is formulated as a multi-agent decentralized partially observable constrained Markov decision process. Each zone has a central control agent that embeds graph convolution networks (GCNs) in the policy network to improve the decision-making capability. The GCN extracts graph-structured features from the ADN topology, reflecting the relationship between VVC and grid topology, and can filter noise and impute missing data. The training process includes primal-dual policy optimization to rigorously satisfy voltage safety constraints. Simulations on a 141-bus distribution system demonstrate that the proposed method can effectively minimize network energy loss and reduce voltage deviations, even in the presence of noisy or incomplete input measurements.},
  keywords = {0 - DRL,0 - RL,0 - Unmentioned,A - High Penetration of Distributed Generation,A - Voltage Regulation,DRL - Actor-Critic,DRL - Primal-Dual Policy Optimization,GNN - GCN,RL - Multi-Agent},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2023 - Yan et al - Multi Agent Safe Graph Reinforcement Learning for PV Inverter s Based Real-Time.pdf;../../../documents/school/dissertation/zotero/storage/95QMEAPA/10128717.html}
}

@inproceedings{yuanXGNNModelLevelExplanations2020,
  title = {{{XGNN}}: {{Towards Model-Level Explanations}} of {{Graph Neural Networks}}},
  shorttitle = {{{XGNN}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Yuan, Hao and Tang, Jiliang and Hu, Xia and Ji, Shuiwang},
  year = {2020},
  month = aug,
  series = {{{KDD}} '20},
  pages = {430--438},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3394486.3403085},
  urldate = {2024-01-01},
  abstract = {Graphs neural networks (GNNs) learn node features by aggregating and combining neighbor information, which have achieved promising performance on many graph tasks. However, GNNs are mostly treated as black-boxes and lack human intelligible explanations. Thus, they cannot be fully trusted and used in certain application domains if GNN models cannot be explained. In this work, we propose a novel approach, known as XGNN, to interpret GNNs at the model-level. Our approach can provide high-level insights and generic understanding of how GNNs work. In particular, we propose to explain GNNs by training a graph generator so that the generated graph patterns maximize a certain prediction of the model. We formulate the graph generation as a reinforcement learning task, where for each step, the graph generator predicts how to add an edge into the current graph. The graph generator is trained via a policy gradient method based on information from the trained GNNs. In addition, we incorporate several graph rules to encourage the generated graphs to be valid. Experimental results on both synthetic and real-world datasets show that our proposed methods help understand and verify the trained GNNs. Furthermore, our experimental results indicate that the generated graphs can provide guidance on how to improve the trained GNNs.},
  isbn = {978-1-4503-7998-4},
  keywords = {0 - Cited,0 - GNN,0 - GRL,0 - RL,A - Explainability of GNNs,GNN - GCN,GNN - Graph Generation,RL - Single Agent,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2020 - Yuan et al - XGNN.pdf}
}

@article{zengCombiningSubgoalGraphs2019,
  title = {Combining {{Subgoal Graphs}} with {{Reinforcement Learning}} to {{Build}} a {{Rational Pathfinder}}},
  author = {Zeng, J. and Qin, L. and Hu, Y. and Hu, C. and Yin, Q.},
  year = {2019},
  journal = {Applied Sciences (Switzerland)},
  volume = {9},
  number = {2},
  issn = {2076-3417},
  doi = {10.3390/app9020323},
  abstract = {In this paper, we present a hierarchical path planning framework called SG-RL (subgoal graphs-reinforcement learning), to plan rational paths for agents maneuvering in continuous and uncertain environments. By "rational", we mean (1) efficient path planning to eliminate first-move lags; (2) collision-free and smooth for agents with kinematic constraints satisfied. SG-RL works in a two-level manner. At the first level, SG-RL uses a geometric path-planning method, i.e., Simple Subgoal Graphs (SSG), to efficiently find optimal abstract paths, also called subgoal sequences. At the second level, SG-RL uses an RL method, i.e., Least-Squares Policy Iteration (LSPI), to learn near-optimal motion-planning policies which can generate kinematically feasible and collision-free trajectories between adjacent subgoals. The first advantage of the proposed method is that SSG can solve the limitations of sparse reward and local minima trap for RL agents; thus, LSPI can be used to generate paths in complex environments. The second advantage is that, when the environment changes slightly (i.e., unexpected obstacles appearing), SG-RL does not need to reconstruct subgoal graphs and replan subgoal sequences using SSGs, since LSPI can deal with uncertainties by exploiting its generalization ability to handle changes in environments. Simulation experiments in representative scenarios demonstrate that, compared with existing methods, SG-RL can work well on large-scale maps with relatively low action-switching frequencies and shorter path lengths, and SG-RL can deal with small changes in environments. We further demonstrate that the design of reward functions and the types of training environments are important factors for learning feasible policies. {\textcopyright} 2019 by the authors.},
  langid = {english},
  keywords = {0 - GRL,0 - RL,0 - Unmentioned,0 - Untagged,A - Path Planning,T - Subgoal Graphs,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2019 - Zeng et al - Combining Subgoal Graphs with Reinforcement Learning to Build a Rational.pdf;../../../documents/school/dissertation/zotero/storage/3SHWE9N3/display.html}
}

@article{zhangDeepLearningGraphs2022,
  title = {Deep {{Learning}} on {{Graphs}}: {{A Survey}}},
  shorttitle = {Deep {{Learning}} on {{Graphs}}},
  author = {Zhang, Ziwei and Cui, Peng and Zhu, Wenwu},
  year = {2022},
  month = jan,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {34},
  number = {1},
  pages = {249--270},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2020.2981333},
  urldate = {2023-11-27},
  abstract = {Deep learning has been shown to be successful in a number of domains, ranging from acoustics, images, to natural language processing. However, applying deep learning to the ubiquitous graph data is non-trivial because of the unique characteristics of graphs. Recently, substantial research efforts have been devoted to applying deep learning methods to graphs, resulting in beneficial advances in graph analysis techniques. In this survey, we comprehensively review the different types of deep learning methods on graphs. We divide the existing methods into five categories based on their model architectures and training strategies: graph recurrent neural networks, graph convolutional networks, graph autoencoders, graph reinforcement learning, and graph adversarial methods. We then provide a comprehensive overview of these methods in a systematic manner mainly by following their development history. We also analyze the differences and compositions of different methods. Finally, we briefly outline the applications in which they have been used and discuss potential future research directions.},
  keywords = {0 - GNN,0 - Unmentioned,Type - Survey},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2022 - Zhang et al - Deep Learning on Graphs.pdf;../../../documents/school/dissertation/zotero/storage/FYTP9U7E/9039675.html}
}

@misc{zhangGraphNeuralNetworks2023,
  title = {Graph {{Neural Networks}} for {{Power Grid Operational Risk Assessment}}},
  author = {Zhang, Yadong and Karve, Pranav M. and Mahadevan, Sankaran},
  year = {2023},
  month = nov,
  number = {arXiv:2311.03661},
  eprint = {2311.03661},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2311.03661},
  urldate = {2023-11-20},
  abstract = {In this article, the utility of graph neural network (GNN) surrogates for Monte Carlo (MC) sampling-based risk quantification in daily operations of power grid is investigated. The MC simulation process necessitates solving a large number of optimal power flow (OPF) problems corresponding to the sample values of stochastic grid variables (power demand and renewable generation), which is computationally prohibitive. Computationally inexpensive surrogates of the OPF problem provide an attractive alternative for expedited MC simulation. GNN surrogates are especially suitable due to their superior ability to handle graph-structured data. Therefore, GNN surrogates of OPF problem are trained using supervised learning. They are then used to obtain Monte Carlo (MC) samples of the quantities of interest (operating reserve, transmission line flow) given the (hours-ahead) probabilistic wind generation and load forecast. The utility of GNN surrogates is evaluated by comparing OPF-based and GNN-based grid reliability and risk for IEEE Case118 synthetic grid. It is shown that the GNN surrogates are sufficiently accurate for predicting the (bus-level, branch-level and system-level) grid state and enable fast as well as accurate operational risk quantification for power grids. The article thus develops various tools for fast reliability and risk quantification for real-world power grids using GNNs.},
  archiveprefix = {arxiv},
  keywords = {0 - Unmentioned,0 - Untagged},
  file = {../../../documents/school/dissertation/bibliography/Preprint/2023 - Zhang et al - Graph Neural Networks for Power Grid Operational Risk Assessment.pdf;../../../documents/school/dissertation/zotero/storage/Z5VMVARU/2311.html}
}

@inproceedings{zhangLearningDispatchJob2020,
  title = {Learning to {{Dispatch}} for {{Job Shop Scheduling}} via {{Deep Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Cong and Song, Wen and Cao, Zhiguang and Zhang, Jie and Tan, Puay Siew and Chi, Xu},
  year = {2020},
  volume = {33},
  pages = {1621--1632},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2024-01-02},
  abstract = {Priority dispatching rule (PDR) is widely used for solving real-world Job-shop scheduling problem (JSSP). However, the design of effective PDRs is a tedious task, requiring a myriad of specialized knowledge and often delivering limited performance. In this paper, we propose to automatically learn PDRs via an end-to-end deep reinforcement learning agent. We exploit the disjunctive graph representation of JSSP, and propose a Graph Neural Network based scheme to embed the states encountered during solving. The resulting policy network is size-agnostic, effectively enabling generalization on large-scale instances. Experiments show that the agent can learn high-quality PDRs from scratch with elementary raw features, and demonstrates strong performance against the best existing PDRs. The learned policies also perform well on much larger instances that are unseen in training.},
  keywords = {0 - Cited,0 - DRL,0 - GNN,0 - GRL,0 - RL,A - Job Shop Scheduling,DRL - PPO,GNN - GIN,RL - Multi-Agent,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Conference Paper/2020 - Zhang et al - Learning to Dispatch for Job Shop Scheduling via Deep Reinforcement Learning.pdf}
}

@article{zhaoLargeScaleMachineLearning2022,
  title = {Large-{{Scale Machine Learning Cluster Scheduling}} via {{Multi-Agent Graph Reinforcement Learning}}},
  author = {Zhao, Xiaoyang and Wu, Chuan},
  year = {2022},
  month = dec,
  journal = {IEEE Transactions on Network and Service Management},
  volume = {19},
  number = {4},
  pages = {4962--4974},
  issn = {1932-4537},
  doi = {10.1109/TNSM.2021.3139607},
  urldate = {2023-10-29},
  abstract = {Efficient scheduling of distributed deep learning (DL) jobs in large GPU clusters is crucial for resource efficiency and job performance. While server sharing among jobs improves resource utilization, interference among co-located DL jobs occurs due to resource contention. Interference-aware job placement has been studied, with white-box approaches based on explicit interference modeling and black-box schedulers with reinforcement learning. In today's clusters containing thousands of GPU servers, running a single scheduler to manage all arrival jobs in a timely and effective manner is challenging, due to the large workload scale. We adopt multiple schedulers in a large-scale cluster/data center, and propose a multi-agent reinforcement learning (MARL) scheduling framework to cooperatively learn fine-grained job placement policies, towards the objective of minimizing job completion time (JCT). To achieve topology-aware placements, our proposed framework uses hierarchical graph neural networks to encode the data center topology and server architecture. In view of a common lack of precise reward samples corresponding to different placements, a job interference model is further devised to predict interference levels in face of various co-locations, for training of the MARL schedulers. Testbed and trace-driven evaluations show that our scheduler framework outperforms representative scheduling schemes by more than 20\% in terms of average JCT, and is adaptive to various machine learning cluster topologies.},
  keywords = {0 - Unmentioned,0 - Untagged},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2022 - Zhao-Wu - Large-Scale Machine Learning Cluster Scheduling via Multi-Agent Graph.pdf;../../../documents/school/dissertation/zotero/storage/QJG6CEY4/9667106.html}
}

@article{zhaoLearningSequentialDistribution2022,
  title = {Learning {{Sequential Distribution System Restoration}} via {{Graph-Reinforcement Learning}}},
  author = {Zhao, Tianqiao and Wang, Jianhui},
  year = {2022},
  month = mar,
  journal = {IEEE Transactions on Power Systems},
  volume = {37},
  number = {2},
  pages = {1601--1611},
  issn = {1558-0679},
  doi = {10.1109/TPWRS.2021.3102870},
  urldate = {2023-10-29},
  abstract = {A distribution service restoration algorithm as a fundamental resilient paradigm for system operators provides an optimally coordinated, resilient solution to enhance the restoration performance. The restoration problem is formulated to coordinate distribution generators and controllable switches optimally. A model-based control scheme is usually designed to solve this problem, relying on a precise model and resulting in low scalability. To tackle these limitations, this work proposes a graph-reinforcement learning framework for the restoration problem. We link the power system topology with a graph convolutional network, which captures the complex mechanism of network restoration in power networks and understands the mutual interactions among controllable devices. Latent features over graphical power networks produced by graph convolutional layers are exploited to learn the control policy for network restoration using deep reinforcement learning. The solution scalability is guaranteed by modeling distributed generators as agents in a multi-agent environment and a proper pre-training paradigm. Comparative studies on IEEE 123-node and 8500-node test systems demonstrate the performance of the proposed solution.},
  keywords = {0 - Cited,0 - DRL,0 - RL,A - Service Restoration,A - Smart Grid,D - IEEE123,D - PES Test Feeder,DRL - DQL,DRL - DQN,GNN - GCN,RL - Multi-Agent,T - Attention Mechanism,T - Multi-head Attention,Type - Article},
  file = {../../../documents/school/dissertation/bibliography/Journal Article/2022 - Zhao-Wang - Learning Sequential Distribution System Restoration via Graph-Reinforcement.pdf;../../../documents/school/dissertation/zotero/storage/W7FPEZDC/9508140.html}
}

@incollection{zhaoRepresentationLearning2022,
  title = {Representation {{Learning}}},
  booktitle = {Graph {{Neural Networks}}: {{Foundations}}, {{Frontiers}}, and {{Applications}}},
  author = {Zhao, Liang and Wu, Lingfei and Cui, Peng and Pei, Jian},
  editor = {Wu, Lingfei and Cui, Peng and Pei, Jian and Zhao, Liang},
  year = {2022},
  pages = {3--15},
  publisher = {{Springer Nature}},
  address = {{Singapore}},
  doi = {10.1007/978-981-16-6054-2_1},
  urldate = {2024-01-07},
  abstract = {In this chapter, we first describe what representation learning is and why we need representation learning. Among the various ways of learning representations, this chapter focuses on deep learning methods: those that are formed by the composition of multiple non-linear transformations, with the goal of resulting in more abstract and ultimately more useful representations. We summarize the representation learning techniques in different domains, focusing on the unique challenges and models for different data types including images, natural languages, speech signals and networks. Last, we summarize this chapter.},
  isbn = {9789811660542},
  langid = {english},
  file = {../../../documents/school/dissertation/bibliography/Book Section/2022 - Zhao et al - Representation Learning.pdf}
}
