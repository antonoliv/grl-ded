\chapter{Hyperparameters} \label{appendix1}

\section{\ac{SAC} Parameters}

\begin{table}[ht!]
	\centering
	\caption{\ac{SAC} Parameters Description}
	\rowcolors{2}{gray!15}{white}
	\begin{tabular}{lp{8cm}}
		\toprule
		\textbf{Name} & \textbf{Description} \\
		\midrule
		\texttt{policy} & The policy model to use  \\
		\texttt{learning\_rate} & Learning rate applied in Q-Values, Actor and Value functions \\
		\texttt{buffer\_size} & Size of the replay buffer \\
		\texttt{learning\_starts} & Number of transitions to collect before the start of training \\
		\texttt{batch\_size} &  Minibatch size for each gradient update\\
		\texttt{tau} &  Soft update coefficient or Polyak Update \\
		\texttt{gamma} & Discount factor \\
		\texttt{train\_freq} &  Update frequency in training \\
		\texttt{gradient\_steps} & Number of gradient steps per rollout \\
		\texttt{action\_noise} & Type of action noise \\
		\texttt{ent\_coef} & Entropy regularization coefficient \\
		\texttt{target\_update\_interval} &  Step period to update the target network \\
		\texttt{target\_entropy} & Target entropy during learning of \texttt{ent\_coef} \\
		\texttt{use\_sde} &  Use generalized State Dependent Exploration (gSDE) \\
		\texttt{sde\_sample\_freq} & Step period to sample new noise matrix for gSDE \\
		\texttt{use\_sde\_at\_warmup} &  Use gSDE instead of uniform sampling before learning starts \\
		\texttt{optimize\_memory\_usage} & Enable a memory efficient variant of the replay buffer
		at a cost of more complexity \\
		\texttt{replay\_buffer\_class} & Class of replay buffer \\
		\texttt{replay\_buffer\_kwargs} & Additional keyword arguments to pass to replay buffer \\
		\texttt{stats\_window\_size} & Size of statistics output window \\
		\texttt{tensorboard\_log} & Log location for tensorboard \\
		\texttt{verbose} & Verbosity level \\
		\texttt{seed} & Seed for the pseudo random generators \\
		\texttt{device} & Device which the code will be executed in \\
		\texttt{\_init\_setup\_model} & Wether to build the network at the creation of the instance \\
		\texttt{env} & Gymnasium Environment \\
		\texttt{policy\_kwargs} & Additional arguments for policy \\
		
		\bottomrule
	\end{tabular}
	\label{tab:sac-params}
\end{table}

\begin{table}[ht!]
	\centering
	\rowcolors{2}{gray!15}{white}
	\caption{\ac{SAC} Policy Parameters Description}
	\begin{tabular}{lp{8cm}}
		\toprule
		\textbf{Name} & \textbf{Description} \\
		\midrule
		\texttt{net\_arch} & Architecture of policy and value networks \\
		\texttt{optimizer\_class} & Optimizer to use \\
		\texttt{optimizer\_kwargs} & Additional keyword arguments for optimizer \\
		\texttt{activation\_fn} & Activation function \\
		\texttt{n\_critics} & Number of critic networks \\
		\texttt{features\_extractor\_class} & Features extractor for the minibatches  \\
		\texttt{features\_extractor\_kwargs} & Additional keyword arguments for feature extractor \\
		\bottomrule
	\end{tabular}
	\label{tab:sac-pol-params}
\end{table}

\section{\ac{GNN} Parameters}

\begin{table}[ht!]
	\centering
	\caption{\ac{GNN} Parameters description}
	\rowcolors{2}{gray!15}{white}
	\begin{tabular}{lp{8cm}}
		\toprule
		\textbf{Name} & \textbf{Description} \\
		\midrule
		\texttt{in\_channels} & Input sample size \\
		\texttt{hidden\_channels} & Hidden sample size\\
		\texttt{num\_layers} & Number of message passing layers  \\
		\texttt{out\_channels} & Output size \\
		\texttt{dropout} & Dropout probability \\
		\texttt{act} & Non-linear activation function \\
		\texttt{act\_first} & Apply activation before normalization \\
		\texttt{act\_kwargs} & Additional keyword arguments for activation function \\
		\texttt{norm} & Normalization function \\
		\texttt{norm\_kwargs} & Additional arguments passed to normalization function \\
		\texttt{jk} & Jumping knowledge mode \\
		\texttt{aggr} & Aggregation scheme \\
		\texttt{aggr\_kwargs} & Additional keyword arguments for aggregation scheme \\
		\texttt{flow} & Message passing flow direction \\
		\texttt{node\_dim} & The axis along which to propagate \\
		\texttt{decomposed\_layers} & Number of feature decomposition layers\\
		
		\texttt{improved} & If \texttt{True}, layer computes $\mathbf{\hat{A}}$ as $\mathbf{A} + 2\mathbf{I}$ \\
		\texttt{cached} & If \texttt{True}, layer will cache $\mathbf{\hat{D}}^{-1/2} \mathbf{\hat{A}} \mathbf{\hat{D}}^{-1/2}$ (only for transductive scenarios) \\
		\texttt{add\_self\_loops} & Add self-loops to the input graph \\
		\texttt{normalize} & Add self-loops and compute symmetric normalization coefficients \\
		\texttt{bias} & Learn additive bias \\
	
		\texttt{heads} & Number of heads for multi-head attention \\
		\texttt{v2} & Use \textit{GATv2Conv} instead of \textit{GATConv} \\
		\texttt{concat} & Concatenate multi-head attentions instead of averaging \\
		\texttt{negative\_slope} & LeakyReLU angle of the negative slope \\
		\texttt{edge\_dim} & Edge feature dimensionality \\
		\texttt{fill\_value} & How to fill edge features of self-loops \\
		\bottomrule
	\end{tabular}
	\label{tab:gnn-params}
\end{table}

\begin{table}[ht!]
	\centering
	\rowcolors{2}{gray!15}{white}
	\caption{\ac{GCN}-Specific Parameters description}
	\begin{tabular}{lp{8cm}}
		\toprule
		\textbf{Name} & \textbf{Description} \\
		\midrule
		\texttt{improved} & If \texttt{True}, layer computes $\mathbf{\hat{A}}$ as $\mathbf{A} + 2\mathbf{I}$ \\
		\texttt{cached} & If \texttt{True}, layer will cache $\mathbf{\hat{D}}^{-1/2} \mathbf{\hat{A}} \mathbf{\hat{D}}^{-1/2}$ (only for transductive scenarios) \\
		\texttt{add\_self\_loops} & Add self-loops to the input graph \\
		\texttt{bias} & Learn additive bias \\
		\bottomrule
	\end{tabular}
	\label{tab:gcn-params}
\end{table}

\begin{table}[ht!]
	\centering
	\caption{\ac{GAT}-Specific Parameters description}
	\rowcolors{2}{gray!15}{white}
	\begin{tabular}{lp{8cm}}
		\toprule
		\textbf{Name} & \textbf{Description} \\
		\midrule
				
		\texttt{heads} & Number of heads for multi-head attention \\
		\texttt{bias} & Learn additive bias \\
		\texttt{add\_self\_loops} & Add self-loops to the input graph \\
		\texttt{v2} & Use \textit{GATv2Conv} instead of \textit{GATConv} \\
		\texttt{concat} & Concatenate multi-head attentions instead of averaging \\
		\texttt{negative\_slope} & LeakyReLU angle of the negative slope \\
		\texttt{edge\_dim} & Edge feature dimensionality \\
		\texttt{fill\_value} & How to fill edge features of self-loops \\
		\bottomrule
	\end{tabular}
	\label{tab:gat-params}
\end{table}


\section{Environment Parameters}

\begin{table}[ht!]
	\centering
	\rowcolors{2}{gray!15}{white}
	\caption{\ac{GNN} Parameters description}
	\begin{tabular}{lp{8cm}}
		\toprule
		\textbf{Name} & \textbf{Description} \\
		\midrule
		\texttt{env\_path} & \\
		\texttt{reward} & \\
		\texttt{obs\_scaled} & \\
		\texttt{obs\_step} & \\
		\texttt{act\_no\_curtail} & \\
		\texttt{act\_limit\_inf} & \\
		\texttt{climit\_type} & \\
		\texttt{climit\_end} & \\
		\texttt{climit\_low} & \\
		\texttt{climit\_factor} & \\
		\bottomrule
	\end{tabular}
	\label{tab:env-params}
\end{table}
